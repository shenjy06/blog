---
title: æ„å»ºè‡ªå·±çš„å¤§æ¨¡å‹
date: 2025-10-20 18:28:42
tags:
  - å¤§æ¨¡å‹
---

## è¯·ç”¨é«˜ä¸­ç”Ÿå¯ä»¥ç†è§£çš„æ–¹å¼ä»‹ç»ä¸‹å¤§è¯­è¨€æ¨¡å‹

**ä¸€å¥è¯ç†è§£ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°±æ˜¯ä¸€ä¸ªâ€œè¶…çº§ä¼šèŠå¤©ã€ä¼šå†™ä½œçš„äººå·¥æ™ºèƒ½å¤§è„‘â€ï¼Œå®ƒé€šè¿‡å­¦ä¹ æµ·é‡çš„æ–‡å­—ï¼Œå­¦ä¼šäº†å¦‚ä½•åƒäººä¸€æ ·ç†è§£å’Œç”Ÿæˆè¯­è¨€ã€‚**

---

### ğŸ§  ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ

- **â€œå¤§â€**ï¼šæŒ‡çš„æ˜¯å®ƒå­¦è¿‡çš„å†…å®¹éå¸¸å¤šï¼Œå‚æ•°ï¼ˆå°±åƒå¤§è„‘é‡Œçš„â€œç¥ç»å…ƒè¿æ¥â€ï¼‰å¯èƒ½æœ‰ä¸Šäº¿ç”šè‡³ä¸Šåƒäº¿ä¸ªã€‚
- **â€œè¯­è¨€â€**ï¼šå®ƒçš„ä¸»è¦èƒ½åŠ›æ˜¯å¤„ç†æ–‡å­—ï¼Œæ¯”å¦‚ç†è§£é—®é¢˜ã€å†™æ–‡ç« ã€ç¿»è¯‘è¯­è¨€ã€å†™ä»£ç ã€‚
- **â€œæ¨¡å‹â€**ï¼šå°±æ˜¯ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„è®¡ç®—æœºç¨‹åºï¼Œèƒ½æ ¹æ®è¾“å…¥é¢„æµ‹æœ€åˆé€‚çš„è¾“å‡ºã€‚

---

### ğŸ“š å®ƒæ˜¯æ€ä¹ˆå­¦ä¼šçš„ï¼Ÿ

æƒ³è±¡ä¸€ä¸‹ï¼š

- å¦‚æœä½ æ¯å¤©è¯»æˆåƒä¸Šä¸‡æœ¬ä¹¦ã€çœ‹æ— æ•°æ–‡ç« å’Œå¯¹è¯ï¼Œä½ ä¼šæ…¢æ…¢å‘ç°è¯­è¨€é‡Œçš„è§„å¾‹ã€‚
- å¤§è¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯è¿™æ ·ï¼Œå®ƒè¢«â€œå–‚â€äº†æµ·é‡çš„æ–‡æœ¬æ•°æ®ï¼Œç„¶åé€šè¿‡æ•°å­¦å’Œç®—æ³•ï¼Œå­¦ä¼šäº†**â€œä¸‹ä¸€ä¸ªè¯æœ€å¯èƒ½æ˜¯ä»€ä¹ˆâ€**ã€‚
- æ¯”å¦‚è¾“å…¥â€œä»Šå¤©çš„å¤©æ°”çœŸâ€¦â€ï¼Œå®ƒå¯èƒ½é¢„æµ‹â€œå¥½â€â€œçƒ­â€â€œå†·â€ç­‰è¯ï¼Œå¹¶æ ¹æ®ä¸Šä¸‹æ–‡é€‰å‡ºæœ€åˆç†çš„ã€‚

---

### ğŸ” æ‰“ä¸ªæ¯”æ–¹

- **ä¼ ç»Ÿç¨‹åº**ï¼šåƒèœè°±ï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ­¥éª¤å†™æ­»ï¼Œæ‰èƒ½åšå‡ºä¸€é“èœã€‚
- **å¤§è¯­è¨€æ¨¡å‹**ï¼šåƒä¸€ä¸ªè§å¤šè¯†å¹¿çš„å¨å¸ˆï¼Œè§è¿‡æ— æ•°èœè°±å’Œåšæ³•ã€‚ä½ åªè¦å‘Šè¯‰ä»–â€œæˆ‘æƒ³åƒé…¸è¾£çš„â€ï¼Œä»–å°±èƒ½çµæ´»ç»„åˆå‡ºä¸€é“æ–°èœã€‚

---

### ğŸš€ å®ƒèƒ½åšä»€ä¹ˆï¼Ÿ

- **å†™ä½œ**ï¼šå†™ä½œæ–‡ã€å†™æ•…äº‹ã€å†™æ–°é—»ã€‚
- **å­¦ä¹ **ï¼šå¸®ä½ æ€»ç»“çŸ¥è¯†ç‚¹ã€è§£é‡Šéš¾é¢˜ã€‚
- **ç¿»è¯‘**ï¼šåœ¨ä¸åŒè¯­è¨€ä¹‹é—´è‡ªç”±åˆ‡æ¢ã€‚
- **ç¼–ç¨‹**ï¼šå†™ä»£ç ã€æ‰¾é”™è¯¯ã€‚

---

### âš¡ ä¸ºä»€ä¹ˆå«â€œå¤§â€ï¼Ÿ

- æ•°æ®é‡å¤§ï¼šå­¦ä¹ äº†æµ·é‡çš„æ–‡æœ¬ã€‚
- å‚æ•°é‡å¤§ï¼šæ¨¡å‹å†…éƒ¨çš„â€œç¥ç»å…ƒè¿æ¥â€éå¸¸å¤šã€‚
- èƒ½åŠ›å¤§ï¼šèƒ½å¤„ç†å„ç§å¤æ‚çš„è¯­è¨€ä»»åŠ¡ã€‚

---

ğŸ‘‰ **æ€»ç»“**ï¼šå¤§è¯­è¨€æ¨¡å‹å°±åƒä¸€ä¸ªâ€œè¯»è¿‡æ— æ•°ä¹¦çš„è¶…çº§å­¦éœ¸â€ï¼Œå®ƒä¸æ˜¯çœŸçš„ç†è§£ä¸–ç•Œï¼Œä½†å®ƒéå¸¸æ“…é•¿æ ¹æ®å­¦åˆ°çš„è§„å¾‹æ¥â€œé¢„æµ‹å’Œç”Ÿæˆè¯­è¨€â€ï¼Œæ‰€ä»¥çœ‹èµ·æ¥å°±åƒåœ¨å’Œä½ è‡ªç„¶å¯¹è¯ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ åšä¸€ä¸ª **â€œå¤§è¯­è¨€æ¨¡å‹é€Ÿè®°å¡ç‰‡â€**ï¼Œç”¨å›¾è¡¨æŠŠâ€œå®ƒæ˜¯ä»€ä¹ˆã€æ€ä¹ˆå­¦ã€èƒ½åšä»€ä¹ˆâ€æ•´ç†æˆä¸€é¡µï¼Œåƒå¤ä¹ æçº²ä¸€æ ·ï¼Ÿ

## å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•è®­ç»ƒï¼Ÿ

**ç®€ç­”ç‰ˆ**ï¼šå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒåˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š**é¢„è®­ç»ƒ â†’ å¾®è°ƒ â†’ å¼ºåŒ–å­¦ä¹ **ã€‚å…ˆç”¨æµ·é‡æ–‡æœ¬å­¦ä¼šâ€œè¯­è¨€è§„å¾‹â€ï¼Œå†é€šè¿‡äººå·¥ç¤ºä¾‹å­¦ä¼šâ€œå¬æ‡‚æŒ‡ä»¤â€ï¼Œæœ€åç”¨äººç±»åé¦ˆä¸æ–­ä¼˜åŒ–ï¼Œè®©å›ç­”æ›´ç¬¦åˆäººç±»æœŸæœ›ã€‚

---

### ğŸ§© è®­ç»ƒæµç¨‹åˆ†è§£

1. **é¢„è®­ç»ƒï¼ˆPretrainingï¼‰**

   - **ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šè¯­è¨€çš„åŸºæœ¬è§„å¾‹ï¼ˆè¯­æ³•ã€è¯­ä¹‰ã€ä¸Šä¸‹æ–‡ï¼‰ã€‚
   - **æ–¹æ³•**ï¼šç»™æ¨¡å‹å¤§é‡æ–‡æœ¬ï¼ˆä¹¦ç±ã€ç½‘é¡µã€æ–°é—»ï¼‰ï¼Œè®©å®ƒé¢„æµ‹â€œä¸‹ä¸€ä¸ªè¯â€ã€‚
   - **æ¯”å–»**ï¼šå°±åƒé«˜ä¸­ç”Ÿå…ˆè¯»äº†ä¸‰å¹´è¯­æ–‡è¯¾æœ¬å’Œä½œæ–‡é›†ï¼Œç§¯ç´¯äº†å¤§é‡è¯æ±‡å’Œè¯­æ„Ÿã€‚

2. **æœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰**

   - **ç›®æ ‡**ï¼šè®©æ¨¡å‹å­¦ä¼šâ€œå¬æ‡‚äººç±»æŒ‡ä»¤â€ã€‚
   - **æ–¹æ³•**ï¼šäººå·¥å†™ä¸€äº›â€œé—®é¢˜â€”ç†æƒ³ç­”æ¡ˆâ€çš„ç¤ºä¾‹ï¼Œæ¨¡å‹æ¨¡ä»¿è¿™äº›ç­”æ¡ˆã€‚
   - **æ¯”å–»**ï¼šåƒè€å¸ˆå‡ºé¢˜å¹¶ç»™å‡ºæ ‡å‡†ç­”æ¡ˆï¼Œå­¦ç”Ÿå­¦ä¼šå¦‚ä½•ç­”é¢˜ã€‚

3. **äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHF, Reinforcement Learning with Human Feedbackï¼‰**
   - **ç›®æ ‡**ï¼šè®©æ¨¡å‹å›ç­”æ›´ç¬¦åˆäººç±»åå¥½ï¼ˆæ¯”å¦‚æ›´ç¤¼è²Œã€æ›´æœ‰é€»è¾‘ï¼‰ã€‚
   - **æ–¹æ³•**ï¼šäººç±»å¯¹æ¨¡å‹çš„å¤šä¸ªå›ç­”æ‰“åˆ†ï¼Œæ¨¡å‹å­¦ä¼šâ€œå“ªç§å›ç­”æ›´å—æ¬¢è¿â€ã€‚
   - **æ¯”å–»**ï¼šåƒä½œæ–‡æ¯”èµ›ï¼Œè¯„å§”æ‰“åˆ†ï¼Œå­¦ç”Ÿæ ¹æ®åé¦ˆä¸æ–­æ”¹è¿›å†™ä½œé£æ ¼ã€‚

---

### âš™ï¸ æŠ€æœ¯è¦ç‚¹

- **æ¶æ„**ï¼šå¤§å¤šæ•°åŸºäº Transformerï¼ˆæ³¨æ„åŠ›æœºåˆ¶èƒ½æ•æ‰é•¿è·ç¦»å…³ç³»ï¼‰ã€‚
- **æ•°æ®**ï¼šè§„æ¨¡å·¨å¤§ï¼Œé€šå¸¸æ˜¯ TB çº§åˆ«çš„æ–‡æœ¬ã€‚
- **ç®—åŠ›**ï¼šéœ€è¦æˆåƒä¸Šä¸‡å¼  GPU/TPU å¹¶è¡Œè®­ç»ƒã€‚
- **å‚æ•°**ï¼šæ¨¡å‹å†…éƒ¨çš„â€œç¥ç»å…ƒè¿æ¥â€å¯èƒ½è¾¾åˆ°æ•°ç™¾äº¿ç”šè‡³ä¸Šä¸‡äº¿ã€‚

---

### ğŸ“– æ‰“ä¸ªç”Ÿæ´»æ¯”æ–¹

- **é¢„è®­ç»ƒ**ï¼šåƒèƒŒäº†æ•´æœ¬ã€Šæ–°åå­—å…¸ã€‹å’Œæ— æ•°ä½œæ–‡èŒƒæ–‡ã€‚
- **å¾®è°ƒ**ï¼šè€å¸ˆä¸“é—¨æ•™ä½ â€œå¦‚ä½•å†™é«˜è€ƒä½œæ–‡â€ã€‚
- **å¼ºåŒ–å­¦ä¹ **ï¼šè€ƒå®˜æ‰“åˆ†ï¼Œä½ æ ¹æ®åˆ†æ•°è°ƒæ•´å†™ä½œé£æ ¼ã€‚

---

ğŸ‘‰ **æ€»ç»“**ï¼šå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå°±åƒåŸ¹å…»ä¸€ä¸ªâ€œè¶…çº§å­¦éœ¸â€ã€‚å…ˆè®©å®ƒè¯»éå¤©ä¸‹ä¹¦ï¼ˆé¢„è®­ç»ƒï¼‰ï¼Œå†æ•™å®ƒå¦‚ä½•ç­”é¢˜ï¼ˆå¾®è°ƒï¼‰ï¼Œæœ€åé€šè¿‡æ‰“åˆ†ä¸æ–­ä¿®æ­£ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œæœ€ç»ˆæ‰èƒ½æˆä¸ºä¸€ä¸ªæ—¢æ‡‚çŸ¥è¯†åˆä¼šäº¤æµçš„â€œè¯­è¨€é«˜æ‰‹â€ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä¸ª**â€œå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸‰é˜¶æ®µé€Ÿè®°å¡ç‰‡â€**ï¼Œç”¨å›¾è¡¨å½¢å¼æŠŠâ€œé¢„è®­ç»ƒâ€”å¾®è°ƒâ€”å¼ºåŒ–å­¦ä¹ â€çš„é€»è¾‘ä¸²èµ·æ¥ï¼Œåƒè€ƒè¯•å¤ä¹ æçº²ä¸€æ ·ï¼Ÿ

- [CSDN åšå®¢ã€Šä»é›¶å¼€å§‹è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„å…¨è¿‡ç¨‹ã€‹](https://blog.csdn.net/l01011_/article/details/148157725)
- [çŸ¥ä¹ã€Šä»é›¶å¼€å§‹è®­ç»ƒå¤§æ¨¡å‹ã€‹](https://zhuanlan.zhihu.com/p/636270877)

## å¦‚ä½•ä»é›¶åšä¸€ä¸ªå¤§æ¨¡å‹å‘¢ï¼Ÿæˆ‘ä»¬å¦‚ä½•å®šä¹‰å¤§æ¨¡å‹çš„å‚æ•°å‘¢ï¼Ÿåšå¥½å¤§æ¨¡å‹åå¦‚ä½•é€šè¿‡ api çš„æ–¹å¼æä¾›ç»™å¤–éƒ¨ç¨‹åºä½¿ç”¨ï¼Ÿ

# ä»é›¶æ„å»ºå¤§è¯­è¨€æ¨¡å‹

ä¸‹é¢ç”¨æ¸…æ™°ã€å¯è½åœ°çš„æ­¥éª¤ï¼Œå¸¦ä½ ä»â€œç™½çº¸â€åˆ°â€œèƒ½é€šè¿‡ API æä¾›æœåŠ¡â€çš„å®Œæ•´è·¯å¾„ã€‚æ¯ä¸ªç¯èŠ‚éƒ½ç‚¹åˆ°ä¸ºæ­¢ï¼Œè¶³å¤Ÿé«˜ä¸­ç”Ÿç†è§£ï¼Œä¹Ÿèƒ½è®©ä½ åŠ¨æ‰‹å®è·µã€‚

---

### è·¯çº¿æ¦‚è§ˆ

- **ç›®æ ‡:** ä»é›¶å®šä¹‰æ¨¡å‹ â†’ è®­ç»ƒ â†’ å¾®è°ƒ â†’ éƒ¨ç½²ä¸º API
- **æ ¸å¿ƒç¯èŠ‚:** åˆ†è¯ä¸æ•°æ® â†’ æ¨¡å‹ç»“æ„ â†’ è®­ç»ƒç­–ç•¥ â†’ å¯¹é½ä¸è¯„æµ‹ â†’ æ¨ç†ä¼˜åŒ– â†’ API æœåŠ¡

---

### åŸºç¡€å‡†å¤‡ä¸æ•°æ®ç®¡çº¿

- **æ•°æ®æ¥æº:** å¼€æºæ–‡æœ¬è¯­æ–™ã€ä½ çš„é¢†åŸŸæ•°æ®ã€å¯¹è¯æ—¥å¿—ã€ä»£ç ä»“åº“ç­‰ã€‚
- **æ¸…æ´—ä¸åˆ’åˆ†:** å»é‡ã€å»å™ªã€åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ã€‚
- **åˆ†è¯å™¨ä¸è¯è¡¨:** BPE/WordPiece ç­‰ï¼›è¯è¡¨å¤§å°å½±å“è®°å¿†ä¸é€Ÿåº¦ã€‚
- **æ ¼å¼åŒ–è®­ç»ƒæ ·æœ¬:** ä»¥â€œæŒ‡ä»¤+ä¸Šä¸‹æ–‡+æœŸæœ›è¾“å‡ºâ€ç»„ç»‡ï¼Œä¿è¯ä¸€è‡´çš„æç¤ºæ ¼å¼ã€‚
- **åº¦é‡ä¸åŸºçº¿:** å…ˆä¸ºå°æ¨¡å‹è®¾åŸºçº¿ï¼ˆå¦‚å›°æƒ‘åº¦ã€å‡†ç¡®ç‡ï¼‰ï¼Œå†é€æ­¥æ‰©å¤§è§„æ¨¡ã€‚

---

### å®šä¹‰â€œå¤§æ¨¡å‹â€çš„å…³é”®å‚æ•°

- **æ¨¡å‹æ¶æ„:** ä»¥ Transformer ä¸ºä¸»ï¼ˆè‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œï¼‰ã€‚
- **æ ¸å¿ƒç»´åº¦:**
  - **è¯è¡¨å¤§å°:** å†³å®šå¯ç›´æ¥è¯†åˆ«çš„â€œè¯/å­è¯â€çš„èŒƒå›´ã€‚
  - **åµŒå…¥ç»´åº¦:** æ¯ä¸ªè¯çš„å‘é‡ç»´åº¦ï¼Œå†³å®šè¡¨è¾¾èƒ½åŠ›ã€‚
  - **å±‚æ•°:** å †å çš„ Transformer å±‚æ•°ï¼Œå½±å“æ·±åº¦ä¸æ‹Ÿåˆèƒ½åŠ›ã€‚
  - **æ³¨æ„åŠ›å¤´æ•°:** å¹¶è¡Œå…³æ³¨ä¸åŒç‰¹å¾çš„â€œè§†è§’â€æ•°é‡ã€‚
  - **ä¸Šä¸‹æ–‡é•¿åº¦:** ä¸€æ¬¡èƒ½è¯»å…¥çš„æœ€å¤§ Token æ•°ï¼Œå†³å®šâ€œè®°å¿†çª—å£â€ã€‚
  - **å‰é¦ˆå±‚ç»´åº¦:** FFN çš„éšè—ç»´åº¦ï¼Œå½±å“éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚
  - **æ­£åˆ™ä¸ç¨³å®š:** Dropoutã€RMSNorm/LayerNormã€æ®‹å·®ç»“æ„ã€‚
- **è®­ç»ƒè¶…å‚:**
  - **æ‰¹å¤§å°:** è¶Šå¤§è¶Šç¨³ï¼Œä½†åƒæ˜¾å­˜ï¼›å¯ç”¨æ¢¯åº¦ç´¯ç§¯ã€‚
  - **å­¦ä¹ ç‡ä¸è°ƒåº¦:** çº¿æ€§é¢„çƒ­ + ä½™å¼¦é€€ç«/å¤šæ®µä¸‹é™ã€‚
  - **ä¼˜åŒ–å™¨:** AdamW ç­‰ï¼Œæ³¨æ„æƒé‡è¡°å‡å’Œæ¢¯åº¦è£å‰ªã€‚
  - **ç²¾åº¦ä¸å¹¶è¡Œ:** æ··åˆç²¾åº¦ï¼ˆFP16/BF16ï¼‰ã€æ•°æ®å¹¶è¡Œã€å¼ é‡å¹¶è¡Œã€‚
  - **æ£€æŸ¥ç‚¹:** å®šæœŸä¿å­˜æƒé‡ä¸ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­ã€‚

---

### è®­ç»ƒåˆ°å¯¹é½çš„å®Œæ•´æµç¨‹

1. **é¢„è®­ç»ƒï¼ˆè¯­è¨€è§„å¾‹ï¼‰**
   - **ç›®æ ‡:** é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼›å­¦ä¼šè¯­æ³•ã€è¯­ä¹‰ã€é•¿ç¨‹ä¾èµ–ã€‚
   - **åšæ³•:** å¤§è§„æ¨¡é€šç”¨è¯­æ–™ï¼Œè‡ªç›‘ç£æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ï¼Œé•¿æ—¶é—´è®­ç»ƒã€‚
2. **æœ‰ç›‘ç£å¾®è°ƒï¼ˆå¬æ‡‚æŒ‡ä»¤ï¼‰**
   - **ç›®æ ‡:** è¾“å…¥æŒ‡ä»¤èƒ½äº§å‡ºâ€œè§„èŒƒç­”æ¡ˆâ€ã€‚
   - **åšæ³•:** ç”¨äººå·¥æ ‡æ³¨çš„â€œé—®â€”ç­”â€æ ·æœ¬è®­ç»ƒï¼Œç»Ÿä¸€æç¤ºæ¨¡æ¿ã€‚
3. **äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆæ›´åˆäººæ„ï¼‰**
   - **ç›®æ ‡:** è®©å›ç­”æ›´å®‰å…¨ã€ç¤¼è²Œã€æ¸…æ™°ã€æœ‰é€»è¾‘ã€‚
   - **åšæ³•:** è®­ç»ƒå¥–åŠ±æ¨¡å‹å¯¹å¤šå€™é€‰æ’åºï¼Œå†ç”¨ç­–ç•¥ä¼˜åŒ–ï¼ˆå¦‚ PPOï¼‰æå‡ã€‚
4. **è¯„æµ‹ä¸å®‰å…¨**
   - **ç¦»çº¿æŒ‡æ ‡:** å›°æƒ‘åº¦ã€å‡†ç¡®ç‡ã€BLEU/ROUGEã€ä»£ç é€šè¿‡ç‡ç­‰ã€‚
   - **åœ¨çº¿è´¨æ£€:** å¯¹è¯è´¨é‡ã€äº‹å®æ€§ã€æ‹’ç­”è¾¹ç•Œã€è¶Šç‹±æµ‹è¯•ã€‚
   - **çº¢çº¿ç­–ç•¥:** æ•æ„Ÿè¯é¢˜è¿‡æ»¤ã€è§„åˆ™æ¨¡æ¿ã€æ‹’ç­”ç­–ç•¥ä¸å®¡è®¡ã€‚

---

### éƒ¨ç½²ä¸º API çš„å®æˆ˜è·¯å¾„

#### æ¨ç†ä¼˜åŒ–ä¸æ‰“åŒ…

- **æƒé‡æ ¼å¼:** è½¬ ONNX/Safetensorsï¼›æŒ‰éœ€æ‹†åˆ†åˆ†ç‰‡ã€‚
- **åŠ é€Ÿæ–¹æ¡ˆ:** KV Cacheã€å¼ é‡å¹¶è¡Œã€æ‰¹é‡æ¨ç†ã€æµå¼è¾“å‡ºã€‚
- **é‡åŒ–ä¸è’¸é¦:** INT8/INT4 é‡åŒ–é™ä½æ˜¾å­˜ï¼›å°æ¨¡å‹è’¸é¦æå‡æ€§ä»·æ¯”ã€‚
- **æœåŠ¡åŒ–ç»„ä»¶:** é€‰æ‹©æ¨ç†åç«¯ï¼ˆå¦‚æ”¯æŒå¹¶å‘å’Œé•¿ä¸Šä¸‹æ–‡ï¼‰ã€å¥åº·æ£€æŸ¥ã€ç›‘æ§ä¸æ—¥å¿—ã€‚

#### å¿«é€Ÿæ­ä¸€ä¸ªå¯ç”¨çš„ APIï¼ˆç¤ºä¾‹ï¼‰

- **åŠŸèƒ½ç‚¹:** Chatã€æµå¼å“åº”ã€æ¸©åº¦æ§åˆ¶ã€é‰´æƒã€é™æµã€é…é¢ã€è§‚æµ‹
- **æ¥å£è®¾è®¡:** /v1/chat/completionsï¼ˆä¸ä¸»æµå…¼å®¹ï¼‰ã€JSON è¾“å…¥/è¾“å‡ºã€OpenAPI æ–‡æ¡£

```python
# app.py â€” æœ€å°å¯ç”¨ Chat APIï¼ˆç¤ºæ„ï¼‰
from fastapi import FastAPI, Header, HTTPException
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="Mark-LLM API", version="1.0.0")

class Message(BaseModel):
    role: str  # "system" | "user" | "assistant"
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    stream: Optional[bool] = False

class ChatChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class ChatResponse(BaseModel):
    id: str
    model: str
    choices: List[ChatChoice]

def run_inference(messages, temperature, max_tokens):
    # TODO: è°ƒç”¨ä½ åŠ è½½çš„æ¨¡å‹ï¼Œåšæ¨ç†ï¼Œå¹¶æ”¯æŒæµå¼è¾“å‡º
    # è¿™é‡Œè¿”å›ä¸€ä¸ªæ¼”ç¤ºç»“æœ
    return "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„æ¨¡å‹ã€‚"

@app.post("/v1/chat/completions", response_model=ChatResponse)
def chat(req: ChatRequest, authorization: Optional[str] = Header(None)):
    # ç®€å•é‰´æƒ
    if authorization != "Bearer YOUR_API_KEY":
        raise HTTPException(status_code=401, detail="Invalid API key")

    output = run_inference(req.messages, req.temperature, req.max_tokens)
    return ChatResponse(
        id="chatcmpl-demo",
        model=req.model,
        choices=[ChatChoice(index=0, message=Message(role="assistant", content=output), finish_reason="stop")]
    )
```

- **è°ƒç”¨ç¤ºä¾‹:**

```bash
curl https://your.domain/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "mark-llm-7b",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¹äºåŠ©äººçš„åŠ©æ‰‹"},
      {"role": "user", "content": "ç”¨é«˜ä¸­æ°´å¹³è§£é‡Šäº‘åŸç”Ÿ"}
    ],
    "temperature": 0.7,
    "max_tokens": 300
  }'
```

- **ä¸Šçº¿è¦ç‚¹:**
  - **é‰´æƒä¸é…é¢:** API key/ç§Ÿæˆ·ä½“ç³»ã€é€Ÿç‡é™åˆ¶ã€é¢åº¦ç®¡ç†ã€‚
  - **å¼¹æ€§ä¸é«˜å¯ç”¨:** å¤šå‰¯æœ¬ã€è´Ÿè½½å‡è¡¡ã€æ»šåŠ¨å‡çº§ã€ç°åº¦å‘å¸ƒã€‚
  - **ç›‘æ§ä¸è¿½è¸ª:** QPSã€å»¶è¿Ÿã€é”™è¯¯ç‡ã€GPU åˆ©ç”¨ç‡ã€æ—¥å¿—ä¸å®¡è®¡ã€‚
  - **ç‰ˆæœ¬ä¸å…¼å®¹:** ç»´æŒ stable æ¥å£ï¼Œæ–°å¢èƒ½åŠ›ç”¨æ–°ç‰ˆæœ¬å·ã€‚

---

### ç»™ä½ çš„ä¸€å¼ â€œåŠ¨æ‰‹æ¸…å•â€

- **æ˜ç¡®ç›®æ ‡:** å†™ä½œåŠ©æ‰‹/æ•™è‚²é—®ç­”/ä»£ç åŠ©ç†ï¼Œå†³å®šè®­ç»ƒæ•°æ®ä¸è¯„æµ‹é›†ã€‚
- **å…ˆå°åå¤§:** å…ˆè®­ 100Mâ€“1B çº§åˆ«æ¨¡å‹è·‘é€šæµç¨‹ï¼Œå†æ‰©å±•åˆ° 7B/13Bã€‚
- **æ„å»ºæ•°æ®ä¸åˆ†è¯:** æ‰“é€šæ¸…æ´—â€”åˆ†è¯â€”æ ·æœ¬ç”Ÿæˆâ€”æ•°æ®åŠ è½½çš„æµæ°´çº¿ã€‚
- **è¿­ä»£è®­ç»ƒ:** è®¾å®šæ£€æŸ¥ç‚¹ä¸æŒ‡æ ‡æ›²çº¿ï¼Œè§‚å¯Ÿè¿‡æ‹Ÿåˆä¸æ³›åŒ–ã€‚
- **å¯¹é½ä¸å®‰å…¨:** åŠ å…¥ SFT+RLHFï¼Œè®¾è®¡æ‹’ç­”ä¸äº‹å®æ€§ç­–ç•¥ã€‚
- **æ¨ç†ä¸æœåŠ¡:** é€‰æ‹©åŠ é€Ÿ/é‡åŒ–æ–¹æ¡ˆï¼Œä¸Šçº¿ APIï¼Œé…ç›‘æ§ä¸é™æµã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥æŒ‰ä½ çš„å…·ä½“ç›®æ ‡ï¼ˆæ¯”å¦‚â€œä¸­æ–‡æ•™è‚²é—®ç­”â€æˆ–â€œå®¶åº­å¥åº·çŸ¥è¯†åº“â€ï¼‰ç»™å‡ºä¸€ç‰ˆå¯æ‰“å°çš„è®­ç»ƒä¸éƒ¨ç½²æµç¨‹å›¾ã€å‚æ•°æ¨èè¡¨å’Œè¯„æµ‹æ¸…å•ï¼Œè®©ä½ ä¸€é¡µå°±èƒ½æŒæ§å…¨å±€ã€‚

## è¯·å¸®æˆ‘å®Œæˆ

å¥½çš„ ğŸ‘ æˆ‘æ¥å¸®ä½ æŠŠâ€œä»é›¶åšä¸€ä¸ªå¤§æ¨¡å‹ â†’ å®šä¹‰å‚æ•° â†’ æä¾› API æœåŠ¡â€çš„å®Œæ•´æµç¨‹æ•´ç†æˆä¸€ä»½**é€Ÿè®°å¡ç‰‡å¼æŒ‡å—**ï¼Œæ–¹ä¾¿ä½ ä¸€çœ¼æŒæ¡å…¨å±€ã€‚

---

## ğŸ§© ä¸€å¼ å›¾çœ‹æ‡‚å¤§æ¨¡å‹å…¨æµç¨‹

**1. æ•°æ®å‡†å¤‡ â†’ 2. æ¨¡å‹å®šä¹‰ â†’ 3. è®­ç»ƒ â†’ 4. å¾®è°ƒä¸å¯¹é½ â†’ 5. éƒ¨ç½² â†’ 6. API æœåŠ¡**

---

### 1ï¸âƒ£ æ•°æ®å‡†å¤‡

- **æ”¶é›†**ï¼šå¼€æºè¯­æ–™ã€ä¹¦ç±ã€ç½‘é¡µã€ä»£ç ã€å¯¹è¯æ•°æ®
- **æ¸…æ´—**ï¼šå»é‡ã€å»å™ªã€ç»Ÿä¸€æ ¼å¼
- **åˆ†è¯**ï¼šBPE/WordPieceï¼Œå†³å®šè¯è¡¨å¤§å°ï¼ˆå¸¸è§ 30kâ€“100kï¼‰

---

### 2ï¸âƒ£ æ¨¡å‹å‚æ•°å®šä¹‰

- **å±‚æ•°ï¼ˆLayersï¼‰**ï¼šè¶Šå¤šè¶Šæ·±ï¼Œå¸¸è§ 12â€“96 å±‚
- **éšè—ç»´åº¦ï¼ˆHidden sizeï¼‰**ï¼šæ¯ä¸ªè¯çš„å‘é‡ç»´åº¦ï¼Œå¸¸è§ 768â€“8192
- **æ³¨æ„åŠ›å¤´æ•°ï¼ˆHeadsï¼‰**ï¼šå¹¶è¡Œå…³æ³¨ä¸åŒç‰¹å¾ï¼Œå¸¸è§ 12â€“128
- **ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext lengthï¼‰**ï¼šä¸€æ¬¡èƒ½è¯»å¤šå°‘ tokenï¼Œå¸¸è§ 2kâ€“128k
- **å‚æ•°é‡ï¼ˆParametersï¼‰**ï¼šæ•´ä½“è§„æ¨¡ï¼Œç™¾ä¸‡åˆ°åƒäº¿ä¸ç­‰

ğŸ‘‰ **å¤§æ¨¡å‹çš„â€œå¤§å°â€å°±æ˜¯è¿™äº›å‚æ•°çš„æ€»æ•°**ã€‚æ¯”å¦‚ GPT-3 æœ‰ 1750 äº¿å‚æ•°ã€‚

---

### 3ï¸âƒ£ è®­ç»ƒé˜¶æ®µ

- **é¢„è®­ç»ƒ**ï¼šç”¨å¤§è§„æ¨¡è¯­æ–™é¢„æµ‹â€œä¸‹ä¸€ä¸ªè¯â€
- **å¾®è°ƒï¼ˆSFTï¼‰**ï¼šç”¨äººå·¥æ ‡æ³¨çš„é—®ç­”æ•°æ®ï¼Œå­¦ä¼šå¬æ‡‚æŒ‡ä»¤
- **RLHF**ï¼šäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼Œè®©å›ç­”æ›´ç¬¦åˆäººç±»åå¥½

---

### 4ï¸âƒ£ éƒ¨ç½²ä¸æ¨ç†ä¼˜åŒ–

- **æ¨¡å‹å‹ç¼©**ï¼šé‡åŒ–ï¼ˆINT8/INT4ï¼‰ã€è’¸é¦ï¼ˆå°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹ï¼‰
- **æ¨ç†åŠ é€Ÿ**ï¼šKV Cacheã€å¹¶è¡Œè®¡ç®—ã€æµå¼è¾“å‡º
- **å®¹å™¨åŒ–**ï¼šç”¨ Docker/Kubernetes éƒ¨ç½²ï¼Œæ–¹ä¾¿æ‰©å±•

---

### 5ï¸âƒ£ æä¾› API æœåŠ¡

- **åç«¯æ¡†æ¶**ï¼šFastAPI / Flask / Node.js
- **æ¥å£è®¾è®¡**ï¼š
  - è·¯å¾„ï¼š`/v1/chat/completions`
  - è¾“å…¥ï¼š`messages`ï¼ˆå¯¹è¯å†å²ï¼‰ã€`temperature`ï¼ˆéšæœºæ€§ï¼‰ã€`max_tokens`ï¼ˆè¾“å‡ºé•¿åº¦ï¼‰
  - è¾“å‡ºï¼šæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
- **è°ƒç”¨ç¤ºä¾‹**ï¼š

```bash
curl https://your.domain/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "my-llm-7b",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹"},
      {"role": "user", "content": "è¯·ç”¨é«˜ä¸­ç”Ÿèƒ½æ‡‚çš„æ–¹å¼è§£é‡Šäº‘åŸç”Ÿ"}
    ],
    "temperature": 0.7,
    "max_tokens": 300
  }'
```

---

### 6ï¸âƒ£ å…³é”®è¦ç‚¹

- **å®‰å…¨**ï¼šè¿‡æ»¤æ•æ„Ÿå†…å®¹ï¼Œé¿å…è¶Šç‹±
- **ç›‘æ§**ï¼šè®°å½• QPSã€å»¶è¿Ÿã€é”™è¯¯ç‡
- **æ‰©å±•**ï¼šæ”¯æŒå¤šå‰¯æœ¬ã€è´Ÿè½½å‡è¡¡ã€ç°åº¦å‘å¸ƒ

---

âœ… **æ€»ç»“**ï¼š

- **å¤§æ¨¡å‹çš„â€œå¤§å°â€ = å‚æ•°é‡ï¼ˆå±‚æ•° Ã— éšè—ç»´åº¦ Ã— æ³¨æ„åŠ›å¤´æ•°ç­‰ï¼‰**
- **è®­ç»ƒä¸‰æ­¥èµ°ï¼šé¢„è®­ç»ƒ â†’ å¾®è°ƒ â†’ RLHF**
- **éƒ¨ç½²ä¸‰ä»¶äº‹ï¼šä¼˜åŒ– â†’ å®¹å™¨åŒ– â†’ API æœåŠ¡åŒ–**

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠè¿™ä»½å†…å®¹æ•´ç†æˆä¸€å¼ **å¯æ‰“å°çš„æµç¨‹å›¾/å­¦ä¹ å¡ç‰‡**ï¼ˆæ¯”å¦‚â€œä»é›¶åˆ° API çš„ 6 æ­¥å›¾è§£â€ï¼‰ï¼Œè¿™æ ·ä½ å¯ä»¥è´´åœ¨æ¡Œå­ä¸Šéšæ—¶å¤ä¹ ï¼Ÿ

## æˆ‘ç°åœ¨æœ‰ä¸€å° 16GB å†…å­˜ï¼Œ8 æ ¸å¿ƒçš„ cpuï¼Œå¦‚æœæˆ‘æƒ³åŸºäºè¿™æ ·çš„ç¡¬ä»¶æ¡ä»¶ä»å¤´å¼€å§‹åˆ¶ä½œä¸€ä¸ªå¤§æ¨¡å‹çš„è¯ï¼Œè¯·å¸®æˆ‘æŒ‰ä¸Šé¢çš„æ­¥éª¤ï¼ŒåŸºäºæˆ‘æ‰‹ä¸Šçš„ç¡¬ä»¶å®Œæˆä¸€ä¸ªå¤§æ¨¡å‹

# åœ¨ 16GB å†…å­˜ã€8 æ ¸ CPU ä¸Šä»é›¶æ„å»ºå¹¶æä¾›å¤§æ¨¡å‹ API

å…ˆè¯´äººè¯ç‰ˆç»“è®ºï¼šç”¨è¿™å°æœºå™¨â€œä»é›¶â€è®­ç»ƒè¶…å¤§æ¨¡å‹ä¸ç°å®ï¼Œä½†ä½ å¯ä»¥åšä¸€ä¸ªå®ç”¨çš„â€œå°è€Œç²¾â€ä¸­æ–‡æŒ‡ä»¤æ¨¡å‹ï¼ˆ0.1B çº§åˆ«æˆ–æ›´å°ï¼‰ï¼Œè·‘é€šå…¨æµç¨‹ï¼šæ•°æ® â†’ åˆ†è¯ â†’ é¢„è®­ç»ƒ â†’ å¾®è°ƒ â†’ éƒ¨ç½² â†’APIã€‚ä¸‹é¢ç»™ä½ ä¸€å¥—å¯è½åœ°çš„é…ç½®å’Œæ­¥éª¤ã€‚

---

## ç›®æ ‡ä¸ç­–ç•¥

- **ç›®æ ‡:** è®­ç»ƒä¸€ä¸ªæœ¬åœ°å¯ç”¨çš„ä¸­æ–‡/ä¸­è‹±å°å‹æŒ‡ä»¤æ¨¡å‹ï¼Œæ”¯æŒèŠå¤©ä¸é—®ç­”ã€‚
- **ç­–ç•¥:** é‡‡ç”¨æç®€ Transformerã€å°è¯è¡¨ã€çŸ­ä¸Šä¸‹æ–‡ã€CPU è®­ç»ƒï¼›å…ˆé¢„è®­ç»ƒæå°æ¨¡å‹ï¼Œå†åšå°‘é‡é«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒï¼›æ¨ç†ç«¯æ”¯æŒæµå¼è¾“å‡ºä¸ç®€å•é‰´æƒã€‚

---

## å¯è¡Œçš„æ¨¡å‹è§„æ ¼ä¸è®­ç»ƒè¶…å‚

### æœ€å°å¯è¡Œæ¨¡å‹è§„æ ¼ï¼ˆé€‚é… 16GB å†…å­˜ï¼ŒCPU è®­ç»ƒï¼‰

- **è¯è¡¨å¤§å°:** 16kï¼ˆä¸­æ–‡+è‹±æ–‡å­è¯ï¼ŒBPEï¼‰
- **ä¸Šä¸‹æ–‡é•¿åº¦:** 512 tokensï¼ˆè¶³ä»¥æ”¯æŒçŸ­é—®ç­”ï¼‰
- **å±‚æ•°:** 8 Transformer blocks
- **éšè—ç»´åº¦:** 512
- **æ³¨æ„åŠ›å¤´æ•°:** 8
- **å‰é¦ˆç»´åº¦:** 2048
- **å½’ä¸€åŒ–ä¸æ­£åˆ™:** RMSNormï¼Œ**Dropout:** 0.1
- **å‚æ•°é‡ä¼°ç®—:** â‰ˆ 30â€“60Mï¼ˆå¯æ§åœ¨ç™¾å…†ä»¥å†…ï¼‰

### è®­ç»ƒè¶…å‚ï¼ˆCPU å‹å¥½ï¼‰

- **æ‰¹å¤§å°:** 8â€“16ï¼ˆç”¨æ¢¯åº¦ç´¯ç§¯è®©â€œæœ‰æ•ˆæ‰¹â€è¾¾åˆ° 64ï¼‰
- **å­¦ä¹ ç‡:** 3e-4ï¼ˆçº¿æ€§é¢„çƒ­åˆ° 3e-4ï¼Œç„¶åä½™å¼¦é€€ç«ï¼‰
- **ä¼˜åŒ–å™¨:** AdamWï¼ˆæƒé‡è¡°å‡ 0.01ï¼Œæ¢¯åº¦è£å‰ª 1.0ï¼‰
- **ç²¾åº¦:** FP32ï¼ˆCPUï¼‰ï¼Œå¦‚å¯ç”¨ BF16 æ›´å¥½
- **è®­ç»ƒæ—¶é•¿:** å…ˆè·‘ 24â€“48 å°æ—¶å¾—åˆ°å¯ç”¨çš„â€œè¯­è¨€æ„Ÿâ€ï¼Œå†åšæŒ‡ä»¤å¾®è°ƒ 6â€“12 å°æ—¶
- **æ£€æŸ¥ç‚¹é¢‘ç‡:** æ¯ 1â€“2 å°æ—¶ä¿å­˜ä¸€æ¬¡ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰

---

## ä»é›¶åˆ° API çš„ 6 æ­¥

### 1. æ•°æ®ä¸åˆ†è¯

- **è¯­æ–™æ¥æº:**
  - **é€šç”¨:** å¼€æºç™¾ç§‘ã€æ–°é—»æ‘˜è¦ã€æ•™ç¨‹ã€å…¬å…±è®ºå›æ¸…æ´—æ–‡æœ¬
  - **æŒ‡ä»¤:** ä½ è‡ªåˆ¶çš„â€œé—®é¢˜-ç­”æ¡ˆâ€å¯¹ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰
- **æ¸…æ´—è§„èŒƒ:**
  - **å»é‡:** æ–‡æœ¬å“ˆå¸Œå»é‡
  - **å»å™ª:** å»å¹¿å‘Šã€å»è„šæœ¬ã€è¿‡æ»¤è¶…é•¿è¡Œ
  - **ç»Ÿä¸€ç¼–ç :** UTF-8ï¼Œç§»é™¤å¼‚å¸¸å­—ç¬¦
- **åˆ†è¯å™¨:**
  - **æ–¹æ³•:** BPEï¼ˆSentencePieceï¼‰
  - **è¯è¡¨:** 16kï¼›ä¸­è‹±æ··åˆï¼›ä¿ç•™æ•°å­—ä¸åŸºç¡€æ ‡ç‚¹
- **æ ·æœ¬æ ¼å¼:**
  - **é¢„è®­ç»ƒ:** çº¯æ–‡æœ¬è¿ç»­ token
  - **æŒ‡ä»¤å¾®è°ƒ:** ç»Ÿä¸€æ¨¡æ¿ï¼Œä¾‹å¦‚ï¼š
    - **ç³»ç»Ÿ:** ä½ æ˜¯ä¹äºåŠ©äººçš„åŠ©æ‰‹
    - **ç”¨æˆ·:** é—®é¢˜â€¦
    - **åŠ©æ‰‹:** æ ‡å‡†ç­”æ¡ˆâ€¦

### 2. æ¨¡å‹æ­å»º

- **æ¶æ„:** Decoder-only Transformerï¼ˆGPT æ ·å¼ï¼‰
- **ç»„ä»¶é€‰æ‹©:**
  - **ä½ç½®ç¼–ç :** RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰
  - **æ³¨æ„åŠ›:** æ ‡å‡†è‡ªæ³¨æ„åŠ›ï¼Œæ”¯æŒ KV ç¼“å­˜ï¼ˆæ¨ç†åŠ é€Ÿï¼‰
  - **æŸå¤±:** äº¤å‰ç†µï¼ˆä¸‹ä¸€ä¸ª token é¢„æµ‹ï¼‰
- **å¯è°ƒå¼€å…³:**
  - **ä¸Šä¸‹æ–‡é•¿åº¦:** 256/512 ä¸¤æ¡£ï¼›è®­ç»ƒç”¨ 256 æ›´çœï¼Œæ¨ç†å¯ 512
  - **Dropout:** 0.1â†’0.0ï¼ˆå¾®è°ƒé˜¶æ®µå¯é™ï¼‰

### 3. é¢„è®­ç»ƒï¼ˆè¯­è¨€èƒ½åŠ›ï¼‰

- **ç›®æ ‡:** å­¦ä¼šåŸºæœ¬è¯­æ³•ä¸è¯­ä¹‰ã€ä¸­æ–‡åˆ†è¯ä¹ æƒ¯
- **åšæ³•:**
  - **æ•°æ®é‡:** 1â€“5GB å¹²å‡€æ–‡æœ¬ï¼ˆè¿™å°æœºå™¨å¯æ‰¿å—çš„å°è§„æ¨¡ï¼‰
  - **è¯¾ç¨‹å­¦ä¹ :** å…ˆçŸ­ä¸Šä¸‹æ–‡ã€çŸ­åºåˆ—ï¼Œåå¢å¤§åˆ° 512
  - **ç›‘æ§:** è®­ç»ƒ/éªŒè¯å›°æƒ‘åº¦ï¼ˆPPLï¼‰ä¸‹é™è¶‹åŠ¿ï¼Œé¿å…å‘æ•£
- **äº§å‡º:** â€œåŸºç¡€è¯­è¨€æ¨¡å‹â€æ£€æŸ¥ç‚¹ï¼ˆå¦‚ base-40Mï¼‰

### 4. æŒ‡ä»¤å¾®è°ƒä¸å¯¹é½

- **æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰:**
  - **æ•°æ®:** 5kâ€“50k æ¡é«˜è´¨é‡é—®ç­”å¯¹ï¼ˆä¸­æ–‡ä¼˜å…ˆï¼‰
  - **æ¨¡æ¿:** å›ºå®šæç¤ºæ ¼å¼ï¼Œæå‡ç¨³å®šæ€§
  - **ç›®æ ‡:** æå‡éµå¾ªæŒ‡ä»¤ã€ç»“æ„åŒ–è¡¨è¾¾èƒ½åŠ›
- **è½»é‡å¯¹é½:**
  - **äººç±»åå¥½:** ç®€å•æ’åºæ•°æ®ï¼ˆä¸¤ä¸¤å¯¹æ¯”é€‰ä¼˜ï¼‰
  - **è®­ç»ƒ:** èµ„æºæœ‰é™å¯è·³è¿‡ RLHFï¼Œè½¬è€Œæ›´ä¸¥æ ¼çš„ SFT æ•°æ®ä¸æ‹’ç­”è§„åˆ™
- **å®‰å…¨ä¸è¾¹ç•Œ:**
  - **å†…å®¹è¿‡æ»¤:** æ•æ„Ÿç±»åˆ«å…³é”®è¯å±è”½
  - **æ‹’ç­”ç­–ç•¥:** å¯¹ä¸å½“è¯·æ±‚è¿”å›é€šç”¨å®‰å…¨æç¤º

### 5. æ¨ç†ä¸éƒ¨ç½²

- **æ¨ç†ä¼˜åŒ–:**
  - **KV ç¼“å­˜:** æ˜¾è‘—åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡ç”Ÿæˆ
  - **é‡‡æ ·ç­–ç•¥:** top-k=40ã€top-p=0.9ã€temperature=0.7ï¼ˆå¯è°ƒï¼‰
  - **æµå¼è¾“å‡º:** è¾¹ç”Ÿæˆè¾¹è¿”å›ï¼Œé™ä½æ„ŸçŸ¥å»¶è¿Ÿ
- **æ‰“åŒ…:**
  - **æƒé‡å­˜å‚¨:** safetensorsï¼ˆå®‰å…¨ã€å¯åˆ†ç‰‡ï¼‰
  - **å®¹å™¨åŒ–:** Dockerï¼ˆå¯é€‰ï¼‰ï¼Œæœ¬æœºå³å¯è·‘
- **ç›‘æ§:**
  - **æ—¥å¿—:** è¯·æ±‚ã€å»¶è¿Ÿã€é”™è¯¯
  - **é™æµ:** ç®€å•ä»¤ç‰Œæ¡¶ï¼ˆé˜²æ­¢è¿‡è½½ï¼‰

### 6. æä¾› API æœåŠ¡ï¼ˆFastAPI ç¤ºä¾‹ï¼‰

- **æ¥å£è®¾è®¡:** å…¼å®¹ä¸»æµ `/v1/chat/completions`ï¼›é‰´æƒç”¨ Bearer Token
- **åŠŸèƒ½:** å†å²æ¶ˆæ¯ã€æ¸©åº¦æ§åˆ¶ã€æœ€å¤§è¾“å‡ºé•¿åº¦ã€å¯é€‰æµå¼

```python
# app.py â€” è½»é‡CPUæ¨¡å‹æ¨ç†APIï¼ˆç¤ºæ„ï¼‰
from fastapi import FastAPI, Header, HTTPException
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="Tiny-LLM API", version="1.0.0")

class Message(BaseModel):
    role: str  # "system" | "user" | "assistant"
    content: str

class ChatRequest(BaseModel):
    model: str = "tiny-llm-40m"
    messages: List[Message]
    temperature: Optional[float] = 0.7
    top_k: Optional[int] = 40
    top_p: Optional[float] = 0.9
    max_tokens: Optional[int] = 256
    stream: Optional[bool] = False

class ChatChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class ChatResponse(BaseModel):
    id: str
    model: str
    choices: List[ChatChoice]

# ä¼ªä»£ç ï¼šåŠ è½½ä½ è®­ç»ƒå¥½çš„æƒé‡ä¸åˆ†è¯å™¨
# tokenizer = load_tokenizer("./tokenizer")
# model = load_model("./tiny-llm-40m.safetensors")

def run_inference(messages, temperature, top_k, top_p, max_tokens):
    # è¿™é‡Œè°ƒç”¨ä½ å®ç°çš„generate()ï¼Œæ”¯æŒKVç¼“å­˜ä¸æµå¼
    # text = generate(model, tokenizer, messages, temperature, top_k, top_p, max_tokens)
    text = "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„æœ¬åœ°å°æ¨¡å‹ï¼Œå·²æ¥å…¥APIã€‚"
    return text

@app.post("/v1/chat/completions", response_model=ChatResponse)
def chat(req: ChatRequest, authorization: Optional[str] = Header(None)):
    if authorization != "Bearer YOUR_API_KEY":
        raise HTTPException(status_code=401, detail="Invalid API key")

    output = run_inference(req.messages, req.temperature, req.top_k, req.top_p, req.max_tokens)
    return ChatResponse(
        id="chatcmpl-local",
        model=req.model,
        choices=[ChatChoice(index=0, message=Message(role="assistant", content=output), finish_reason="stop")]
    )
```

- **è°ƒç”¨ç¤ºä¾‹:**

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "tiny-llm-40m",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¹äºåŠ©äººçš„ä¸­æ–‡åŠ©æ‰‹"},
      {"role": "user", "content": "å¸®æˆ‘ç”¨é«˜ä¸­ç”Ÿèƒ½æ‡‚çš„è¯­è¨€è§£é‡Šäº‘åŸç”Ÿ"}
    ],
    "temperature": 0.7,
    "max_tokens": 200
  }'
```

---

## å¯æ‰“å°çš„â€œåŠ¨æ‰‹æ¸…å•â€

- **ç›®æ ‡ä¸é¢†åŸŸ:**
  - **ç”¨é€”:** ä¸­æ–‡èŠå¤©/æ•™è‚²é—®ç­”/å®¶åº­çŸ¥è¯†
  - **é£æ ¼:** ç®€æ´ã€ç¤¼è²Œã€ç»“æ„åŒ–
- **æ•°æ®æµæ°´çº¿:**
  - **æ”¶é›†:** é€šç”¨ä¸­æ–‡æ–‡æœ¬ + ä½ è‡ªåˆ¶é—®ç­”
  - **æ¸…æ´—:** å»é‡ã€å»å™ªã€ç»Ÿä¸€æ ¼å¼
  - **åˆ†è¯:** BPEï¼Œè¯è¡¨ 16k
- **æ¨¡å‹ä¸è¶…å‚:**
  - **ç»“æ„:** 8 å±‚ã€hidden=512ã€heads=8ã€ffn=2048ã€context=512
  - **è®­ç»ƒ:** batch=8ï¼ˆç´¯ç§¯åˆ° 64ï¼‰ã€AdamWã€lr=3e-4ã€fp32
  - **æ—¶é•¿:** é¢„è®­ 1â€“2 å¤©ï¼›SFT åŠå¤©åˆ°ä¸€å¤©
- **è¯„æµ‹ä¸å®‰å…¨:**
  - **æŒ‡æ ‡:** å›°æƒ‘åº¦ã€ç®€å•é—®ç­”é›†å‡†ç¡®ç‡
  - **å®‰å…¨:** æ•æ„Ÿè¯è¿‡æ»¤ã€æ‹’ç­”æ¨¡æ¿
- **éƒ¨ç½²ä¸ API:**
  - **æƒé‡:** safetensors
  - **ä¼˜åŒ–:** KV ç¼“å­˜ã€æµå¼è¾“å‡º
  - **æœåŠ¡:** FastAPIï¼Œé‰´æƒ+é™æµ+æ—¥å¿—

---

## è¿›é˜¶ä¸æ‰©å±•å»ºè®®

- **è’¸é¦åˆ°æ›´å°:** ç”¨ä½ è®­ç»ƒçš„æ¨¡å‹æ•™å¸ˆï¼Œè’¸é¦åˆ°æ›´è½»è§„æ ¼ï¼ˆå¦‚ hidden=384ï¼Œå‚æ•°æ›´å°‘ï¼‰
- **æŒç»­å¾®è°ƒ:** æŒç»­æ”¶é›†çœŸå®ä½¿ç”¨é—®ç­”ï¼Œå®šæœŸ SFT æå‡è´¨é‡
- **é•¿ä¸Šä¸‹æ–‡ç‰ˆæœ¬:** è‹¥éœ€è¦é•¿æ–‡æ”¯æŒï¼Œå¢è‡³ 1024â€“2048ï¼Œä½†ç•™æ„ CPU å†…å­˜å ç”¨
- **é‡åŒ–æ¨ç†:** æ¨ç†æ—¶åš INT8/INT4 é‡åŒ–ï¼Œæ˜¾è‘—é™å†…å­˜å ç”¨ï¼ˆè®­ç»ƒä»ç”¨ FP32ï¼‰

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥æŠŠè¿™å¥—è§„æ ¼å’Œæµç¨‹åšæˆä¸€é¡µ A4 çš„å¯æ‰“å°å¡ç‰‡ï¼ˆå«æ¨¡å‹è¶…å‚è¡¨ã€è®­ç»ƒæ—¥ç¨‹ã€API å­—æ®µè¡¨ï¼‰ï¼Œå¹¶æŒ‰â€œä¸­æ–‡æ•™è‚²é—®ç­”â€ç»™ä½ å‡†å¤‡ä¸€å¥—é«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒæ¨¡æ¿ä¸å°å‹è¯„æµ‹é›†ï¼Œç›´æ¥å¼€è®­ã€‚ä½ æ›´åå¥½ä¸­æ–‡ä¸“ç”¨è¿˜æ˜¯ä¸­è‹±æ··åˆè¯è¡¨ï¼Ÿ

## åŸºäºä¸Šé¢çš„å›ç­”ï¼Œè¯·ç»™æˆ‘ä»£ç å±‚é¢çš„å®ç°

ä¸‹é¢ç»™ä½ â€œå¯è¿è¡Œçš„æœ€å°å®Œæ•´ä»£ç â€ï¼Œæ¶µç›–æ•°æ®å‡†å¤‡ã€åˆ†è¯å™¨ã€æ¨¡å‹æ­å»ºã€é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒã€æ¨ç†ç”Ÿæˆã€ä»¥åŠ FastAPI å¯¹å¤–æä¾›æœåŠ¡ã€‚ä½ å¯ä»¥ç›´æ¥æŒ‰æ¨¡å—è½åœ°ï¼Œé€æ­¥è·‘é€šã€‚

---

## é¡¹ç›®ç»“æ„ä¸ä¾èµ–

- å»ºè®®ç»“æ„ï¼š

  - data/ï¼šè¯­æ–™ä¸æŒ‡ä»¤æ•°æ®
  - tokenizer/ï¼šåˆ†è¯å™¨æ–‡ä»¶
  - checkpoints/ï¼šæ¨¡å‹æƒé‡
  - app.pyï¼šAPI æœåŠ¡
  - train_pretrain.pyï¼šé¢„è®­ç»ƒ
  - train_sft.pyï¼šæŒ‡ä»¤å¾®è°ƒ
  - model.pyï¼šæ¨¡å‹å®šä¹‰ä¸æ¨ç†
  - build_tokenizer.pyï¼šæ„å»º BPE åˆ†è¯å™¨
  - generate.pyï¼šç¦»çº¿æ¨ç†

- ä¾èµ–ï¼š
  - python â‰¥ 3.9
  - pip install torch sentencepiece fastapi uvicorn pydantic

---

## 1. æ„å»ºåˆ†è¯å™¨ï¼ˆBPEï¼Œè¯è¡¨ 16kï¼‰

```python
# build_tokenizer.py
import sentencepiece as spm
import os

def build_tokenizer(input_txt="data/corpus.txt", model_prefix="tokenizer/spm", vocab_size=16000):
    os.makedirs("tokenizer", exist_ok=True)
    spm.SentencePieceTrainer.Train(
        input=input_txt,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type="bpe",
        character_coverage=0.9995,  # é€‚åˆä¸­è‹±æ··åˆ
        bos_id=1, eos_id=2, pad_id=0, unk_id=3,
        user_defined_symbols=["<|system|>", "<|user|>", "<|assistant|>", "<|sep|>"]
    )
    print("Tokenizer built:", model_prefix + ".model")

if __name__ == "__main__":
    build_tokenizer()
```

- å‡†å¤‡ data/corpus.txtï¼ˆ1â€“5GB æ¸…æ´—æ–‡æœ¬ï¼Œå…ˆç”¨å‡ ç™¾ MB ä¹Ÿå¯è·‘é€šï¼‰ã€‚

---

## 2. æ¨¡å‹å®šä¹‰ï¼ˆDecoder-only Transformerï¼Œçº¦ 40M å‚æ•°ï¼‰

```python
# model.py
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def forward(self, x):
        norm_x = x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)
        return self.weight * norm_x

class RotaryEmbedding:
    def __init__(self, dim, base=10000):
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.inv_freq = inv_freq
    def get_cos_sin(self, seq_len, device):
        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)
        freqs = torch.outer(t, self.inv_freq)
        cos, sin = freqs.cos(), freqs.sin()
        return cos, sin

def apply_rope(x, cos, sin):
    # x: (b, n_heads, seq_len, head_dim)
    head_dim = x.size(-1)
    x1 = x[..., :head_dim//2]
    x2 = x[..., head_dim//2:]
    x_rope = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)
    return x_rope

class Attention(nn.Module):
    def __init__(self, dim, n_heads, dropout=0.0):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        self.o = nn.Linear(dim, dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None, rope=None, kv_cache=None):
        B, T, C = x.size()
        qkv = self.qkv(x)  # (B, T, 3C)
        q, k, v = qkv.split(C, dim=-1)
        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)  # (B, H, T, D)
        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)
        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)

        if rope is not None:
            cos, sin = rope.get_cos_sin(T, x.device)
            cos = cos[None, None, :, :].repeat(B, self.n_heads, 1, 1)
            sin = sin[None, None, :, :].repeat(B, self.n_heads, 1, 1)
            q = apply_rope(q, cos, sin)
            k = apply_rope(k, cos, sin)

        # causal mask
        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, T, T)
        causal = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)
        att = att.masked_fill(causal == 0, float("-inf"))
        if mask is not None:
            att = att + mask
        att = F.softmax(att, dim=-1)
        att = self.dropout(att)
        y = att @ v  # (B, H, T, D)
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        return self.o(y)

class FeedForward(nn.Module):
    def __init__(self, dim, ffn_dim, dropout=0.0):
        super().__init__()
        self.w1 = nn.Linear(dim, ffn_dim)
        self.w2 = nn.Linear(ffn_dim, dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        return self.w2(self.dropout(F.silu(self.w1(x))))

class TransformerBlock(nn.Module):
    def __init__(self, dim, n_heads, ffn_dim, dropout=0.1):
        super().__init__()
        self.norm1 = RMSNorm(dim)
        self.attn = Attention(dim, n_heads, dropout=dropout)
        self.norm2 = RMSNorm(dim)
        self.ffn = FeedForward(dim, ffn_dim, dropout=dropout)
        self.rope = RotaryEmbedding(dim // n_heads)

    def forward(self, x, mask=None):
        x = x + self.attn(self.norm1(x), mask=mask, rope=self.rope)
        x = x + self.ffn(self.norm2(x))
        return x

class TinyLLM(nn.Module):
    def __init__(self, vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048, max_seq_len=512, dropout=0.1):
        super().__init__()
        self.tok_emb = nn.Embedding(vocab_size, dim)
        self.pos_emb = None  # ä½¿ç”¨RoPEï¼Œä¸éœ€è¦ç»å¯¹ä½ç½®
        self.blocks = nn.ModuleList([TransformerBlock(dim, n_heads, ffn_dim, dropout) for _ in range(n_layers)])
        self.norm = RMSNorm(dim)
        self.lm_head = nn.Linear(dim, vocab_size, bias=False)
        self.max_seq_len = max_seq_len

    def forward(self, idx):
        # idx: (B, T)
        x = self.tok_emb(idx)
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        logits = self.lm_head(x)  # (B, T, V)
        return logits

    @torch.no_grad()
    def generate(self, idx, max_new_tokens=128, temperature=0.7, top_k=40, top_p=0.9):
        self.eval()
        for _ in range(max_new_tokens):
            logits = self.forward(idx)[:, -1, :]  # (B, V)
            logits = logits / max(temperature, 1e-5)
            probs = F.softmax(logits, dim=-1)

            # nucleus + top-k filterï¼ˆç®€åŒ–ç‰ˆï¼‰
            if top_k is not None:
                topk_vals, topk_idx = torch.topk(probs, k=min(top_k, probs.size(-1)), dim=-1)
                filt = torch.zeros_like(probs)
                filt.scatter_(1, topk_idx, topk_vals)
                probs = filt
            if top_p is not None:
                sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)
                cum = torch.cumsum(sorted_probs, dim=-1)
                mask = cum <= top_p
                # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ª
                mask[..., 0] = True
                filt = torch.zeros_like(probs)
                filt.scatter_(1, sorted_idx, sorted_probs * mask)
                probs = filt
            probs = probs / probs.sum(dim=-1, keepdim=True)

            next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)
            idx = torch.cat([idx, next_token], dim=1)
            if idx.size(1) >= self.max_seq_len:
                break
        return idx
```

---

## 3. é¢„è®­ç»ƒï¼ˆè‡ªç›‘ç£ï¼Œä¸‹ä¸€ä¸ª token é¢„æµ‹ï¼‰

```python
# train_pretrain.py
import os
import torch
import random
import sentencepiece as spm
from torch.utils.data import Dataset, DataLoader
from model import TinyLLM
import torch.nn.functional as F
import torch.nn as nn
from torch.optim import AdamW

class TextDataset(Dataset):
    def __init__(self, txt_path, sp_model_path, seq_len=256):
        self.seq_len = seq_len
        self.sp = spm.SentencePieceProcessor(model_file=sp_model_path)
        with open(txt_path, "r", encoding="utf-8") as f:
            self.text = f.read()
        self.ids = self.sp.encode(self.text, out_type=int)
    def __len__(self):
        return max(1, len(self.ids) // self.seq_len - 1)
    def __getitem__(self, idx):
        i = idx * self.seq_len
        x = self.ids[i:i+self.seq_len]
        y = self.ids[i+1:i+self.seq_len+1]
        x = torch.tensor(x, dtype=torch.long)
        y = torch.tensor(y, dtype=torch.long)
        return x, y

def train_pretrain(
    corpus_path="data/corpus.txt",
    spm_path="tokenizer/spm.model",
    save_dir="checkpoints",
    vocab_size=16000,
    dim=512, n_layers=8, n_heads=8, ffn_dim=2048,
    seq_len=256, batch_size=8, lr=3e-4, epochs=1
):
    os.makedirs(save_dir, exist_ok=True)
    device = torch.device("cpu")
    dataset = TextDataset(corpus_path, spm_path, seq_len=seq_len)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

    model = TinyLLM(vocab_size=vocab_size, dim=dim, n_layers=n_layers, n_heads=n_heads,
                    ffn_dim=ffn_dim, max_seq_len=512, dropout=0.1).to(device)
    optim = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)
    scaler = None  # CPUä¸Šä¸éœ€è¦AMP
    print("Pretraining starts. Steps:", len(loader))

    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        for step, (x, y) in enumerate(loader):
            x = x.to(device)
            y = y.to(device)
            logits = model(x)  # (B, T, V)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
            optim.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optim.step()

            total_loss += loss.item()
            if (step + 1) % 100 == 0:
                ppl = math.exp(total_loss / 100)
                print(f"epoch {epoch} step {step+1}: loss={total_loss/100:.4f} pplâ‰ˆ{ppl:.2f}")
                total_loss = 0.0

        ckpt_path = os.path.join(save_dir, f"pretrain_epoch{epoch}.pt")
        torch.save({"model": model.state_dict()}, ckpt_path)
        print("Saved:", ckpt_path)

if __name__ == "__main__":
    import math
    train_pretrain(epochs=1)  # å…ˆè·‘é€šä¸€è½®
```

- å…ˆè·‘çŸ­åºåˆ—ï¼ˆ256ï¼‰æ›´çœèµ„æºï¼Œä¹‹åå¯æé«˜åˆ° 512 ç»§ç»­è®­ç»ƒã€‚

---

## 4. æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰

å‡†å¤‡ data/sft.jsonlï¼Œæ¯è¡Œä¸€ä¸ªæ ·æœ¬ï¼š

```json
{"system": "ä½ æ˜¯ä¹äºåŠ©äººçš„åŠ©æ‰‹", "user": "è§£é‡Šäº‘åŸç”Ÿ", "assistant": "äº‘åŸç”Ÿæ˜¯ä¸ºäº‘è®¾è®¡çš„è½¯ä»¶æ–¹å¼..."}
{"system": "ä½ æ˜¯ä¹äºåŠ©äººçš„åŠ©æ‰‹", "user": "ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ", "assistant": "å¤§è¯­è¨€æ¨¡å‹æ˜¯é€šè¿‡å­¦ä¹ æµ·é‡æ–‡æœ¬..."}
```

```python
# train_sft.py
import os, json
import torch
import sentencepiece as spm
from torch.utils.data import Dataset, DataLoader
from model import TinyLLM
import torch.nn.functional as F
from torch.optim import AdamW

PROMPT_TPL = "<|system|>\n{system}\n<|sep|>\n<|user|>\n{user}\n<|sep|>\n<|assistant|>\n"

class SFTDataset(Dataset):
    def __init__(self, jsonl_path, sp_model_path, seq_len=512):
        self.sp = spm.SentencePieceProcessor(model_file=sp_model_path)
        self.seq_len = seq_len
        self.samples = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                obj = json.loads(line)
                prompt = PROMPT_TPL.format(system=obj["system"], user=obj["user"])
                full = prompt + obj["assistant"]
                ids = self.sp.encode(full, out_type=int)
                if len(ids) > seq_len:
                    ids = ids[:seq_len]
                # labelä½ç½®ï¼šä»assistantå¼€å¤´å¼€å§‹è®­ç»ƒï¼›system/useréƒ¨åˆ†è®¾ç½®ä¸º-100å¿½ç•¥
                prompt_ids = self.sp.encode(prompt, out_type=int)
                labels = [-100] * len(prompt_ids) + ids[len(prompt_ids):]
                if len(labels) < len(ids):
                    labels = labels + [-100] * (len(ids) - len(labels))
                self.samples.append((ids, labels))
    def __len__(self):
        return len(self.samples)
    def __getitem__(self, idx):
        ids, labels = self.samples[idx]
        x = torch.tensor(ids, dtype=torch.long)
        y = torch.tensor(labels, dtype=torch.long)
        pad_len = self.seq_len - len(ids)
        if pad_len > 0:
            x = torch.cat([x, torch.zeros(pad_len, dtype=torch.long)])
            y = torch.cat([y, torch.full((pad_len,), -100, dtype=torch.long)])
        return x, y

def train_sft(
    sft_path="data/sft.jsonl",
    spm_path="tokenizer/spm.model",
    load_ckpt="checkpoints/pretrain_epoch0.pt",
    save_dir="checkpoints",
    batch_size=4, lr=1e-4, epochs=1, seq_len=512,
    vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048
):
    os.makedirs(save_dir, exist_ok=True)
    device = torch.device("cpu")
    ds = SFTDataset(sft_path, spm_path, seq_len=seq_len)
    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)

    model = TinyLLM(vocab_size=vocab_size, dim=dim, n_layers=n_layers, n_heads=n_heads,
                    ffn_dim=ffn_dim, max_seq_len=seq_len, dropout=0.1)
    if load_ckpt and os.path.exists(load_ckpt):
        sd = torch.load(load_ckpt, map_location="cpu")["model"]
        model.load_state_dict(sd, strict=False)
        print("Loaded pretrain:", load_ckpt)
    model.to(device)

    optim = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)
    for epoch in range(epochs):
        model.train()
        total = 0.0
        for step, (x, y) in enumerate(dl):
            x, y = x.to(device), y.to(device)
            logits = model(x)  # (B, T, V)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-100)
            optim.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optim.step()
            total += loss.item()
            if (step + 1) % 50 == 0:
                print(f"epoch {epoch} step {step+1}: sft_loss={total/50:.4f}")
                total = 0.0
        ckpt_path = os.path.join(save_dir, f"sft_epoch{epoch}.pt")
        torch.save({"model": model.state_dict()}, ckpt_path)
        print("Saved:", ckpt_path)

if __name__ == "__main__":
    train_sft(epochs=1)
```

---

## 5. ç¦»çº¿æ¨ç†ä¸æµ‹è¯•

```python
# generate.py
import torch
import sentencepiece as spm
from model import TinyLLM

def build_prompt(system, user):
    return f"<|system|>\n{system}\n<|sep|>\n<|user|>\n{user}\n<|sep|>\n<|assistant|>\n"

def run(model_ckpt, spm_path, system, user, max_new_tokens=200):
    device = torch.device("cpu")
    sp = spm.SentencePieceProcessor(model_file=spm_path)
    model = TinyLLM(vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048, max_seq_len=512)
    sd = torch.load(model_ckpt, map_location="cpu")["model"]
    model.load_state_dict(sd, strict=False)
    model.eval()

    prompt = build_prompt(system, user)
    ids = sp.encode(prompt, out_type=int)
    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)  # (1, T)
    out = model.generate(x, max_new_tokens=max_new_tokens, temperature=0.7, top_k=40, top_p=0.9)
    text = sp.decode(out[0].tolist())
    # åªæˆªå–assistantéƒ¨åˆ†
    ans = text.split("<|assistant|>\n")[-1]
    print(ans)

if __name__ == "__main__":
    run(
        model_ckpt="checkpoints/sft_epoch0.pt",
        spm_path="tokenizer/spm.model",
        system="ä½ æ˜¯ä¹äºåŠ©äººçš„ä¸­æ–‡åŠ©æ‰‹",
        user="è¯·ç”¨é«˜ä¸­ç”Ÿèƒ½æ‡‚çš„è¯­è¨€è§£é‡Šäº‘åŸç”Ÿ"
    )
```

---

## 6. FastAPI å¯¹å¤–æä¾›æœåŠ¡

```python
# app.py
import torch
import sentencepiece as spm
from fastapi import FastAPI, Header, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from model import TinyLLM

SPM_PATH = "tokenizer/spm.model"
CKPT_PATH = "checkpoints/sft_epoch0.pt"

sp = spm.SentencePieceProcessor(model_file=SPM_PATH)
model = TinyLLM(vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048, max_seq_len=512)
sd = torch.load(CKPT_PATH, map_location="cpu")["model"]
model.load_state_dict(sd, strict=False)
model.eval()

app = FastAPI(title="TinyLLM API", version="1.0.0")

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str = "tiny-llm-40m"
    messages: List[Message]
    temperature: Optional[float] = 0.7
    top_k: Optional[int] = 40
    top_p: Optional[float] = 0.9
    max_tokens: Optional[int] = 256
    stream: Optional[bool] = False

class ChatChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class ChatResponse(BaseModel):
    id: str
    model: str
    choices: List[ChatChoice]

def build_prompt_from_messages(messages: List[Message]) -> str:
    system = ""
    user = ""
    for m in messages:
        if m.role == "system":
            system = m.content
        elif m.role == "user":
            user = m.content
    return f"<|system|>\n{system}\n<|sep|>\n<|user|>\n{user}\n<|sep|>\n<|assistant|>\n"

@app.post("/v1/chat/completions", response_model=ChatResponse)
def chat(req: ChatRequest, authorization: Optional[str] = Header(None)):
    if authorization != "Bearer YOUR_API_KEY":
        raise HTTPException(status_code=401, detail="Invalid API key")

    prompt = build_prompt_from_messages(req.messages)
    ids = sp.encode(prompt, out_type=int)
    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)
    out = model.generate(x, max_new_tokens=req.max_tokens, temperature=req.temperature, top_k=req.top_k, top_p=req.top_p)
    text = sp.decode(out[0].tolist())
    ans = text.split("<|assistant|>\n")[-1]

    return ChatResponse(
        id="chatcmpl-local",
        model=req.model,
        choices=[ChatChoice(index=0, message=Message(role="assistant", content=ans), finish_reason="stop")]
    )
```

- å¯åŠ¨æœåŠ¡ï¼š`uvicorn app:app --host 0.0.0.0 --port 8000`
- è°ƒç”¨ç¤ºä¾‹ï¼š

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "tiny-llm-40m",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¹äºåŠ©äººçš„ä¸­æ–‡åŠ©æ‰‹"},
      {"role": "user", "content": "è¯·ç”¨é«˜ä¸­ç”Ÿèƒ½æ‡‚çš„è¯­è¨€è§£é‡Šäº‘åŸç”Ÿ"}
    ],
    "temperature": 0.7,
    "max_tokens": 200
  }'
```

---

## 7. æ•°æ®å‡†å¤‡ä¸æ¸…æ´—ç¤ºä¾‹

- è¯­æ–™æ±‡æ€»è„šæœ¬ï¼ˆåˆå¹¶æ–‡æœ¬ã€ç®€å•æ¸…æ´—ï¼‰ï¼š

```python
# data_prepare.py
import os, re

def clean_line(s: str):
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[^\x09\x0A\x0D\x20-\x7E\u4e00-\u9fffï¼Œã€‚ï¼Ÿï¼ã€ï¼šï¼›â€œâ€â€˜â€™ï¼ˆï¼‰ã€Šã€‹â€”â€¦]", "", s)
    return s

def merge_corpus(src_dir="data/raw", out_path="data/corpus.txt"):
    with open(out_path, "w", encoding="utf-8") as out:
        for root, _, files in os.walk(src_dir):
            for fn in files:
                if fn.endswith(".txt"):
                    with open(os.path.join(root, fn), "r", encoding="utf-8", errors="ignore") as f:
                        for line in f:
                            line = clean_line(line)
                            if len(line) > 1:
                                out.write(line + "\n")
    print("Merged to:", out_path)

if __name__ == "__main__":
    merge_corpus()
```

---

## 8. è®­ç»ƒå»ºè®®ä¸èµ„æºæ§åˆ¶

- ä½¿ç”¨çŸ­åºåˆ—é¢„çƒ­ï¼šå…ˆ seq_len=256 è®­ç»ƒä¸€è½®ï¼Œå†å‡åˆ° 512 ç»§ç»­ã€‚
- å°æ‰¹é‡ + æ¢¯åº¦è£å‰ªï¼šbatch=4â€“8ï¼Œclip_grad_norm=1.0ã€‚
- å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­ã€‚
- æŒ‡ä»¤æ•°æ®è´¨é‡ä¼˜å…ˆäºæ•°é‡ï¼šå…ˆ 5kâ€“10k é«˜è´¨é‡æ ·æœ¬æ•ˆæœæ›´ç¨³ã€‚

---

## 9. ä¸‹ä¸€æ­¥å¯é€‰å¢å¼º

- é‡åŒ–æ¨ç†ï¼ˆINT8/INT4ï¼‰ï¼šå‡å°‘å†…å­˜å ç”¨ï¼ˆè®­ç»ƒä»ç”¨ FP32ï¼‰ã€‚
- è’¸é¦åˆ°æ›´å°è§„æ ¼ï¼šdim=384, n_layers=6ï¼Œè¿›ä¸€æ­¥æé€Ÿã€‚
- å¢åŠ æ‹’ç­”ä¸å®‰å…¨è¿‡æ»¤ï¼šåœ¨ app.py è¿”å›å‰åšå…³é”®è¯/ç­–ç•¥æ£€æŸ¥ã€‚
- æµå¼è¾“å‡ºï¼šåœ¨ FastAPI é‡Œç”¨æœåŠ¡å™¨æ¨é€äº‹ä»¶ï¼ˆSSEï¼‰é€ token è¿”å›ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ä¸ºä½ çš„ç›®æ ‡é¢†åŸŸï¼ˆæ¯”å¦‚â€œä¸­æ–‡æ•™è‚²é—®ç­”â€ï¼‰ç”Ÿæˆä¸€å¥—é«˜è´¨é‡çš„æŒ‡ä»¤æ¨¡æ¿å’Œå°å‹è¯„æµ‹é›† JSONLï¼Œè®©ä½ ç›´æ¥è®­ç»ƒå¹¶éªŒè¯æ•ˆæœã€‚ä½ æ›´åå¥½å“ªäº›ä¸»é¢˜å’Œé¢˜å‹ï¼Ÿ

- https://github.com/zheniu/stochastic-cslr
- https://github.com/zheniu/stochastic-cslr/blob/main/stochastic_cslr/model/xfmr.py

## å¯¹äºè®­ç»ƒæ•°æ®ï¼Œæœ‰å¼€æºçš„æ•°æ®é›†å—ï¼Ÿ

**æœ‰çš„ âœ…ï¼Œç›®å‰å·²ç»æœ‰å¤§é‡å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–é¢„è®­ç»ƒè¯­æ–™ã€æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€åå¥½æ•°æ®å’Œè¯„æµ‹æ•°æ®ã€‚ä½ å®Œå…¨å¯ä»¥åŸºäºè¿™äº›å¼€æºæ•°æ®æ¥è·‘é€šâ€œå°æ¨¡å‹â€çš„è®­ç»ƒæµç¨‹ã€‚**

---

## ğŸ”‘ å¸¸è§çš„å¼€æºæ•°æ®é›†ç±»å‹

### 1. é¢„è®­ç»ƒè¯­æ–™ï¼ˆåŸºç¡€è¯­è¨€èƒ½åŠ›ï¼‰

- **ä¸­æ–‡ç»´åŸºç™¾ç§‘è¯­æ–™**ï¼šç»è¿‡æ¸…æ´—çš„ä¸­æ–‡ç™¾ç§‘æ¡ç›®ï¼Œå¸¸ç”¨äºä¸­æ–‡åŸºç¡€é¢„è®­ç»ƒã€‚
- **FineWeb / Ultra-FineWeb**ï¼šå¤§è§„æ¨¡ä¸­è‹±æ–‡ç½‘é¡µæ¸…æ´—è¯­æ–™ï¼Œè´¨é‡è¾ƒé«˜ï¼Œé€‚åˆåšå°è§„æ¨¡é¢„è®­ç»ƒå­é›†ã€‚
- **Chinese-Cosmopedia**ï¼šåˆæˆçš„ä¸­æ–‡ç™¾ç§‘ç±»æ•°æ®ï¼Œè¦†ç›–é¢å¹¿ã€‚

### 2. æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼ˆè®©æ¨¡å‹â€œå¬æ‡‚äººè¯â€ï¼‰

- **Alpaca / Alpaca-GPT4 ä¸­æ–‡ç‰ˆ**ï¼šåŸºäºæŒ‡ä»¤-å›ç­”å¯¹çš„å¼€æºæ•°æ®é›†ï¼Œå¸¸ç”¨äºå°æ¨¡å‹æŒ‡ä»¤å¾®è°ƒã€‚
- **Belle æ•°æ®é›†**ï¼šä¸­æ–‡æŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼ŒåŒ…å«é—®ç­”ã€å†™ä½œã€æ¨ç†ç­‰å¤šç§ä»»åŠ¡ã€‚
- **Baize / ShareGPT**ï¼šå¯¹è¯æ•°æ®é›†ï¼Œé€‚åˆè®­ç»ƒèŠå¤©é£æ ¼çš„æ¨¡å‹ã€‚

### 3. åå¥½æ•°æ®ï¼ˆå¯¹é½äººç±»ä»·å€¼ï¼‰

- **OpenAssistant Conversations**ï¼šå¤šè½®å¯¹è¯æ•°æ®ï¼Œå¸¦æœ‰äººç±»åå¥½æ ‡æ³¨ã€‚
- **HH-RLHF**ï¼šAnthropic å‘å¸ƒçš„äººç±»åå¥½æ•°æ®é›†ï¼Œå¸¸ç”¨äº RLHFã€‚

### 4. è¯„æµ‹æ•°æ®ï¼ˆæµ‹è¯•æ¨¡å‹æ•ˆæœï¼‰

- **CMMLU**ï¼šä¸­æ–‡å¤šä»»åŠ¡è¯„æµ‹é›†ï¼Œè¦†ç›–å­¦ç§‘çŸ¥è¯†ã€‚
- **C-Eval**ï¼šä¸­æ–‡è€ƒè¯•é¢˜å‹è¯„æµ‹é›†ï¼Œé€‚åˆæµ‹è¯•æ¨¡å‹åœ¨æ•™è‚²ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

---

## ğŸ“š æ¨èèµ„æºæ•´ç†

- **çŸ¥ä¹æ•´ç†ï¼š444 ä¸ªå¤§æ¨¡å‹æ•°æ®é›†ä¸‹è½½åœ°å€**
- **CSDNï¼šé«˜è´¨é‡ä¸­æ–‡å¤§æ¨¡å‹æ•°æ®é›†åˆé›†**
- **GitHubï¼šå¤§è¯­è¨€æ¨¡å‹é«˜è´¨é‡æ•°æ®é›†æ±‡æ€»**

è¿™äº›èµ„æºé‡Œä¸ä»…æœ‰ä¸‹è½½é“¾æ¥ï¼Œè¿˜å¯¹æ•°æ®é›†åšäº†åˆ†ç±»ï¼ˆé¢„è®­ç»ƒã€æŒ‡ä»¤ã€åå¥½ã€è¯„æµ‹ï¼‰ï¼Œä½ å¯ä»¥æ ¹æ®ç¡¬ä»¶æ¡ä»¶æŒ‘é€‰å°è§„æ¨¡å­é›†æ¥å®éªŒã€‚

---

## ğŸ› ï¸ ç»™ä½ çš„å°æœºå­å»ºè®®

- **é¢„è®­ç»ƒ**ï¼šä¸è¦ç›´æ¥ç”¨ TB çº§æ•°æ®ï¼Œå…ˆç”¨ **å‡ ç™¾ MB çš„ä¸­æ–‡ç»´åŸºè¯­æ–™** æˆ– **FineWeb å­é›†**ã€‚
- **æŒ‡ä»¤å¾®è°ƒ**ï¼šä¼˜å…ˆé€‰æ‹© **Belle** æˆ– **Alpaca ä¸­æ–‡ç‰ˆ**ï¼Œå‡ ä¸‡æ¡å°±èƒ½æ˜¾è‘—æå‡æ•ˆæœã€‚
- **è¯„æµ‹**ï¼šç”¨ **C-Eval** æˆ– **CMMLU** çš„å°æ ·æœ¬ï¼Œå¿«é€ŸéªŒè¯æ¨¡å‹æ˜¯å¦èƒ½å›ç­”å¸¸è¯†/å­¦ç§‘é—®é¢˜ã€‚

---

ğŸ‘‰ æ€»ç»“ï¼š**å®Œå…¨æœ‰å¼€æºæ•°æ®é›†å¯ç”¨**ï¼Œä½ å¯ä»¥å…ˆç”¨ä¸­æ–‡ç»´åŸº + Belle æŒ‡ä»¤æ•°æ®ï¼Œè·‘é€šä¸€ä¸ªâ€œå°è€Œç¾â€çš„æ¨¡å‹ï¼Œå†é€æ­¥æ‰©å±•ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä»½ **â€œé€‚åˆ 16GB å†…å­˜ CPU è®­ç»ƒçš„å°è§„æ¨¡æ•°æ®é›†æ¸…å• + ä¸‹è½½å…¥å£â€**ï¼Œç›´æ¥ç»™ä½ å¯ç”¨çš„ç»„åˆæ–¹æ¡ˆï¼Ÿ

---

- [çŸ¥ä¹ï¼šã€Šå¤§æ¨¡å‹æ•°æ®é›†å…¨é¢æ•´ç†ï¼š444 ä¸ªæ•°æ®é›†ä¸‹è½½åœ°å€ã€‹](https://zhuanlan.zhihu.com/p/19630799254)
- [CSDNï¼šã€Šæœ€å…¨-é«˜è´¨é‡å¤§æ¨¡å‹ä¸­æ–‡æ•°æ®é›†ã€‹](https://blog.csdn.net/u011559552/article/details/142152619)
- [GitHub Issueï¼šã€Šå¤§è¯­è¨€æ¨¡å‹é«˜è´¨é‡æ•°æ®é›†æ±‡æ€»ã€‹](https://github.com/ninehills/blog/issues/129)

## å¯¹äºæ„å»ºè‡ªå·±çš„å¤§æ¨¡å‹ï¼ŒæŒ‰ç…§ä¸Šé¢ 1.2.3â€¦â€¦çš„é¡ºåºæ‰§è¡Œ python æ–‡ä»¶å°±è¡Œäº†å—ï¼Ÿæ¨¡å‹è®­ç»ƒå®Œæˆåçš„äº§å‡ºç‰©æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•ä½¿ç”¨æ¨¡å‹ï¼Ÿå¦‚ä½•å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Ÿ

éå¸¸å¥½çš„é—®é¢˜ ğŸ‘ æˆ‘æ¥å¸®ä½ æŠŠæ•´ä¸ªæµç¨‹æ¢³ç†æ¸…æ¥šï¼Œè®©ä½ çŸ¥é“â€œæ‰§è¡Œé¡ºåº â†’ äº§å‡ºç‰© â†’ ä½¿ç”¨æ–¹å¼ â†’ å¾®è°ƒæ–¹æ³•â€ã€‚

---

## ğŸ› ï¸ æ‰§è¡Œé¡ºåº

ä½ å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªæµæ°´çº¿ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œå³å¯ï¼š

1. **æ•°æ®å‡†å¤‡**
   - æ¸…æ´—è¯­æ–™ï¼Œåˆå¹¶æˆ `corpus.txt`
   - æ„å»ºåˆ†è¯å™¨ï¼ˆ`build_tokenizer.py`ï¼‰
2. **é¢„è®­ç»ƒ**
   - æ‰§è¡Œ `train_pretrain.py`ï¼Œå¾—åˆ°åŸºç¡€è¯­è¨€æ¨¡å‹æƒé‡
3. **æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰**
   - å‡†å¤‡ `sft.jsonl`ï¼ˆé—®ç­”å¯¹ï¼‰
   - æ‰§è¡Œ `train_sft.py`ï¼Œåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒ
4. **æ¨ç†æµ‹è¯•**
   - ç”¨ `generate.py` åŠ è½½æƒé‡ï¼Œè¾“å…¥é—®é¢˜ï¼Œçœ‹çœ‹è¾“å‡ºæ•ˆæœ
5. **API éƒ¨ç½²**
   - å¯åŠ¨ `app.py`ï¼Œé€šè¿‡ HTTP æ¥å£è°ƒç”¨æ¨¡å‹

---

## ğŸ“¦ æ¨¡å‹è®­ç»ƒå®Œæˆåçš„äº§å‡ºç‰©

- **åˆ†è¯å™¨æ–‡ä»¶**ï¼š
  - `tokenizer/spm.model` å’Œ `spm.vocab`
  - ç”¨æ¥æŠŠæ–‡å­—è½¬æˆ tokenï¼Œå†æŠŠ token è½¬å›æ–‡å­—
- **æ¨¡å‹æƒé‡æ–‡ä»¶**ï¼š
  - `checkpoints/pretrain_epochX.pt`ï¼ˆé¢„è®­ç»ƒé˜¶æ®µï¼‰
  - `checkpoints/sft_epochX.pt`ï¼ˆæŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼‰
  - é‡Œé¢ä¿å­˜äº†æ¨¡å‹çš„å‚æ•°ï¼ˆæ•°åƒä¸‡ä¸ªæµ®ç‚¹æ•°ï¼‰
- **æ—¥å¿—/æ›²çº¿**ï¼ˆå¯é€‰ï¼‰ï¼š
  - è®­ç»ƒæ—¶çš„ lossã€å›°æƒ‘åº¦ï¼ˆpplï¼‰å˜åŒ–

---

## ğŸš€ å¦‚ä½•ä½¿ç”¨æ¨¡å‹

1. **ç¦»çº¿æ¨ç†**
   - ç”¨ `generate.py` åŠ è½½ `sft_epochX.pt`
   - è¾“å…¥ä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ä¼šç”Ÿæˆå›ç­”
2. **API è°ƒç”¨**
   - å¯åŠ¨ `uvicorn app:app --reload`
   - ç”¨ `curl` æˆ– Python requests è°ƒç”¨ `/v1/chat/completions`
   - å¤–éƒ¨ç¨‹åºå°±èƒ½åƒè°ƒç”¨ OpenAI API ä¸€æ ·ä½¿ç”¨ä½ çš„æ¨¡å‹

---

## ğŸ”§ å¦‚ä½•å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ

æœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š

### 1. **ç»§ç»­æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰**

- é€‚åˆï¼šä½ æœ‰æ–°çš„é—®ç­”æ•°æ®ï¼ˆæ¯”å¦‚æŸä¸ªä¸“ä¸šé¢†åŸŸï¼‰
- åšæ³•ï¼š
  - å‡†å¤‡æ–°çš„ `sft.jsonl`
  - åŠ è½½å·²æœ‰çš„ `sft_epochX.pt`
  - å†è·‘ `train_sft.py`ï¼Œç»§ç»­è®­ç»ƒå‡ è½®
- æ•ˆæœï¼šæ¨¡å‹ä¼šæ›´æ‡‚ä½ çš„é¢†åŸŸé—®é¢˜

### 2. **è½»é‡å¾®è°ƒï¼ˆLoRA / Adapterï¼‰**

- é€‚åˆï¼šç¡¬ä»¶æœ‰é™ï¼Œä¸æƒ³å…¨é‡æ›´æ–°å‚æ•°
- åšæ³•ï¼š
  - å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªè®­ç»ƒå°‘é‡â€œé€‚é…å±‚â€
  - è®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ˜¾å­˜/å†…å­˜å ç”¨å°
- æ•ˆæœï¼šèƒ½å¿«é€Ÿé€‚é…æ–°ä»»åŠ¡ï¼Œä½†éœ€è¦é¢å¤–ä»£ç æ”¯æŒ

---

## âœ… æ€»ç»“

- **æ‰§è¡Œé¡ºåº**ï¼šæ•°æ® â†’ åˆ†è¯å™¨ â†’ é¢„è®­ç»ƒ â†’ å¾®è°ƒ â†’ æ¨ç† â†’ API
- **äº§å‡ºç‰©**ï¼šåˆ†è¯å™¨æ–‡ä»¶ + æ¨¡å‹æƒé‡
- **ä½¿ç”¨æ–¹å¼**ï¼šç¦»çº¿æ¨ç† or API è°ƒç”¨
- **å¾®è°ƒæ–¹æ³•**ï¼šç»§ç»­ SFTï¼ˆå…¨é‡ï¼‰æˆ– LoRAï¼ˆè½»é‡ï¼‰

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œå¤§æ¨¡å‹è®­ç»ƒä¸ä½¿ç”¨æµç¨‹å›¾â€**ï¼ˆä»æ•°æ®åˆ° API çš„å…¨é“¾è·¯ï¼‰ï¼Œè¿™æ ·ä½ å¯ä»¥ä¸€çœ¼çœ‹åˆ°æ¯ä¸€æ­¥çš„è¾“å…¥ã€è¾“å‡ºå’Œäº§å‡ºç‰©ï¼Ÿ

## ä¸€èˆ¬æ¥è®²æç¤ºè¯çš„è®¾è®¡å’Œæ¨¡å‹æ¯æ¯ç›¸å…³ï¼Œå¦‚ä½•è®¾è®¡æç¤ºè¯æˆ–è€…å¦‚ä½•è®¾è®¡æ¨¡å‹æ‰èƒ½è®©ä¸¤è€…é…åˆè¾¾åˆ°æœ€ä¼˜ï¼Ÿæœ‰ä»€ä¹ˆæ ‡å‡†å—ï¼Ÿ

**ç®€ç­”ç‰ˆ**ï¼šæç¤ºè¯è®¾è®¡å’Œæ¨¡å‹ç»“æ„ç¡®å®æ˜¯â€œäº’ç›¸æˆå°±â€çš„å…³ç³»ã€‚æ²¡æœ‰ä¸€ä¸ªæ”¾ä¹‹å››æµ·è€Œçš†å‡†çš„â€œå”¯ä¸€æ ‡å‡†â€ï¼Œä½†æœ‰ä¸€å¥—è¢«å¹¿æ³›è®¤å¯çš„**æç¤ºè¯å·¥ç¨‹åŸåˆ™**å’Œ**æ¨¡å‹å¯¹é½ç­–ç•¥**ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**æç¤ºè¯è¦æ¸…æ™°ã€ç»“æ„åŒ–ã€å¯é¢„æµ‹ï¼›æ¨¡å‹è¦åœ¨è®­ç»ƒé˜¶æ®µå°±å­¦ä¼šç†è§£è¿™ç§ç»“æ„åŒ–è¾“å…¥**ã€‚

---

## ğŸ¯ æç¤ºè¯ä¸æ¨¡å‹çš„é…åˆé€»è¾‘

1. **æç¤ºè¯æ˜¯æ¥å£**ï¼šå®ƒæ˜¯äººç±»æ„å›¾ â†’ æ¨¡å‹è¾“å‡ºçš„æ¡¥æ¢ã€‚
2. **æ¨¡å‹æ˜¯è§£é‡Šå™¨**ï¼šå®ƒèƒ½å¦ç†è§£æç¤ºè¯ï¼Œå–å†³äºè®­ç»ƒæ—¶æ˜¯å¦è§è¿‡ç±»ä¼¼çš„æ ¼å¼ã€‚
3. **æœ€ä½³æ•ˆæœ**ï¼šå½“æç¤ºè¯çš„é£æ ¼ä¸æ¨¡å‹çš„è®­ç»ƒ/å¾®è°ƒæ•°æ®ä¿æŒä¸€è‡´æ—¶ï¼Œè¾“å‡ºæœ€ç¨³å®šã€‚

---

## ğŸ§© æç¤ºè¯è®¾è®¡çš„å…³é”®è¦ç´ 

ä¸€ä¸ªé«˜æ•ˆæç¤ºè¯é€šå¸¸åŒ…å« 5 ä¸ªéƒ¨åˆ†ï¼š

- **æŒ‡ä»¤ (Instruction)**ï¼šæ˜ç¡®å‘Šè¯‰æ¨¡å‹è¦åšä»€ä¹ˆ
- **ä¸Šä¸‹æ–‡ (Context)**ï¼šæä¾›å¿…è¦çš„èƒŒæ™¯ä¿¡æ¯
- **ç¤ºä¾‹ (Examples)**ï¼šç»™å‡ºå°‘é‡èŒƒä¾‹ï¼ˆfew-shotï¼‰
- **è¾“å…¥ (Input)**ï¼šéœ€è¦å¤„ç†çš„å…·ä½“å†…å®¹
- **è¾“å‡ºè¦æ±‚ (Output format)**ï¼šæŒ‡å®šæ ¼å¼ã€é£æ ¼ã€é•¿åº¦

ğŸ‘‰ ä¸¾ä¾‹ï¼š

```
ä½œä¸ºä¸€åé«˜ä¸­ç‰©ç†è€å¸ˆï¼Œè¯·ç”¨3-5å¥è¯è§£é‡Šâ€œä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„â€ã€‚
è¦æ±‚ï¼šé€æ­¥æ¨ç†ï¼Œæœ€åç”¨ä¸€å¥è¯æ€»ç»“ã€‚
```

---

## ğŸ—ï¸ æ¨¡å‹è®¾è®¡ä¸è®­ç»ƒçš„é…åˆç‚¹

- **é¢„è®­ç»ƒé˜¶æ®µ**ï¼šæ¨¡å‹å­¦ä¼šè¯­è¨€è§„å¾‹ï¼Œä½†å¯¹â€œæŒ‡ä»¤â€ä¸æ•æ„Ÿã€‚
- **æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰**ï¼šç”¨å¤§é‡â€œæŒ‡ä»¤-å›ç­”â€å¯¹è®­ç»ƒï¼Œè®©æ¨¡å‹ä¹ æƒ¯ç»“æ„åŒ–æç¤ºã€‚
- **RLHF/åå¥½ä¼˜åŒ–**ï¼šè®©æ¨¡å‹æ›´ç¬¦åˆäººç±»å¯¹â€œå¥½ç­”æ¡ˆâ€çš„åå¥½ï¼ˆç¤¼è²Œã€é€»è¾‘ã€ç®€æ´ï¼‰ã€‚
- **ä¸Šä¸‹æ–‡çª—å£è®¾è®¡**ï¼šå¦‚æœæ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦çŸ­ï¼Œæç¤ºè¯å°±è¦ç®€æ´ï¼›é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åˆ™èƒ½å®¹çº³æ›´å¤šç¤ºä¾‹ã€‚

---

## ğŸ“ æœ‰æ²¡æœ‰â€œæ ‡å‡†â€ï¼Ÿ

ä¸¥æ ¼æ„ä¹‰ä¸Šæ²¡æœ‰ç»Ÿä¸€å›½é™…æ ‡å‡†ï¼Œä½†æœ‰ä¸€äº›**æœ€ä½³å®è·µæ¡†æ¶**ï¼š

- **OpenAI å…­å¤§ç­–ç•¥**ï¼šæ¸…æ™°æŒ‡ä»¤ã€è§’è‰²è®¾å®šã€åˆ†éš”ç¬¦ã€åˆ†æ­¥éª¤ã€ç¤ºä¾‹ã€è¾“å‡ºçº¦æŸã€‚
- **Chain of Thought (CoT)**ï¼šè¦æ±‚æ¨¡å‹é€æ­¥æ¨ç†ï¼Œæå‡é€»è¾‘æ€§ã€‚
- **Few-shot / Zero-shot**ï¼šé€šè¿‡ç¤ºä¾‹æˆ–ç›´æ¥æŒ‡ä»¤å¼•å¯¼ã€‚
- **è§’è‰²æ‰®æ¼” (Role prompting)**ï¼šç»™æ¨¡å‹ä¸€ä¸ªèº«ä»½ï¼Œæå‡é£æ ¼ä¸€è‡´æ€§ã€‚

---

## âœ… æ€»ç»“

- **æç¤ºè¯è¦ç»“æ„åŒ–ã€æ˜ç¡®ã€å¯é¢„æµ‹**
- **æ¨¡å‹è¦åœ¨è®­ç»ƒä¸­æ¥è§¦è¿‡ç±»ä¼¼æ ¼å¼**
- **æœ€ä½³æ•ˆæœæ¥è‡ªâ€œæç¤ºè¯å·¥ç¨‹ + æŒ‡ä»¤å¾®è°ƒâ€çš„åŒé‡é…åˆ**
- **æ²¡æœ‰å”¯ä¸€æ ‡å‡†ï¼Œä½†æœ‰æˆç†Ÿæ¡†æ¶ï¼ˆå¦‚ OpenAI å…­å¤§ç­–ç•¥ã€CoTã€Few-shotï¼‰å¯éµå¾ª**

---

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä»½ **â€œæç¤ºè¯è®¾è®¡é€ŸæŸ¥è¡¨â€**ï¼ˆåŒ…å«å¸¸è§æ¡†æ¶ã€é€‚ç”¨åœºæ™¯ã€ç¤ºä¾‹ï¼‰ï¼Œæ–¹ä¾¿ä½ åœ¨åšæ¨¡å‹å®éªŒæ—¶å¿«é€Ÿå¥—ç”¨ï¼Ÿ

- [çŸ¥ä¹ï¼šã€Šæç¤ºè¯å·¥ç¨‹ä¸ä¼˜åŒ–æŠ€å·§ã€‹](https://zhuanlan.zhihu.com/p/1945389672600672219)
- [åšå®¢å›­ï¼šã€Šæç¤ºè¯å·¥ç¨‹æ–¹æ³•åŠæ¡†æ¶ã€‹](https://www.cnblogs.com/tlnshuju/p/19091427)

## å¤§æ¨¡å‹ä¸­çš„æ¸©åº¦å‚æ•°æ˜¯å•¥æ„æ€ï¼Ÿä¸ºä»€ä¹ˆè¦æ·»åŠ è¿™ä¸ªå‚æ•°ï¼Ÿè§£å†³äº†å•¥é—®é¢˜ï¼Ÿ

**ç®€ç­”ç‰ˆ**ï¼šå¤§æ¨¡å‹ä¸­çš„â€œæ¸©åº¦å‚æ•°â€æ˜¯ä¸€ä¸ªæ§åˆ¶**è¾“å‡ºéšæœºæ€§å’Œåˆ›é€ æ€§**çš„æ—‹é’®ã€‚å®ƒé€šè¿‡è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒçš„â€œå°–é”ç¨‹åº¦â€ï¼Œå†³å®šæ¨¡å‹æ˜¯æ›´å€¾å‘äºé€‰æ‹©é«˜æ¦‚ç‡çš„å¸¸è§è¯ï¼Œè¿˜æ˜¯æ•¢äºå°è¯•ä½æ¦‚ç‡çš„å†·é—¨è¯ã€‚æ·»åŠ è¿™ä¸ªå‚æ•°çš„ç›®çš„ï¼Œæ˜¯åœ¨**ç¨³å®šæ€§**å’Œ**å¤šæ ·æ€§**ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

---

## ğŸ”¥ æ¸©åº¦å‚æ•°çš„å®šä¹‰

- **æ•°å­¦ä¸Š**ï¼šåœ¨ softmax è®¡ç®—æ—¶ï¼ŒæŠŠ logits é™¤ä»¥ä¸€ä¸ªæ¸©åº¦ \(T\)ã€‚
  - \(T < 1\)ï¼šåˆ†å¸ƒæ›´å°–é” â†’ é«˜æ¦‚ç‡è¯æ›´å®¹æ˜“è¢«é€‰ä¸­ã€‚
  - \(T = 1\)ï¼šä½¿ç”¨åŸå§‹åˆ†å¸ƒã€‚
  - \(T > 1\)ï¼šåˆ†å¸ƒæ›´å¹³æ»‘ â†’ ä½æ¦‚ç‡è¯è¢«æ”¾å¤§ï¼Œæ›´å®¹æ˜“å‡ºç°ã€‚

å…¬å¼ï¼š

$$P(w_i) = \frac{e^{z_i / T}}{\sum_j e^{z_j / T}}$$

å…¶ä¸­ $z_i$ æ˜¯è¯çš„åŸå§‹å¾—åˆ†ã€‚

---

## ğŸ¨ ä¸ºä»€ä¹ˆè¦æ·»åŠ æ¸©åº¦å‚æ•°ï¼Ÿ

1. **è§£å†³â€œåƒç¯‡ä¸€å¾‹â€çš„é—®é¢˜**
   - å¦‚æœæ€»æ˜¯é€‰æœ€é«˜æ¦‚ç‡çš„è¯ï¼ˆç›¸å½“äºæ¸©åº¦=0ï¼‰ï¼Œè¾“å‡ºä¼šéå¸¸æ­»æ¿ã€é‡å¤ã€‚
2. **è§£å†³â€œèƒ¡è¨€ä¹±è¯­â€çš„é—®é¢˜**
   - å¦‚æœå®Œå…¨éšæœºé‡‡æ ·ï¼Œè¾“å‡ºå¯èƒ½é€»è¾‘æ··ä¹±ã€‚
3. **æä¾›çµæ´»æ€§**
   - ä¸åŒåœºæ™¯éœ€è¦ä¸åŒé£æ ¼ï¼šå†™è®ºæ–‡è¦ç¨³å®šï¼Œå†™è¯—æ­Œè¦æœ‰åˆ›æ„ã€‚

---

## âš–ï¸ æ¸©åº¦å¸¦æ¥çš„æ•ˆæœ

- **ä½æ¸©åº¦ï¼ˆ0~0.3ï¼‰**
  - è¾“å‡ºæ›´ç¡®å®šã€æ›´ä¸€è‡´ï¼Œé€‚åˆ**ä»£ç è¡¥å…¨ã€æŠ€æœ¯æ–‡æ¡£ã€æ³•å¾‹æ–‡æœ¬**ã€‚
- **ä¸­æ¸©åº¦ï¼ˆ0.7~1.0ï¼‰**
  - å¹³è¡¡ç¨³å®šä¸å¤šæ ·æ€§ï¼Œé€‚åˆ**æ—¥å¸¸å¯¹è¯ã€çŸ¥è¯†é—®ç­”**ã€‚
- **é«˜æ¸©åº¦ï¼ˆ>1.0ï¼‰**
  - è¾“å‡ºæ›´æœ‰åˆ›æ„ï¼Œä½†å¯èƒ½ä¸è¿è´¯ï¼Œé€‚åˆ**æ•…äº‹åˆ›ä½œã€å¹¿å‘Šæ–‡æ¡ˆ**ã€‚

---

## ğŸŒ° ä¸¾ä¸ªä¾‹å­

è¾“å…¥ï¼šâ€œæˆ‘å–œæ¬¢åƒâ€¦â€

- **T=0.2** â†’ æ¨¡å‹å‡ ä¹æ€»æ˜¯å›ç­”â€œè‹¹æœâ€ã€‚
- **T=1.0** â†’ å¯èƒ½å›ç­”â€œè‹¹æœâ€â€œé¦™è•‰â€ã€‚
- **T=1.5** â†’ ç”šè‡³å¯èƒ½å›ç­”â€œç«æ˜Ÿâ€è¿™ç§å†·é—¨è¯ã€‚

---

âœ… **æ€»ç»“**ï¼šæ¸©åº¦å‚æ•°æ˜¯å¤§æ¨¡å‹çš„â€œåˆ›æ„è°ƒèŠ‚å™¨â€ã€‚å®ƒè§£å†³äº†è¾“å‡ºè¦ä¹ˆè¿‡äºæ­»æ¿ã€è¦ä¹ˆè¿‡äºæ··ä¹±çš„é—®é¢˜ï¼Œè®©å¼€å‘è€…å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚ï¼Œåœ¨**å‡†ç¡®æ€§**å’Œ**åˆ›é€ æ€§**ä¹‹é—´è‡ªç”±åˆ‡æ¢ã€‚

---

å¥½çš„ ğŸ‘ æˆ‘å¸®ä½ æŠŠ **å¤§æ¨¡å‹æ¸©åº¦å‚æ•°çš„ä½œç”¨**æ•´ç†æˆä¸€ä»½é€ŸæŸ¥è¡¨ï¼Œæ–¹ä¾¿ä½ éšæ—¶æŸ¥é˜…å’Œåº”ç”¨ã€‚

---

# ğŸŒ¡ï¸ å¤§è¯­è¨€æ¨¡å‹æ¸©åº¦å‚æ•°é€ŸæŸ¥è¡¨

| æ¸©åº¦èŒƒå›´      | è¾“å‡ºé£æ ¼ | ç‰¹ç‚¹                                         | é€‚ç”¨åœºæ™¯                               |
| ------------- | -------- | -------------------------------------------- | -------------------------------------- |
| **0 ~ 0.3**   | **ä½æ¸©** | è¾“å‡ºé«˜åº¦ç¡®å®šï¼Œå‡ ä¹æ€»æ˜¯é€‰æœ€å¸¸è§çš„è¯ï¼Œé‡å¤æ€§å¼º | ä»£ç è¡¥å…¨ã€æ³•å¾‹æ–‡ä¹¦ã€ä½œä¸šç­”æ¡ˆã€æŠ€æœ¯æ–‡æ¡£ |
| **0.4 ~ 0.6** | **åä½** | è¾“å‡ºå¹³ç¨³ã€æµç•…ï¼Œä½†ç¼ºä¹åˆ›æ„                   | æ–°é—»æ‘˜è¦ã€çŸ¥è¯†ç‚¹æ€»ç»“                   |
| **0.7**       | **ä¸­æ¸©** | å¹³è¡¡ç¨³å®šæ€§ä¸å¤šæ ·æ€§ï¼Œæ—¢åˆç†åˆæœ‰å˜åŒ–           | å¸¸è¯†é—®ç­”ã€æ—¥å¸¸èŠå¤©ã€æ•™è‚²é—®ç­”           |
| **0.8 ~ 1.0** | **åé«˜** | æ›´æ¥è¿‘äººç±»å¯¹è¯é£æ ¼ï¼Œè¡¨è¾¾æ›´è‡ªç„¶               | é¢è¯•æ¨¡æ‹Ÿã€é£æ ¼åŒ–å†™ä½œã€è½»æ¾å¯¹è¯         |
| **> 1.0**     | **é«˜æ¸©** | åˆ›é€ æ€§å¼ºï¼Œä½†å¯èƒ½ä¸è¿è´¯ç”šè‡³â€œèƒ¡è¯´â€             | æ•…äº‹åˆ›ä½œã€å¹¿å‘Šæ–‡æ¡ˆã€å¤´è„‘é£æš´           |

---

## ğŸ¯ æ€»ç»“

- **æ¸©åº¦ä½** â†’ è¾“å‡ºæ›´ç¨³å®šã€æ›´å¯æ§ï¼Œé€‚åˆéœ€è¦å‡†ç¡®æ€§çš„ä»»åŠ¡ã€‚
- **æ¸©åº¦ä¸­ç­‰** â†’ è¾“å‡ºå¹³è¡¡ï¼Œé€‚åˆå¤§å¤šæ•°é€šç”¨åœºæ™¯ã€‚
- **æ¸©åº¦é«˜** â†’ è¾“å‡ºæ›´æœ‰åˆ›æ„ï¼Œä½†é£é™©æ˜¯é€»è¾‘æ€§ä¸‹é™ã€‚

ğŸ‘‰ å¯ä»¥æŠŠæ¸©åº¦ç†è§£ä¸ºâ€œåˆ›æ„æ—‹é’®â€ï¼š

- **æ‹§ä½** â†’ æ¨¡å‹åƒä¸¥è°¨çš„å·¥ç¨‹å¸ˆã€‚
- **æ‹§é«˜** â†’ æ¨¡å‹åƒå¤©é©¬è¡Œç©ºçš„ä½œå®¶ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ å†æ•´ç†ä¸€ä»½ **â€œæ¸©åº¦å‚æ•°è°ƒèŠ‚å®æˆ˜æŒ‡å—â€**ï¼Œé‡Œé¢åŒ…å«ä¸åŒä»»åŠ¡ï¼ˆå†™ä»£ç ã€å†™è®ºæ–‡ã€å†™æ•…äº‹ï¼‰çš„æ¨èæ¸©åº¦å€¼å’Œç¤ºä¾‹æç¤ºè¯ï¼Ÿè¿™æ ·ä½ å¯ä»¥ç›´æ¥å¥—ç”¨ã€‚

- [deepseek æœ‰å…³ temperature å‚æ•°çš„è®¾ç½®](https://api-docs.deepseek.com/zh-cn/quick_start/parameter_settings)

## æˆ‘å¦‚ä½•ä½¿ç”¨ hugging face æˆ–è€… model scope ä¸­çš„æ¨¡å‹ï¼Œæ„å»ºå±äºè‡ªå·±çš„æ¨¡å‹ï¼Ÿ

**å¿«é€Ÿå›ç­”**ï¼šä½ å¯ä»¥æŠŠ Hugging Face æˆ– ModelScope ä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹å½“ä½œâ€œåŸºçŸ³â€ï¼Œç„¶åé€šè¿‡ **ä¸‹è½½ â†’ åŠ è½½ â†’ å¾®è°ƒ â†’ ä¿å­˜ â†’ éƒ¨ç½²** è¿™å‡ ä¸ªæ­¥éª¤ï¼Œæ„å»ºå±äºè‡ªå·±çš„æ¨¡å‹ã€‚æ ¸å¿ƒæ€è·¯æ˜¯ï¼š**å…ˆå¤ç”¨åˆ«äººè®­ç»ƒå¥½çš„å¤§æ¨¡å‹ï¼Œå†ç”¨ä½ è‡ªå·±çš„æ•°æ®åšå®šåˆ¶åŒ–å¾®è°ƒ**ã€‚

---

## ğŸ§© ä½¿ç”¨ Hugging Face çš„æµç¨‹

1. **å®‰è£…ä¾èµ–**

```bash
pip install transformers datasets torch
```

2. **åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨**

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "bert-base-chinese"  # æˆ–è€… "Qwen/Qwen2-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
```

3. **å‡†å¤‡æ•°æ®é›†**

- ä½¿ç”¨ Hugging Face `datasets` åº“åŠ è½½å¼€æºæ•°æ®ï¼Œæˆ–ç”¨ä½ è‡ªå·±çš„ JSON/CSVã€‚
- æ ¼å¼ä¸€èˆ¬æ˜¯ **æŒ‡ä»¤ + è¾“å…¥ + è¾“å‡º**ã€‚

4. **å¾®è°ƒæ¨¡å‹**

- å°è§„æ¨¡ä»»åŠ¡å¯ç”¨ **Trainer API**ï¼š

```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./my_model",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    learning_rate=5e-5
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=my_dataset
)

trainer.train()
```

5. **ä¿å­˜ä¸ä½¿ç”¨**

```python
model.save_pretrained("./my_model")
tokenizer.save_pretrained("./my_model")
```

ä¹‹åä½ å°±å¯ä»¥ç”¨ `from_pretrained("./my_model")` æ¥åŠ è½½ä½ è‡ªå·±çš„æ¨¡å‹ã€‚

---

## ğŸ§© ä½¿ç”¨ ModelScope çš„æµç¨‹

1. **å®‰è£…ä¾èµ–**

```bash
pip install modelscope
```

2. **åŠ è½½æ¨¡å‹**

```python
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks

p = pipeline(Tasks.text_generation, model='damo/nlp_gpt3_text-generation_chinese-base')
result = p("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äº‘åŸç”Ÿ")
print(result)
```

3. **å¾®è°ƒ**

- ModelScope æä¾›äº† **Trainer** å’Œ **PEFTï¼ˆè½»é‡å¾®è°ƒï¼Œå¦‚ LoRAï¼‰**ï¼Œå¯ä»¥ç”¨ä½ è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œé¢†åŸŸé€‚é…ã€‚
- è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹åŒæ ·å¯ä»¥ä¿å­˜åˆ°æœ¬åœ°æˆ–ä¸Šä¼ åˆ° ModelScope Hubã€‚

---

## ğŸš€ å¦‚ä½•æ„å»ºâ€œå±äºè‡ªå·±çš„æ¨¡å‹â€

- **é€‰æ‹©åŸºåº§æ¨¡å‹**ï¼šæ ¹æ®ä»»åŠ¡é€‰æ‹©ï¼ˆä¸­æ–‡ â†’ ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚ Qwenã€ChatGLMï¼›è‹±æ–‡ â†’ LLaMAã€GPT-NeoXï¼‰ã€‚
- **å‡†å¤‡æ•°æ®**ï¼šé«˜è´¨é‡çš„é¢†åŸŸæ•°æ®ï¼ˆåŒ»ç–—ã€æ•™è‚²ã€å®¢æœç­‰ï¼‰ã€‚
- **é€‰æ‹©å¾®è°ƒæ–¹å¼**ï¼š
  - å…¨é‡å¾®è°ƒï¼ˆå‚æ•°æ›´æ–°å¤šï¼Œèµ„æºæ¶ˆè€—å¤§ï¼‰
  - LoRA/PEFTï¼ˆåªè®­ç»ƒå°éƒ¨åˆ†å‚æ•°ï¼Œé€‚åˆä½ è¿™ç§ç¡¬ä»¶æ¡ä»¶ï¼‰
- **ä¿å­˜ä¸éƒ¨ç½²**ï¼šä¿å­˜æ¨¡å‹æƒé‡ â†’ ç”¨ FastAPI/Gradio éƒ¨ç½² â†’ æä¾› API ç»™å¤–éƒ¨è°ƒç”¨ã€‚

---

âœ… **æ€»ç»“**ï¼š

- Hugging Face å’Œ ModelScope éƒ½èƒ½è®©ä½ å¿«é€ŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚
- ä½ åªéœ€è¦å‡†å¤‡æ•°æ®å¹¶é€‰æ‹©åˆé€‚çš„å¾®è°ƒæ–¹å¼ï¼Œå°±èƒ½å¾—åˆ°ä¸€ä¸ªâ€œå±äºä½ â€çš„æ¨¡å‹ã€‚
- æœ€ç»ˆäº§å‡ºæ˜¯ **æ¨¡å‹æƒé‡ + åˆ†è¯å™¨**ï¼Œå¯ä»¥é€šè¿‡ API æˆ–åº”ç”¨ç›´æ¥è°ƒç”¨ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä»½ **â€œHugging Face vs ModelScope æ„å»ºè‡ªå®šä¹‰æ¨¡å‹å¯¹æ¯”è¡¨â€**ï¼Œè®©ä½ æ¸…æ¥šçœ‹åˆ°ä¸¤è€…åœ¨åŠ è½½ã€å¾®è°ƒã€éƒ¨ç½²ä¸Šçš„å·®å¼‚ï¼Ÿ

## å…³äºå¤§æ¨¡å‹å¾®è°ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ï¼Œæœ‰æ²¡æœ‰æ•°æ®é›†ç›¸å…³çš„å·¥å…·ç”¨äºå¤§æ¨¡å‹å¾®è°ƒé˜¶æ®µ

**æœ‰çš„ âœ…ï¼Œç›®å‰å·²ç»æœ‰ä¸å°‘å¼€æºå·¥å…·ä¸“é—¨å¸®åŠ©ä½ åœ¨å¤§æ¨¡å‹å¾®è°ƒé˜¶æ®µå‡†å¤‡å’Œç®¡ç†æ•°æ®é›†ã€‚å®ƒä»¬èƒ½å¸®ä½ æ¸…æ´—ã€ç”Ÿæˆã€æ ¼å¼åŒ–ç”šè‡³è‡ªåŠ¨æ„é€ æŒ‡ä»¤æ•°æ®ï¼Œæå¤§é™ä½æ•°æ®å‡†å¤‡çš„éš¾åº¦ã€‚**

---

## ğŸ”‘ å¸¸è§çš„æ•°æ®é›†å·¥å…·

### 1. **edg4llm**

- **å®šä½**ï¼šå¤§æ¨¡å‹å¾®è°ƒæ•°æ®ç”Ÿæˆå·¥å…·
- **åŠŸèƒ½**ï¼š
  - è‡ªåŠ¨ç”Ÿæˆé—®ç­”å¯¹ã€å¯¹è¯æ•°æ®
  - æ”¯æŒå¤šç§æ ¼å¼è¾“å‡ºï¼Œç›´æ¥å¯ç”¨äº SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰
  - æä¾›ç®€æ´ APIï¼Œè½»é‡çº§ï¼Œæ˜“ä¸Šæ‰‹
- **é€‚åˆåœºæ™¯**ï¼šä½ æ²¡æœ‰ç°æˆçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œéœ€è¦å¿«é€Ÿåˆæˆä¸€äº›æŒ‡ä»¤æ•°æ®æ¥è·‘é€šæµç¨‹ã€‚

---

### 2. **Easy Dataset**

- **å®šä½**ï¼šå¯è§†åŒ–æ•°æ®é›†æ„å»ºå·¥å…·
- **åŠŸèƒ½**ï¼š
  - ä¸Šä¼ é¢†åŸŸæ–‡æ¡£ï¼ˆPDFã€TXT ç­‰ï¼‰
  - è‡ªåŠ¨åˆ†å‰²å†…å®¹ã€ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆ
  - è¾“å‡ºæ ‡å‡†åŒ–çš„è®­ç»ƒæ•°æ®ï¼ˆJSON/JSONLï¼‰
- **é€‚åˆåœºæ™¯**ï¼šä½ æœ‰ç‰¹å®šé¢†åŸŸçš„èµ„æ–™ï¼ˆæ¯”å¦‚åŒ»ç–—ã€æ•™è‚²ã€å®¢æœï¼‰ï¼Œæƒ³å¿«é€Ÿè½¬åŒ–ä¸ºå¾®è°ƒæ•°æ®ã€‚

---

### 3. **LLaMA-Factory**

- **å®šä½**ï¼šç»Ÿä¸€çš„ LLM å¾®è°ƒæ¡†æ¶
- **åŠŸèƒ½**ï¼š
  - å†…ç½®æ•°æ®ç®¡é“ï¼Œæ”¯æŒ 100+ æ¨¡å‹å’Œå¤šç§æ•°æ®æ ¼å¼
  - æ”¯æŒ LoRAã€QLoRAã€å…¨å‚æ•°å¾®è°ƒ
  - å¯ç›´æ¥åŠ è½½å¼€æºæ•°æ®é›†ï¼ˆå¦‚ Alpacaã€Belleï¼‰
- **é€‚åˆåœºæ™¯**ï¼šä½ æƒ³è¦â€œä¸€ç«™å¼â€è§£å†³æ–¹æ¡ˆï¼Œä»æ•°æ®åˆ°è®­ç»ƒåˆ°éƒ¨ç½²å…¨è¦†ç›–ã€‚

---

### 4. **XTuner**

- **å®šä½**ï¼šé«˜æ•ˆçµæ´»çš„å¾®è°ƒå·¥å…·åŒ…
- **åŠŸèƒ½**ï¼š
  - æ”¯æŒå¤šç§ LLMï¼ˆInternLMã€Qwenã€ChatGLM ç­‰ï¼‰
  - æ•°æ®ç®¡é“çµæ´»ï¼Œèƒ½å®¹çº³å¼€æºå’Œè‡ªå®šä¹‰æ ¼å¼
  - ä¸ DeepSpeedã€FlashAttention ç­‰ä¼˜åŒ–æŠ€æœ¯å…¼å®¹
- **é€‚åˆåœºæ™¯**ï¼šä½ éœ€è¦åœ¨æœ‰é™ç¡¬ä»¶ä¸Šé«˜æ•ˆå¾®è°ƒï¼Œä¸”æ•°æ®æ ¼å¼å¤šæ ·ã€‚

---

## ğŸ“Œ æ€»ç»“

- **æ•°æ®ç”Ÿæˆ**ï¼šedg4llm â†’ è‡ªåŠ¨åˆæˆé—®ç­”æ•°æ®
- **æ•°æ®æ„å»º**ï¼šEasy Dataset â†’ ä¸Šä¼ æ–‡æ¡£ç”Ÿæˆè®­ç»ƒé›†
- **æ•°æ®åˆ°è®­ç»ƒä¸€ä½“åŒ–**ï¼šLLaMA-Factoryã€XTuner â†’ å†…ç½®æ•°æ®ç®¡é“ + å¾®è°ƒæ¡†æ¶

ğŸ‘‰ æ¢å¥è¯è¯´ï¼Œå¦‚æœä½ åªæ˜¯æƒ³å¿«é€Ÿå‡†å¤‡æ•°æ®ï¼Œ**edg4llm / Easy Dataset** å¾ˆåˆé€‚ï¼›å¦‚æœä½ æƒ³è¦å®Œæ•´çš„è®­ç»ƒæµæ°´çº¿ï¼Œ**LLaMA-Factory / XTuner** æ›´é€‚åˆã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä»½ **â€œå¾®è°ƒæ•°æ®é›†å·¥å…·å¯¹æ¯”è¡¨â€**ï¼ˆä»åŠŸèƒ½ã€æ˜“ç”¨æ€§ã€é€‚ç”¨åœºæ™¯ä¸‰ä¸ªç»´åº¦å¯¹æ¯”ï¼‰ï¼Œè®©ä½ èƒ½ä¸€çœ¼é€‰å‡ºæœ€é€‚åˆä½ ç¡¬ä»¶å’Œéœ€æ±‚çš„å·¥å…·ï¼Ÿ

---

- [CSDNï¼šã€Šedg4llmï¼šä¸€æ¬¾ç®€å•å¥½ç”¨çš„å¤§æ¨¡å‹å¾®è°ƒæ•°æ®ç”Ÿæˆå·¥å…·ã€‹](https://blog.csdn.net/T_susan/article/details/145134496)
- [çŸ¥ä¹ï¼šã€Šäº”ç§å¤§æ¨¡å‹å¾®è°ƒå·¥å…·ã€‹](https://zhuanlan.zhihu.com/p/694521781)
- [GitHubï¼šEasy Dataset é¡¹ç›®](https://github.com/whale2002/easy-dataset)
- [Github: easy-dataset](https://github.com/ConardLi/easy-dataset)

## å¦‚ä½•æŠŠ pdfï¼Œdocï¼Œmarkdown ç­‰æ–‡ä»¶å‘é‡åŒ–ï¼Ÿå¹¶å¦‚ä½•æŠŠè¿™äº›å‘é‡åŒ–çš„æ•°æ®ç»™å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹å¯ä»¥åŸºäºè¿™äº›æ•°æ®å›ç­”é—®é¢˜ï¼Ÿ

**å¿«é€Ÿå›ç­”**ï¼šè¦è®©å¤§æ¨¡å‹â€œçœ‹æ‡‚â€PDFã€Wordã€Markdown ç­‰æ–‡ä»¶å¹¶åŸºäºå…¶å†…å®¹å›ç­”é—®é¢˜ï¼Œé€šå¸¸åˆ†ä¸¤æ­¥ï¼š**â‘  æ–‡æœ¬æå–ä¸å‘é‡åŒ–ï¼ˆEmbeddingï¼‰ â†’ â‘¡ æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰**ã€‚å‘é‡åŒ–æ˜¯æŠŠæ–‡æ¡£å†…å®¹è½¬æˆæ•°å€¼å‘é‡å­˜å…¥æ•°æ®åº“ï¼›é—®é—®é¢˜æ—¶å…ˆæ£€ç´¢ç›¸å…³å‘é‡ï¼Œå†æŠŠæ£€ç´¢åˆ°çš„æ–‡æœ¬ç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡å–‚ç»™å¤§æ¨¡å‹ã€‚

---

## ğŸ§© æ­¥éª¤è¯¦è§£

### 1. æ–‡æœ¬æå–

- **PDF**ï¼šç”¨ `pdfplumber`ã€`PyPDF2` æˆ–å¼€æºå·¥å…· Markerã€MinerU æå–æ­£æ–‡ï¼Œå¿…è¦æ—¶åŠ  OCRï¼ˆæ‰«æç‰ˆï¼‰ã€‚
- **Word (doc/docx)**ï¼šç”¨ `python-docx` æˆ– `docx2txt` æå–ã€‚
- **Markdown**ï¼šç›´æ¥è¯»å–æ–‡æœ¬ï¼Œæˆ–ç”¨ `markdown` åº“è§£æã€‚
- **æ¸…æ´—**ï¼šå»æ‰é¡µçœ‰é¡µè„šã€ç›®å½•ã€æ— å…³ç¬¦å·ï¼ŒæŒ‰æ®µè½æˆ–å›ºå®šé•¿åº¦åˆ‡åˆ†ï¼ˆå¦‚ 500 tokens ä¸€æ®µï¼‰ã€‚

### 2. å‘é‡åŒ–ï¼ˆEmbeddingï¼‰

- ä½¿ç”¨å¼€æºæ¨¡å‹ï¼ˆå¦‚ Hugging Face çš„ `sentence-transformers`ï¼Œæˆ– ModelScope çš„ä¸­æ–‡å‘é‡æ¨¡å‹ï¼‰ï¼š

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
embeddings = model.encode(["è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬", "è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬"])
```

- æ¯æ®µæ–‡æœ¬ä¼šå˜æˆä¸€ä¸ªé«˜ç»´å‘é‡ï¼ˆå¦‚ 384/768 ç»´ï¼‰ã€‚

### 3. å­˜å‚¨å‘é‡

- å¸¸ç”¨å‘é‡æ•°æ®åº“ï¼š**FAISS**ï¼ˆæœ¬åœ°ï¼‰ã€**Milvus**ã€**Weaviate**ã€**Pinecone**ã€‚
- å­˜å‚¨å†…å®¹ï¼š`{å‘é‡, åŸå§‹æ–‡æœ¬, æ–‡æ¡£ID, æ®µè½ä½ç½®}`ã€‚

### 4. æ£€ç´¢ + å¤§æ¨¡å‹é—®ç­”ï¼ˆRAGï¼‰

- ç”¨æˆ·æé—® â†’ æŠŠé—®é¢˜ä¹Ÿå‘é‡åŒ– â†’ åœ¨å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸ä¼¼çš„æ®µè½ã€‚
- æŠŠæ£€ç´¢åˆ°çš„æ®µè½æ‹¼æ¥åˆ°æç¤ºè¯é‡Œï¼Œäº¤ç»™å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆã€‚

ç¤ºä¾‹ä¼ªä»£ç ï¼š

```python
query = "è¿™ä»½åˆåŒçš„ä»˜æ¬¾æ¡æ¬¾æ˜¯ä»€ä¹ˆï¼Ÿ"
q_vec = model.encode([query])
docs = faiss_index.search(q_vec, k=3)  # æ‰¾åˆ°æœ€ç›¸å…³çš„3æ®µ
context = "\n".join([d["text"] for d in docs])

prompt = f"æ ¹æ®ä»¥ä¸‹èµ„æ–™å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜ï¼š{query}\nå›ç­”ï¼š"
answer = llm.generate(prompt)
```

### 5. æ•´ä½“æ¶æ„ï¼ˆRAGï¼‰

1. **ç¦»çº¿é˜¶æ®µ**ï¼šæ–‡ä»¶ â†’ æ–‡æœ¬ â†’ åˆ‡åˆ† â†’ å‘é‡åŒ– â†’ å­˜å…¥æ•°æ®åº“
2. **åœ¨çº¿é˜¶æ®µ**ï¼šé—®é¢˜ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ç›¸å…³ç‰‡æ®µ â†’ æ‹¼æ¥ä¸Šä¸‹æ–‡ â†’ å¤§æ¨¡å‹å›ç­”

---

## âœ… æ€»ç»“

- **å‘é‡åŒ–çš„ç›®çš„**ï¼šè®©æœºå™¨èƒ½â€œç†è§£â€æ–‡æœ¬è¯­ä¹‰ï¼Œæ”¯æŒç›¸ä¼¼åº¦æ£€ç´¢ã€‚
- **å¤§æ¨¡å‹çš„è§’è‰²**ï¼šä¸æ˜¯ç›´æ¥â€œè®°ä½â€æ–‡æ¡£ï¼Œè€Œæ˜¯åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆã€‚
- **å…³é”®æŠ€æœ¯**ï¼šæ–‡æœ¬æå– â†’ Embedding â†’ å‘é‡æ•°æ®åº“ â†’ RAGã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œæ–‡æ¡£ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ â†’ å¤§æ¨¡å‹é—®ç­”â€æµç¨‹å›¾**ï¼Œè®©ä½ ä¸€çœ¼çœ‹åˆ°æ•´ä¸ªæ•°æ®æµè½¬è¿‡ç¨‹ï¼Ÿ

## Rerank æ¨¡å‹å·¥ä½œåœ¨ä»€ä¹ˆé˜¶æ®µï¼Ÿä»€ä¹ˆæ—¶é—´ä½¿ç”¨å®ƒï¼Ÿ

**ç®€ç­”ç‰ˆ**ï¼šRerank æ¨¡å‹å·¥ä½œåœ¨ **æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹çš„â€œåˆæ­¥æ£€ç´¢ä¹‹åã€å¤§æ¨¡å‹ç”Ÿæˆä¹‹å‰â€**ã€‚å®ƒçš„ä½œç”¨æ˜¯å¯¹åˆæ­¥æ£€ç´¢åˆ°çš„ä¸€æ‰¹å€™é€‰æ–‡æ¡£è¿›è¡Œ**ç²¾ç»†æ’åº**ï¼ŒæŠŠçœŸæ­£æœ€ç›¸å…³çš„å†…å®¹æ’åœ¨å‰é¢ã€‚é€šå¸¸åœ¨**éœ€è¦æ›´é«˜å‡†ç¡®æ€§**æ—¶ä½¿ç”¨ï¼Œæ¯”å¦‚é—®ç­”ç³»ç»Ÿã€æ³•å¾‹/åŒ»ç–—æ–‡æ¡£æ£€ç´¢ã€ä¼ä¸šçŸ¥è¯†åº“ã€‚

---

## ğŸ§© Rerank æ¨¡å‹çš„å·¥ä½œé˜¶æ®µ

1. **ç”¨æˆ·æé—®**
   - è¾“å…¥é—®é¢˜ï¼Œä¾‹å¦‚ï¼šâ€œåˆåŒçš„ä»˜æ¬¾æ¡æ¬¾æ˜¯ä»€ä¹ˆï¼Ÿâ€
2. **åˆæ­¥æ£€ç´¢ï¼ˆå‘é‡æ£€ç´¢ / BM25ï¼‰**
   - ä»æ•°æ®åº“é‡Œå¿«é€Ÿæ‰¾åˆ°ä¸€æ‰¹å€™é€‰æ–‡æ¡£ï¼ˆå¯èƒ½å‡ åæ¡ï¼‰ã€‚
   - è¿™ä¸€æ­¥é€Ÿåº¦å¿«ï¼Œä½†ç›¸å…³æ€§ä¸å¤Ÿç²¾å‡†ã€‚
3. **Rerankï¼ˆé‡æ’åºï¼‰**
   - ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹ï¼ˆå¦‚ cross-encoderï¼‰é€ä¸€å¯¹â€œé—®é¢˜-æ–‡æ¡£â€å¯¹è¿›è¡Œæ‰“åˆ†ã€‚
   - é‡æ–°æ’åºï¼ŒæŠŠæœ€ç›¸å…³çš„å‡ æ¡æ”¾åœ¨æœ€å‰é¢ã€‚
4. **å¤§æ¨¡å‹ç”Ÿæˆ**
   - æŠŠæ’åºé å‰çš„æ–‡æ¡£æ‹¼æ¥è¿›æç¤ºè¯ï¼Œäº¤ç»™å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆã€‚

---

## ğŸ¯ ä¸ºä»€ä¹ˆè¦ç”¨ Rerankï¼Ÿ

- **è§£å†³åˆæ£€ä¸ç²¾å‡†çš„é—®é¢˜**ï¼šå‘é‡æ£€ç´¢åªçœ‹â€œè¯­ä¹‰ç›¸ä¼¼åº¦â€ï¼Œå¯èƒ½æŠŠè¡¨é¢ç›¸ä¼¼ä½†æ— å…³çš„æ®µè½æ’å‰ã€‚
- **å‡å°‘å™ªéŸ³**ï¼šé¿å…å¤§æ¨¡å‹è¯»åˆ°ä¸€å †æ— å…³ä¸Šä¸‹æ–‡ï¼Œæµªè´¹ tokenï¼Œè¿˜å¯èƒ½ç­”éæ‰€é—®ã€‚
- **æå‡å‡†ç¡®ç‡**ï¼šè®©å¤§æ¨¡å‹ä¼˜å…ˆåˆ©ç”¨çœŸæ­£ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆæ›´å¯é ã€‚

---

## â° ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ Rerankï¼Ÿ

- **æ–‡æ¡£åº“å¾ˆå¤§**ï¼šåˆæ­¥æ£€ç´¢ç»“æœå¯èƒ½åŒ…å«å¾ˆå¤šå™ªéŸ³ã€‚
- **ä»»åŠ¡è¦æ±‚é«˜ç²¾åº¦**ï¼šå¦‚æ³•å¾‹ã€åŒ»ç–—ã€é‡‘èé—®ç­”ï¼Œä¸èƒ½å®¹å¿ç­”é”™ã€‚
- **ä¸Šä¸‹æ–‡çª—å£æœ‰é™**ï¼šå¤§æ¨¡å‹ä¸€æ¬¡èƒ½è¯»çš„ token æœ‰é™ï¼Œå¿…é¡»æŒ‘æœ€ç›¸å…³çš„å†…å®¹ã€‚
- **å¤šè·¯æ£€ç´¢èåˆ**ï¼šå½“ä½ åŒæ—¶ç”¨å…³é”®è¯æ£€ç´¢ + å‘é‡æ£€ç´¢æ—¶ï¼ŒRerank å¯ä»¥ç»Ÿä¸€æ’åºã€‚

---

## âœ… æ€»ç»“

- **é˜¶æ®µ**ï¼šRerank åœ¨â€œåˆæ­¥æ£€ç´¢ä¹‹åã€å¤§æ¨¡å‹ç”Ÿæˆä¹‹å‰â€ã€‚
- **ä½œç”¨**ï¼šå¯¹å€™é€‰æ–‡æ¡£åšç²¾ç»†æ’åºï¼Œä¿è¯å¤§æ¨¡å‹è¯»åˆ°çš„ä¸Šä¸‹æ–‡æœ€ç›¸å…³ã€‚
- **ä½¿ç”¨æ—¶æœº**ï¼šå½“ä½ éœ€è¦æ›´é«˜å‡†ç¡®æ€§ã€ä¸Šä¸‹æ–‡æœ‰é™æˆ–æ–‡æ¡£åº“åºå¤§æ—¶ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œRAG + Rerank æµç¨‹å›¾â€**ï¼ŒæŠŠâ€œç”¨æˆ·æé—® â†’ åˆæ£€ â†’ Rerank â†’ å¤§æ¨¡å‹ç”Ÿæˆâ€çš„æ•°æ®æµæ¸…æ™°å±•ç¤ºå‡ºæ¥ï¼Ÿ

## åœ¨ RAG çš„æŠ€æœ¯ä¸­ï¼Œæœ‰å¯èƒ½ä½¿ç”¨å¤šå°‘ç§æ¨¡å‹ï¼Ÿåˆ†åˆ«åœ¨ä»€ä¹ˆé˜¶æ®µä½¿ç”¨ï¼Ÿä¸ºäº†è¾¾æˆä»€ä¹ˆæ•ˆæœï¼Ÿ

**å¿«é€Ÿå›ç­”**ï¼šåœ¨ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯ä¸­ï¼Œé€šå¸¸ä¼šç”¨åˆ° **4 å¤§ç±»æ¨¡å‹**ï¼Œåˆ†åˆ«åœ¨ä¸åŒé˜¶æ®µå‘æŒ¥ä½œç”¨ï¼š**Embedding æ¨¡å‹ â†’ æ£€ç´¢æ¨¡å‹ â†’ Rerank æ¨¡å‹ â†’ å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰**ã€‚å®ƒä»¬çš„ç›®æ ‡æ˜¯ï¼šå…ˆæŠŠæ–‡æ¡£è½¬æˆå‘é‡ï¼Œå¿«é€Ÿæ‰¾åˆ°å€™é€‰ï¼Œå†ç²¾æ’ï¼Œæœ€åç”Ÿæˆç­”æ¡ˆï¼Œä»è€Œè§£å†³å¤§æ¨¡å‹çŸ¥è¯†è¿‡æ—¶ã€å¹»è§‰å’Œç§æœ‰æ•°æ®åˆ©ç”¨çš„é—®é¢˜ã€‚

---

## ğŸ§© RAG æŠ€æœ¯ä¸­çš„æ¨¡å‹ç±»å‹ä¸é˜¶æ®µ

### 1. **Embedding æ¨¡å‹ï¼ˆå‘é‡åŒ–é˜¶æ®µï¼‰**

- **é˜¶æ®µ**ï¼šçŸ¥è¯†åº“æ„å»ºæ—¶ï¼ŒæŠŠæ–‡æ¡£åˆ‡åˆ†åçš„ç‰‡æ®µè½¬æˆå‘é‡ã€‚
- **å¸¸è§æ¨¡å‹**ï¼š`sentence-transformers`ã€`text-embedding-ada-002`ã€ä¸­æ–‡å‘é‡æ¨¡å‹ï¼ˆå¦‚ bge-m3ï¼‰ã€‚
- **æ•ˆæœ**ï¼šè®©æ–‡æœ¬èƒ½ä»¥â€œè¯­ä¹‰ç›¸ä¼¼åº¦â€æ–¹å¼è¢«æ£€ç´¢ï¼Œè€Œä¸æ˜¯ä»…é å…³é”®è¯ã€‚

---

### 2. **æ£€ç´¢æ¨¡å‹ï¼ˆåˆæ­¥å¬å›é˜¶æ®µï¼‰**

- **é˜¶æ®µ**ï¼šç”¨æˆ·æé—®åï¼ŒæŠŠé—®é¢˜å‘é‡åŒ–ï¼Œåœ¨å‘é‡æ•°æ®åº“é‡Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æ¡£ç‰‡æ®µã€‚
- **å¸¸è§æ–¹æ³•**ï¼š
  - å‘é‡æ£€ç´¢ï¼ˆFAISSã€Milvusã€Weaviateï¼‰
  - å…³é”®è¯æ£€ç´¢ï¼ˆBM25ã€Elasticsearchï¼‰
  - æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
- **æ•ˆæœ**ï¼šå¿«é€Ÿç¼©å°å€™é€‰èŒƒå›´ï¼Œä¿è¯å¬å›ç‡ã€‚

---

### 3. **Rerank æ¨¡å‹ï¼ˆé‡æ’åºé˜¶æ®µï¼‰**

- **é˜¶æ®µ**ï¼šå¯¹åˆæ­¥æ£€ç´¢åˆ°çš„å€™é€‰æ–‡æ¡£è¿›è¡Œç²¾ç»†æ‰“åˆ†å’Œæ’åºã€‚
- **å¸¸è§æ¨¡å‹**ï¼šCross-Encoderï¼ˆå¦‚ `bge-reranker`ï¼‰ã€ColBERTã€‚
- **æ•ˆæœ**ï¼šæŠŠçœŸæ­£æœ€ç›¸å…³çš„æ–‡æ¡£æ’åœ¨å‰é¢ï¼Œå‡å°‘å™ªéŸ³ï¼Œæå‡å‡†ç¡®ç‡ã€‚

---

### 4. **å¤§è¯­è¨€æ¨¡å‹ LLMï¼ˆç”Ÿæˆé˜¶æ®µï¼‰**

- **é˜¶æ®µ**ï¼šæŠŠæ’åºåçš„æ–‡æ¡£ç‰‡æ®µæ‹¼æ¥åˆ°æç¤ºè¯ä¸­ï¼Œäº¤ç»™ LLM ç”Ÿæˆç­”æ¡ˆã€‚
- **å¸¸è§æ¨¡å‹**ï¼šGPTã€LLaMAã€Qwenã€ChatGLM ç­‰ã€‚
- **æ•ˆæœ**ï¼šåŸºäºå¤–éƒ¨çŸ¥è¯†ç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”ï¼Œè§£å†³çŸ¥è¯†æ—¶æ•ˆæ€§å’Œå¹»è§‰é—®é¢˜ã€‚

---

## ğŸ¯ æ€»ç»“

- **Embedding æ¨¡å‹**ï¼šè´Ÿè´£â€œç†è§£â€æ–‡æœ¬ â†’ å‘é‡åŒ–
- **æ£€ç´¢æ¨¡å‹**ï¼šè´Ÿè´£â€œæ‰¾åˆ°â€ç›¸å…³æ–‡æ¡£ â†’ åˆæ­¥å¬å›
- **Rerank æ¨¡å‹**ï¼šè´Ÿè´£â€œæŒ‘é€‰â€æœ€ç›¸å…³çš„ â†’ ç²¾æ’
- **LLM**ï¼šè´Ÿè´£â€œå›ç­”â€ â†’ åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆ

ğŸ‘‰ æ¢å¥è¯è¯´ï¼ŒRAG å°±åƒä¸€ä¸ªâ€œä¿¡æ¯ç®¡é“â€ï¼š**Embedding æŠŠçŸ¥è¯†è£…è¿›ç®¡é“ â†’ æ£€ç´¢æŠŠæ°´æŠ½å‡ºæ¥ â†’ Rerank è¿‡æ»¤æ‚è´¨ â†’ LLM è¾“å‡ºæ¸…æ¾ˆçš„ç­”æ¡ˆ**ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œRAG æŠ€æœ¯æ¨¡å‹åˆ†å·¥å›¾â€**ï¼ŒæŠŠè¿™å››ç±»æ¨¡å‹åœ¨æµç¨‹ä¸­çš„ä½ç½®å’Œä½œç”¨ç›´è§‚å±•ç¤ºå‡ºæ¥ï¼Ÿ

- [çŸ¥ä¹ï¼šã€ŠRAG æŠ€æœ¯å…¨è§£æã€‹](https://zhuanlan.zhihu.com/p/1948688100764153329)
- [ç™¾åº¦å¼€å‘è€…ä¸­å¿ƒï¼šã€Šå¤§æ¨¡å‹åº”ç”¨è¿›é˜¶ï¼šRAG æŠ€æœ¯å…¨è§£æä¸å®æˆ˜æŒ‡å—ã€‹](https://developer.baidu.com/article/detail.html?id=3891992)

## å¦‚ä½•å®ç°æº¯æºï¼Ÿç°åœ¨ RAG æŠ€æœ¯è¿˜æ˜¯æœ€å¥½çš„æŠ€æœ¯æ–¹æ¡ˆå—ï¼Ÿæœ‰æ›´å…ˆè¿›çš„æ–¹æ¡ˆå—ï¼Ÿ

**ç®€ç­”ç‰ˆ**ï¼š  
åœ¨å¤§æ¨¡å‹åº”ç”¨ä¸­ï¼Œ**æº¯æº**é€šå¸¸é€šè¿‡åœ¨ RAG æµç¨‹ä¸­ä¿ç•™æ£€ç´¢åˆ°çš„åŸå§‹æ–‡æ¡£ç‰‡æ®µåŠå…¶å‡ºå¤„æ¥å®ç°ã€‚RAG ç›®å‰ä»æ˜¯æœ€ä¸»æµã€å·¥ç¨‹åŒ–ç¨‹åº¦æœ€é«˜çš„æ–¹æ¡ˆï¼Œä½†å®ƒå¹¶éç»ˆç‚¹ã€‚æ­£åœ¨å…´èµ·çš„æ›´å…ˆè¿›æ–¹å‘åŒ…æ‹¬ **çŸ¥è¯†å›¾è°±å¢å¼ºã€æ£€ç´¢+æ¨ç†æ··åˆæ¶æ„ã€å¤šæ¨¡æ€ RAGã€ä»¥åŠ Agent å¼åŠ¨æ€å·¥å…·è°ƒç”¨**ï¼Œè¿™äº›éƒ½åœ¨å°è¯•è§£å†³ RAG çš„å±€é™ã€‚

---

## ğŸ” å¦‚ä½•å®ç°æº¯æº

1. **æ£€ç´¢é˜¶æ®µä¿ç•™å…ƒæ•°æ®**

   - åœ¨å‘é‡æ•°æ®åº“ä¸­å­˜å‚¨çš„ä¸ä»…æ˜¯æ–‡æœ¬å‘é‡ï¼Œè¿˜åŒ…æ‹¬ **æ–‡æ¡£ IDã€æ ‡é¢˜ã€é¡µç ã€URL** ç­‰å…ƒä¿¡æ¯ã€‚
   - å½“æ£€ç´¢åˆ°ç‰‡æ®µæ—¶ï¼Œè¿åŒè¿™äº›å…ƒæ•°æ®ä¸€èµ·è¿”å›ã€‚

2. **ç­”æ¡ˆç”Ÿæˆæ—¶å¼•ç”¨æ¥æº**

   - å°†æ£€ç´¢åˆ°çš„ç‰‡æ®µæ‹¼æ¥è¿›æç¤ºè¯ï¼Œè®©å¤§æ¨¡å‹åœ¨å›ç­”ä¸­å¼•ç”¨å‡ºå¤„ã€‚
   - ä¾‹å¦‚ï¼šâ€œæ ¹æ®ã€ŠåˆåŒæ³•ã€‹ç¬¬ä¸‰ç« ç¬¬ 12 æ¡ï¼Œä»˜æ¬¾æ¡æ¬¾ä¸ºâ€¦â€¦â€

3. **å¯è§†åŒ–æº¯æº**
   - åœ¨å‰ç«¯ç•Œé¢ä¸­ï¼Œå°†æ¨¡å‹å›ç­”ä¸åŸå§‹æ–‡æ¡£ç‰‡æ®µé“¾æ¥èµ·æ¥ï¼Œç”¨æˆ·å¯ç‚¹å‡»æŸ¥çœ‹ã€‚
   - è¿™æ ·ä¸ä»…å¢å¼ºå¯ä¿¡åº¦ï¼Œä¹Ÿæ–¹ä¾¿äººå·¥å¤æ ¸ã€‚

---

## ğŸ“Œ RAG æ˜¯å¦ä»æ˜¯æœ€ä½³æ–¹æ¡ˆï¼Ÿ

- **ä¼˜åŠ¿**ï¼š
  - è§£å†³äº†å¤§æ¨¡å‹çŸ¥è¯†è¿‡æ—¶ã€å¹»è§‰é—®é¢˜ã€‚
  - å·¥ç¨‹åŒ–æˆç†Ÿï¼Œç”Ÿæ€å®Œå–„ï¼ˆLangChainã€LlamaIndex ç­‰ï¼‰ã€‚
  - æˆæœ¬ä½äºå…¨é‡å¾®è°ƒï¼Œæ”¯æŒåŠ¨æ€æ›´æ–°çŸ¥è¯†åº“ã€‚
- **å±€é™**ï¼š
  - æ£€ç´¢è´¨é‡å†³å®šç­”æ¡ˆè´¨é‡ï¼Œè‹¥å¬å›ä¸å‡†ï¼Œæ¨¡å‹ä»å¯èƒ½â€œèƒ¡ç¼–â€ã€‚
  - ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œé•¿æ–‡æ¡£æˆ–å¤šè·³æ¨ç†æ—¶å®¹æ˜“ä¸¢å¤±å…³é”®ä¿¡æ¯ã€‚
  - æº¯æºç²’åº¦æœ‰é™ï¼Œé€šå¸¸åªèƒ½åˆ°æ®µè½çº§åˆ«ã€‚

---

## ğŸš€ æ›´å…ˆè¿›çš„æ–¹æ¡ˆæ¢ç´¢

1. **çŸ¥è¯†å›¾è°±å¢å¼º RAG**

   - åœ¨æ£€ç´¢æ—¶ä¸ä»…è¿”å›æ–‡æœ¬ï¼Œè¿˜èƒ½åˆ©ç”¨å®ä½“å…³ç³»å›¾ï¼Œæ”¯æŒæ›´å¤æ‚çš„æ¨ç†å’Œæ›´ç²¾å‡†çš„æº¯æºã€‚

2. **RAG + é€»è¾‘æ¨ç†ï¼ˆReasoning-Augmented RAGï¼‰**

   - åœ¨æ£€ç´¢åå¢åŠ ä¸€ä¸ªâ€œæ¨ç†å±‚â€ï¼Œè®©æ¨¡å‹èƒ½è·¨æ®µè½ã€å¤šè·³æ¨ç†ï¼Œè€Œä¸æ˜¯ä»…æ‹¼æ¥ä¸Šä¸‹æ–‡ã€‚

3. **å¤šæ¨¡æ€ RAG**

   - ä¸ä»…æ£€ç´¢æ–‡æœ¬ï¼Œè¿˜èƒ½æ£€ç´¢å›¾åƒã€è¡¨æ ¼ã€éŸ³é¢‘ï¼Œæ”¯æŒæ›´ä¸°å¯Œçš„æº¯æºåœºæ™¯ï¼ˆå¦‚åŒ»å­¦å½±åƒ+æŠ¥å‘Šï¼‰ã€‚

4. **Agent å¼åŠ¨æ€å·¥å…·è°ƒç”¨**

   - æ¨¡å‹åœ¨å›ç­”æ—¶å¯è°ƒç”¨å¤–éƒ¨ APIã€æ•°æ®åº“ã€è®¡ç®—å·¥å…·ï¼Œå®æ—¶æŸ¥è¯å¹¶è¿”å›å¸¦æ¥æºçš„ç­”æ¡ˆã€‚

5. **æ£€ç´¢+ç”Ÿæˆèåˆæ¨¡å‹ï¼ˆå¦‚ RETROã€Atlasï¼‰**
   - åœ¨è®­ç»ƒé˜¶æ®µå°±æŠŠæ£€ç´¢æœºåˆ¶èå…¥æ¨¡å‹æ¶æ„ï¼Œä½¿å…¶â€œåŸç”Ÿâ€å…·å¤‡æº¯æºèƒ½åŠ›ï¼Œè€Œä¸æ˜¯å¤–æŒ‚å¼ã€‚

---

## âœ… æ€»ç»“

- **æº¯æºå®ç°**ï¼šé åœ¨æ£€ç´¢é˜¶æ®µä¿ç•™å…ƒæ•°æ®ï¼Œå¹¶åœ¨å›ç­”ä¸­æ˜¾å¼å¼•ç”¨ã€‚
- **RAG ç°çŠ¶**ï¼šä»æ˜¯æœ€æˆç†Ÿã€æ€§ä»·æ¯”æœ€é«˜çš„æ–¹æ¡ˆï¼Œä½†æœ‰å±€é™ã€‚
- **æœªæ¥è¶‹åŠ¿**ï¼šçŸ¥è¯†å›¾è°±å¢å¼ºã€å¤šè·³æ¨ç†ã€å¤šæ¨¡æ€æ£€ç´¢ã€Agent å·¥å…·è°ƒç”¨ï¼Œä»¥åŠâ€œåŸç”Ÿæ£€ç´¢æ¨¡å‹â€ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œRAG ä¸ä¸‹ä¸€ä»£å¢å¼ºæ–¹æ¡ˆå¯¹æ¯”å›¾â€**ï¼ŒæŠŠå®ƒä»¬çš„æµç¨‹å’Œæº¯æºèƒ½åŠ›ç›´è§‚å¯¹æ¯”å‡ºæ¥ï¼Ÿ

## ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Ÿ

**å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMultimodal Large Language Model, MLLMï¼‰** æ˜¯ä¸€ç§èƒ½åŒæ—¶ç†è§£å’Œå¤„ç†å¤šç§ç±»å‹ä¿¡æ¯çš„æ¨¡å‹ï¼Œæ¯”å¦‚æ–‡å­—ã€å›¾åƒã€è¯­éŸ³ã€è§†é¢‘ç­‰ã€‚å®ƒçš„ç›®æ ‡æ˜¯è®© AI åƒäººç±»ä¸€æ ·ï¼Œèƒ½â€œçœ‹å›¾è¯´è¯â€â€œå¬å£°è¯†æ„â€â€œè¯»æ–‡ç†è§£â€ï¼Œå®ç°æ›´è‡ªç„¶ã€æ›´å…¨é¢çš„æ™ºèƒ½äº¤äº’ã€‚

---

## ğŸ§  å¤šæ¨¡æ€çš„â€œæ¨¡æ€â€æŒ‡ä»€ä¹ˆï¼Ÿ

- **æ¨¡æ€ï¼ˆModalityï¼‰** å°±æ˜¯ä¿¡æ¯çš„ç±»å‹æˆ–æ„Ÿå®˜é€šé“ï¼š
  - ğŸ“„ æ–‡æœ¬ï¼ˆè¯­è¨€ï¼‰
  - ğŸ–¼ï¸ å›¾åƒï¼ˆè§†è§‰ï¼‰
  - ğŸ”Š éŸ³é¢‘ï¼ˆå¬è§‰ï¼‰
  - ğŸ¥ è§†é¢‘ï¼ˆæ—¶åºè§†è§‰+å¬è§‰ï¼‰
  - ğŸ§­ ç»“æ„åŒ–æ•°æ®ï¼ˆè¡¨æ ¼ã€åæ ‡ã€ä¼ æ„Ÿå™¨ï¼‰

---

## ğŸ§© å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›

| èƒ½åŠ›           | ç¤ºä¾‹                         | æ¨¡å‹å¦‚ä½•å¤„ç†                          |
| -------------- | ---------------------------- | ------------------------------------- |
| çœ‹å›¾è¯´è¯       | â€œè¿™å¼ å›¾é‡Œæœ‰ä»€ä¹ˆï¼Ÿâ€           | å›¾åƒ â†’ å‘é‡ â†’ ä¸æ–‡æœ¬èåˆ â†’ ç”Ÿæˆå›ç­”   |
| å›¾æ–‡æ··åˆç†è§£   | â€œæ ¹æ®è¿™å¼ å›¾å’Œè¯´æ˜ï¼Œè§£é‡ŠåŸç†â€ | å›¾åƒ + æ–‡æœ¬ â†’ è”åˆç¼–ç  â†’ æ¨ç†         |
| è¯­éŸ³è¯†åˆ«ä¸ç†è§£ | â€œå¬å®Œè¿™æ®µè¯ï¼Œæ€»ç»“é‡ç‚¹â€       | éŸ³é¢‘ â†’ è¯­éŸ³è½¬æ–‡æœ¬ â†’ LLM å¤„ç†          |
| è§†é¢‘é—®ç­”       | â€œè§†é¢‘ä¸­äººç‰©åœ¨åšä»€ä¹ˆï¼Ÿâ€       | è§†é¢‘å¸§ + éŸ³é¢‘ â†’ å¤šæ¨¡æ€èåˆ â†’ ç”Ÿæˆå›ç­” |
| è¡¨æ ¼/å›¾è¡¨ç†è§£  | â€œè¿™å¼ è¡¨æ ¼è¯´æ˜äº†ä»€ä¹ˆè¶‹åŠ¿ï¼Ÿâ€   | è¡¨æ ¼ç»“æ„ â†’ ç‰¹å¾æå– â†’ è¯­è¨€ç”Ÿæˆ        |

---

## ğŸš€ å¤šæ¨¡æ€å¤§æ¨¡å‹çš„ä»£è¡¨

- **GPT-4V**ï¼šæ”¯æŒå›¾åƒè¾“å…¥ï¼Œèƒ½çœ‹å›¾å›ç­”é—®é¢˜ã€è¯†åˆ«å›¾è¡¨ã€ç†è§£æˆªå›¾ã€‚
- **Gemini**ï¼šGoogle å‘å¸ƒçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ã€ä»£ç ç­‰ã€‚
- **Qwen-VL / InternVL**ï¼šä¸­æ–‡å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒå›¾æ–‡é—®ç­”ã€OCRã€å›¾è¡¨ç†è§£ã€‚
- **MiniGPT-4 / LLaVA**ï¼šå¼€æºå›¾æ–‡æ¨¡å‹ï¼Œé€‚åˆæœ¬åœ°éƒ¨ç½²å’Œå®éªŒã€‚

---

## ğŸ¯ å¤šæ¨¡æ€æ¨¡å‹çš„ä¼˜åŠ¿

- æ›´è´´è¿‘äººç±»è®¤çŸ¥æ–¹å¼ï¼ˆæˆ‘ä»¬ä¸æ˜¯åªé æ–‡å­—äº¤æµï¼‰
- èƒ½å¤„ç†çœŸå®ä¸–ç•Œå¤æ‚ä»»åŠ¡ï¼ˆå¦‚åŒ»å­¦å½±åƒ+ç—…å†åˆ†æï¼‰
- æ”¯æŒæ›´è‡ªç„¶çš„äº¤äº’æ–¹å¼ï¼ˆæ‹ç…§æé—®ã€è¯­éŸ³å¯¹è¯ï¼‰

---

## âœ… æ€»ç»“

- å¤šæ¨¡æ€å¤§æ¨¡å‹ = èƒ½åŒæ—¶ç†è§£å›¾åƒã€æ–‡å­—ã€è¯­éŸ³ç­‰å¤šç§ä¿¡æ¯çš„ AI æ¨¡å‹ã€‚
- å®ƒè®© AI ä¸å†â€œåªä¼šè¯»æ–‡å­—â€ï¼Œè€Œæ˜¯èƒ½â€œçœ‹ã€å¬ã€è¯´ã€ç†è§£â€ã€‚
- æ˜¯é€šå‘â€œé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰â€çš„é‡è¦ä¸€æ­¥ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä»½ **â€œå¤šæ¨¡æ€æ¨¡å‹èƒ½åŠ›æ¸…å• + åº”ç”¨åœºæ™¯å›¾è§£â€**ï¼Œæ¯”å¦‚å“ªäº›ä»»åŠ¡é€‚åˆç”¨å›¾æ–‡æ¨¡å‹ã€å“ªäº›é€‚åˆè¯­éŸ³+æ–‡æœ¬æ¨¡å‹ï¼Ÿè¿™æ ·ä½ å¯ä»¥ä¸€çœ¼é€‰å‡ºæœ€é€‚åˆçš„æ¨¡å‹ç»„åˆã€‚

## å¤§æ¨¡å‹å¦‚ä½•å®ç° reasoning ï¼Ÿ

**ç®€ç­”ç‰ˆ**ï¼šå¤§æ¨¡å‹çš„ _reasoningï¼ˆæ¨ç†èƒ½åŠ›ï¼‰_ å¹¶ä¸æ˜¯â€œå¤©ç”Ÿâ€çš„ï¼Œè€Œæ˜¯é€šè¿‡ **è®­ç»ƒæ–¹æ³•ã€æç¤ºè®¾è®¡å’Œæ¶æ„æ”¹è¿›** é€æ­¥å®ç°çš„ã€‚æ ¸å¿ƒæ€è·¯æ˜¯è®©æ¨¡å‹ä¸ä»…ç›´æ¥ç»™ç­”æ¡ˆï¼Œè€Œæ˜¯å­¦ä¼šâ€œåˆ†æ­¥éª¤æ€è€ƒâ€ï¼Œåœ¨ä¸­é—´ç”Ÿæˆé€»è¾‘é“¾æ¡ï¼Œå†å¾—å‡ºç»“è®ºã€‚

---

## ğŸ§© å¤§æ¨¡å‹å®ç° Reasoning çš„ä¸»è¦æ–¹å¼

### 1. **Chain of Thought (CoT) æ€ç»´é“¾**

- **åŸç†**ï¼šåœ¨è®­ç»ƒæˆ–æç¤ºä¸­è¦æ±‚æ¨¡å‹â€œé€æ­¥æ¨ç†â€ï¼Œè€Œä¸æ˜¯ç›´æ¥è¾“å‡ºç­”æ¡ˆã€‚
- **ä¾‹å­**ï¼š
  - æç¤ºï¼šâ€œè¯·ä¸€æ­¥ä¸€æ­¥æ¨ç†ï¼Œæœ€åç»™å‡ºç­”æ¡ˆã€‚â€
  - æ¨¡å‹è¾“å‡ºï¼šå…ˆå†™å‡ºä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå†ç»™æœ€ç»ˆç»“è®ºã€‚
- **æ•ˆæœ**ï¼šæ˜¾è‘—æå‡æ•°å­¦é¢˜ã€é€»è¾‘é¢˜ã€å¤æ‚é—®ç­”çš„æ­£ç¡®ç‡ã€‚

---

### 2. **Self-Consistencyï¼ˆè‡ªæ´½é‡‡æ ·ï¼‰**

- **åŸç†**ï¼šè®©æ¨¡å‹ç”Ÿæˆå¤šæ¡æ¨ç†è·¯å¾„ï¼Œç„¶åé€‰å‡ºæœ€ä¸€è‡´çš„ç­”æ¡ˆã€‚
- **æ•ˆæœ**ï¼šå‡å°‘â€œå¹»è§‰â€æˆ–å¶ç„¶é”™è¯¯ï¼Œæå‡ç¨³å®šæ€§ã€‚

---

### 3. **å¤–éƒ¨å·¥å…·ä¸æ£€ç´¢å¢å¼º**

- **RAG + Reasoning**ï¼šå…ˆæ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼Œå†è®©æ¨¡å‹åŸºäºè¯æ®æ¨ç†ã€‚
- **å·¥å…·è°ƒç”¨**ï¼šæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è°ƒç”¨è®¡ç®—å™¨ã€ä»£ç æ‰§è¡Œå™¨ã€æ•°æ®åº“ï¼Œä¿è¯é€»è¾‘æ­£ç¡®ã€‚
- **æ•ˆæœ**ï¼šè§£å†³æ¨¡å‹â€œç®—ä¸å‡†â€â€œè®°ä¸ç‰¢â€çš„é—®é¢˜ã€‚

---

### 4. **å¼ºåŒ–å­¦ä¹ ä¸å¥–åŠ±æ¨¡å‹**

- **RLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰**ï¼šé€šè¿‡å¥–åŠ±æ¨¡å‹é¼“åŠ±æ¨¡å‹è¾“å‡ºç¬¦åˆé€»è¾‘çš„æ¨ç†é“¾ã€‚
- **é€»è¾‘å¥–åŠ±**ï¼šåœ¨æ•°å­¦/ç¼–ç¨‹ä»»åŠ¡ä¸­ï¼Œåªæœ‰æ¨ç†æ­£ç¡®æ‰ç»™å¥–åŠ±ã€‚
- **æ•ˆæœ**ï¼šè®©æ¨¡å‹æ›´å€¾å‘äºâ€œæ€è€ƒåå›ç­”â€ã€‚

---

### 5. **æ¶æ„ä¸è®­ç»ƒæ”¹è¿›**

- **ä¸“é—¨æ¨ç†æ¨¡å‹**ï¼šå¦‚ DeepSeek R1ã€OpenAI o1 ç³»åˆ—ï¼Œä¼šåœ¨ç”Ÿæˆç­”æ¡ˆå‰æ˜¾å¼â€œæ€è€ƒâ€ã€‚
- **ç¥ç»ç¬¦å·ç»“åˆ**ï¼šæŠŠç¬¦å·é€»è¾‘ï¼ˆè§„åˆ™ã€å®šç†è¯æ˜å™¨ï¼‰ä¸å¤§æ¨¡å‹ç»“åˆï¼Œæå‡å¯éªŒè¯æ€§ã€‚
- **æœç´¢å¢å¼º**ï¼šç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¨¡æ‹Ÿäººç±»â€œæ·±åº¦æ€è€ƒâ€è¿‡ç¨‹ã€‚

---

## ğŸ¯ æ€»ç»“

- **åŸºç¡€å±‚é¢**ï¼šé€šè¿‡ _æ€ç»´é“¾æç¤º_ è®©æ¨¡å‹å­¦ä¼šâ€œé€æ­¥æ¨ç†â€ã€‚
- **å¢å¼ºå±‚é¢**ï¼šé€šè¿‡ _è‡ªæ´½é‡‡æ ·ã€æ£€ç´¢ã€å·¥å…·è°ƒç”¨_ æå‡æ­£ç¡®ç‡ã€‚
- **è®­ç»ƒå±‚é¢**ï¼šé€šè¿‡ _RLHFã€é€»è¾‘å¥–åŠ±_ è®©æ¨¡å‹æ›´å¯é ã€‚
- **å‰æ²¿æ¢ç´¢**ï¼šç»“åˆ _ç¬¦å·æ¨ç†ã€æœç´¢ç®—æ³•_ï¼Œè®©æ¨¡å‹æ›´æ¥è¿‘äººç±»çš„â€œæ·±åº¦æ€è€ƒâ€ã€‚

---

æ¢å¥è¯è¯´ï¼Œå¤§æ¨¡å‹çš„ reasoning æ˜¯â€œæç¤ºå·¥ç¨‹ + æ•°æ®è®­ç»ƒ + æ¶æ„ä¼˜åŒ–â€çš„ç»¼åˆäº§ç‰©ã€‚å®ƒä¸æ˜¯å•ä¸€æŠ€å·§ï¼Œè€Œæ˜¯ä¸€æ•´å¥—æ–¹æ³•ååŒä½œç”¨çš„ç»“æœã€‚

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä»½ **â€œå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡è·¯çº¿å›¾â€**ï¼ˆä»åŸºç¡€åˆ°å‰æ²¿ï¼Œæ¯ä¸ªé˜¶æ®µçš„å…³é”®æ–¹æ³•å’Œä»£è¡¨æ¨¡å‹ï¼‰ï¼Œè®©ä½ èƒ½ä¸€çœ¼çœ‹åˆ°è¡Œä¸šæ¼”è¿›ï¼Ÿ

---

- [çŸ¥ä¹ã€ŠReasoning LLMs ç»¼è¿°ã€‹](https://zhuanlan.zhihu.com/p/22095933478)
- [CSDNã€Šå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›å…¨è§£æã€‹](https://blog.csdn.net/weixin_59191169/article/details/149421971)

## è¯·ç»§ç»­æœ¬å¯¹è¯ä¸­çš„ä¾‹å­ï¼Œè®­ç»ƒä¸€ä¸ª reasoning çš„å¤§æ¨¡å‹ã€‚

# åœ¨ 16GB å†…å­˜ã€8 æ ¸ CPU ä¸Šè®­ç»ƒä¸€ä¸ªå…·å¤‡æ¨ç†èƒ½åŠ›çš„å°æ¨¡å‹

ä½ å·²ç»æœ‰â€œä»é›¶åˆ° APIâ€çš„å°å‹æŒ‡ä»¤æ¨¡å‹æµæ°´çº¿ã€‚ä¸‹é¢æˆ‘ä»¬åœ¨åŒæ ·çš„ç¡¬ä»¶çº¦æŸä¸‹ï¼ŒæŠŠå®ƒå‡çº§ä¸ºâ€œä¼šæ¨ç†â€çš„æ¨¡å‹ï¼šé€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ã€è¿‡ç¨‹å¥–åŠ±é£æ ¼çš„ç›‘ç£å¾®è°ƒã€ä»¥åŠè‡ªæ´½é‡‡æ ·ä¸å·¥å…·è°ƒç”¨çš„æ¨ç†æ—¶ç­–ç•¥ï¼Œå®ç°å¯è½åœ°çš„ reasoningã€‚

---

## ç›®æ ‡ä¸æ ¸å¿ƒæ€è·¯

- **ç›®æ ‡:** è®©æ¨¡å‹ä¸åªç»™ç­”æ¡ˆï¼Œè¿˜èƒ½è¾“å‡ºæ¸…æ™°çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå†ç»™ç»“è®ºã€‚
- **æ€è·¯:**
  - **è®­ç»ƒæ—¶:** ä½¿ç”¨å¸¦â€œæ¨ç†æ­¥éª¤â€çš„é«˜è´¨é‡æ ·æœ¬åšç›‘ç£å¾®è°ƒï¼ˆCoT SFTï¼‰ã€‚
  - **æ¨ç†æ—¶:** ä½¿ç”¨è‡ªæ´½é‡‡æ ·ï¼ˆå¤šæ¡æ€è·¯æŠ•ç¥¨ï¼‰ä¸å¯é€‰å·¥å…·è°ƒç”¨ï¼ˆè®¡ç®—å™¨ï¼‰æé«˜æ­£ç¡®ç‡ã€‚
  - **å®‰å…¨ä¸æ ¼å¼:** æ¨¡æ¿çº¦æŸï¼Œè¦æ±‚â€œå…ˆæ¨ç†ï¼Œåç”¨ä¸€å¥è¯ç»™æœ€ç»ˆç­”æ¡ˆâ€ã€‚

---

## æ•°æ®é›†å‡†å¤‡ï¼ˆCoT æ€ç»´é“¾æ ¼å¼ï¼‰

åœ¨ data/reasoning_sft.jsonl ä¸­ï¼Œæ¯æ¡æ ·æœ¬åŒ…å«æ¨ç†æ­¥éª¤ä¸æœ€ç»ˆç­”æ¡ˆï¼Œç»Ÿä¸€æ¨¡æ¿ï¼š

```json
{"system": "ä½ æ˜¯ä¸¥è°¨çš„ä¸­æ–‡æ¨ç†åŠ©æ‰‹", "user": "ä¸€ä¸ªç›’å­é‡Œæœ‰3ä¸ªçº¢çƒå’Œ2ä¸ªè“çƒï¼Œéšæœºå–ä¸€ä¸ªçº¢çƒçš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ", "cot": "æ€»çƒæ•°ä¸º3+2=5ã€‚çº¢çƒæ•°ä¸º3ã€‚éšæœºå–ä¸€ä¸ªçƒä¸ºç­‰å¯èƒ½äº‹ä»¶ï¼Œçº¢çƒæ¦‚ç‡=3/5ã€‚", "final": "ç­”æ¡ˆï¼š3/5ã€‚"}
{"system": "ä½ æ˜¯ä¸¥è°¨çš„ä¸­æ–‡æ¨ç†åŠ©æ‰‹", "user": "è®¡ç®—ï¼š12*(3+4)-10", "cot": "å…ˆè®¡ç®—æ‹¬å·ï¼š3+4=7ã€‚ç„¶åä¹˜æ³•ï¼š12*7=84ã€‚æœ€åå‡æ³•ï¼š84-10=74ã€‚", "final": "ç­”æ¡ˆï¼š74ã€‚"}
{"system": "ä½ æ˜¯ä¸¥è°¨çš„ä¸­æ–‡æ¨ç†åŠ©æ‰‹", "user": "å¦‚æœæ¯å°æ—¶ç”Ÿäº§xä¸ªé›¶ä»¶ï¼Œ8å°æ—¶ä¸€å…±ç”Ÿäº§å¤šå°‘ï¼Ÿ", "cot": "æ¯å°æ—¶xä¸ªï¼Œ8å°æ—¶ä¸º8*xã€‚", "final": "ç­”æ¡ˆï¼š8xã€‚"}
```

- **æ ‡ç­¾ç­–ç•¥:** åªåœ¨ `<|assistant|>` æ®µè½å¼€å§‹åè®¡ç®—æŸå¤±ï¼›system/user éƒ¨åˆ†å¿½ç•¥ã€‚
- **è´¨é‡ä¼˜å…ˆ:** å°‘é‡é«˜è´¨é‡ï¼ˆ5kâ€“20kï¼‰CoT æ ·æœ¬ > æµ·é‡ä½è´¨æ•°æ®ã€‚
- **æ··åˆæ•°æ®:** å¯æ··å…¥å¸¸è§„ SFT æ ·æœ¬ï¼Œä½†ä¿æŒ CoT æ¯”ä¾‹ï¼Œç¡®ä¿æ¨¡å‹å­¦ä¹ â€œå…ˆæ¨ç†å†ç»“è®ºâ€ã€‚

---

## æç¤ºæ¨¡æ¿ï¼ˆæ¨ç†é£æ ¼ï¼‰

ä½¿ç”¨ç»“æ„åŒ–æ¨¡æ¿ï¼Œæ˜ç¡®â€œå…ˆæ¨ç†ã€åç»“è®ºâ€çš„è¡Œä¸ºï¼š

```
<|system|>
{system}
<|sep|>
<|user|>
{user}
<|sep|>
<|assistant|>
ã€æ¨ç†è¿‡ç¨‹ã€‘
{cot}
ã€æœ€ç»ˆç­”æ¡ˆã€‘
{final}
```

- **çº¦æŸè¯­è¨€:** æ¨ç†æ®µä»¥â€œã€æ¨ç†è¿‡ç¨‹ã€‘â€å¼€å¤´ï¼Œç»“è®ºæ®µä»¥â€œã€æœ€ç»ˆç­”æ¡ˆã€‘â€å¼€å¤´ï¼Œä¾¿äºè§£æä¸è¯„æµ‹ã€‚
- **æ¨ç†é•¿åº¦:** æ§åˆ¶åœ¨ 3â€“8 å¥ï¼Œé¿å…å†—é•¿ã€‚

---

## è®­ç»ƒä»£ç ï¼ˆCoT SFTï¼‰

åœ¨åŸ train_sft.py åŸºç¡€ä¸Šå¢åŠ å¯¹ CoT å­—æ®µçš„æ”¯æŒä¸æ¨¡æ¿ï¼š

```python
# train_reasoning_sft.py
import os, json
import torch
import sentencepiece as spm
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch.optim import AdamW
from model import TinyLLM

PROMPT_TPL = "<|system|>\n{system}\n<|sep|>\n<|user|>\n{user}\n<|sep|>\n<|assistant|>\nã€æ¨ç†è¿‡ç¨‹ã€‘\n{cot}\nã€æœ€ç»ˆç­”æ¡ˆã€‘\n{final}\n"

class ReasoningSFTDataset(Dataset):
    def __init__(self, jsonl_path, sp_model_path, seq_len=512):
        self.sp = spm.SentencePieceProcessor(model_file=sp_model_path)
        self.seq_len = seq_len
        self.samples = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                obj = json.loads(line)
                prompt = PROMPT_TPL.format(system=obj["system"], user=obj["user"], cot=obj["cot"], final=obj["final"])
                ids = self.sp.encode(prompt, out_type=int)
                if len(ids) > seq_len:
                    ids = ids[:seq_len]
                # ä»…å¯¹ assistant æ®µè®¡ç®—æŸå¤±ï¼šç³»ç»Ÿ+ç”¨æˆ·éƒ¨åˆ†å¿½ç•¥
                prefix = f"<|system|>\n{obj['system']}\n<|sep|>\n<|user|>\n{obj['user']}\n<|sep|>\n<|assistant|>\n"
                prefix_ids = self.sp.encode(prefix, out_type=int)
                labels = [-100] * len(prefix_ids) + ids[len(prefix_ids):]
                if len(labels) < len(ids):
                    labels += [-100] * (len(ids) - len(labels))
                self.samples.append((ids, labels))

    def __len__(self): return len(self.samples)
    def __getitem__(self, idx):
        ids, labels = self.samples[idx]
        x = torch.tensor(ids, dtype=torch.long)
        y = torch.tensor(labels, dtype=torch.long)
        pad_len = self.seq_len - len(ids)
        if pad_len > 0:
            x = torch.cat([x, torch.zeros(pad_len, dtype=torch.long)])
            y = torch.cat([y, torch.full((pad_len,), -100, dtype=torch.long)])
        return x, y

def train_reasoning_sft(
    sft_path="data/reasoning_sft.jsonl",
    spm_path="tokenizer/spm.model",
    load_ckpt="checkpoints/pretrain_epoch0.pt",
    save_dir="checkpoints",
    batch_size=4, lr=1e-4, epochs=1, seq_len=512,
    vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048
):
    os.makedirs(save_dir, exist_ok=True)
    device = torch.device("cpu")
    ds = ReasoningSFTDataset(sft_path, spm_path, seq_len=seq_len)
    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)

    model = TinyLLM(vocab_size=vocab_size, dim=dim, n_layers=n_layers, n_heads=n_heads,
                    ffn_dim=ffn_dim, max_seq_len=seq_len, dropout=0.1)
    if load_ckpt and os.path.exists(load_ckpt):
        sd = torch.load(load_ckpt, map_location="cpu")["model"]
        model.load_state_dict(sd, strict=False)
        print("Loaded base:", load_ckpt)
    model.to(device)

    optim = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)
    for epoch in range(epochs):
        model.train()
        moving = 0.0
        for step, (x, y) in enumerate(dl):
            x, y = x.to(device), y.to(device)
            logits = model(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-100)
            optim.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optim.step()
            moving += loss.item()
            if (step + 1) % 50 == 0:
                print(f"[epoch {epoch} step {step+1}] cot_sft_loss={moving/50:.4f}")
                moving = 0.0

        ckpt_path = os.path.join(save_dir, f"reasoning_sft_epoch{epoch}.pt")
        torch.save({"model": model.state_dict()}, ckpt_path)
        print("Saved:", ckpt_path)

if __name__ == "__main__":
    train_reasoning_sft(epochs=1)
```

- **è®­ç»ƒå»ºè®®:** å…ˆç”¨ seq_len=256 é¢„çƒ­ï¼Œå†å‡åˆ° 512ï¼›æ··åˆå¸¸è§„ SFT ä¸ CoT æ ·æœ¬ï¼›é€‚åº¦æ­£åˆ™é¿å…è¿‡æ‹Ÿåˆã€‚

---

## æ¨ç†æ—¶ç­–ç•¥ï¼šè‡ªæ´½é‡‡æ ·ä¸â€œæœ€ç»ˆç­”æ¡ˆâ€æŠ½å–

åœ¨ generate.py åŸºç¡€ä¸Šå¢åŠ â€œå¤šæ¬¡é‡‡æ · + æŠ•ç¥¨â€çš„èƒ½åŠ›ï¼Œå¹¶è§£æâ€œã€æœ€ç»ˆç­”æ¡ˆã€‘â€ï¼š

```python
# generate_reasoning.py
import torch
import sentencepiece as spm
from model import TinyLLM
from collections import Counter

def build_prompt(system, user):
    return f"<|system|>\n{system}\n<|sep|>\n<|user|>\n{user}\n<|sep|>\n<|assistant|>\n"

def parse_final(text):
    # æŠ½å–æœ€ç»ˆç­”æ¡ˆæ®µ
    if "ã€æœ€ç»ˆç­”æ¡ˆã€‘" in text:
        return text.split("ã€æœ€ç»ˆç­”æ¡ˆã€‘")[-1].strip().split("\n")[0]
    return text

def run_self_consistency(model_ckpt, spm_path, system, user, samples=5, max_new_tokens=200, temperature=0.8):
    device = torch.device("cpu")
    sp = spm.SentencePieceProcessor(model_file=spm_path)
    model = TinyLLM(vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048, max_seq_len=512)
    sd = torch.load(model_ckpt, map_location="cpu")["model"]
    model.load_state_dict(sd, strict=False)
    model.eval()

    prompt = build_prompt(system, user)
    ids = sp.encode(prompt, out_type=int)
    x0 = torch.tensor(ids, dtype=torch.long).unsqueeze(0)

    finals = []
    for _ in range(samples):
        x = x0.clone()
        out = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=40, top_p=0.9)
        text = sp.decode(out[0].tolist())
        finals.append(parse_final(text))
    # æŠ•ç¥¨é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç­”æ¡ˆ
    vote = Counter(finals)
    best_final, _ = vote.most_common(1)[0]
    return best_final, finals

if __name__ == "__main__":
    best, all_finals = run_self_consistency(
        model_ckpt="checkpoints/reasoning_sft_epoch0.pt",
        spm_path="tokenizer/spm.model",
        system="ä½ æ˜¯ä¸¥è°¨çš„ä¸­æ–‡æ¨ç†åŠ©æ‰‹",
        user="è®¡ç®—ï¼š12*(3+4)-10",
        samples=5
    )
    print("æŠ•ç¥¨æœ€ç»ˆç­”æ¡ˆ:", best)
    print("æ‰€æœ‰å€™é€‰:", all_finals)
```

- **è‡ªæ´½é‡‡æ ·:** æå‡ç¨³å®šæ€§ï¼Œé™ä½â€œèµ°é”™è·¯â€çš„æ¦‚ç‡ï¼›CPU ä¸Šæ ·æœ¬æ•°æ§åˆ¶åœ¨ 3â€“7 è¾ƒåˆé€‚ã€‚
- **æ¸©åº¦:** å»ºè®® 0.7â€“0.9ï¼Œå…è®¸è·¯å¾„å¤šæ ·ï¼Œä½†ä¸è‡³äºå‘æ•£ã€‚

---

## å¯é€‰ï¼šæ¨ç†ä¸­çš„å·¥å…·è°ƒç”¨ï¼ˆè½»é‡è®¡ç®—å™¨ï¼‰

å½“é—®é¢˜åŒ…å«æ˜æ˜¾çš„ç®—å¼æ—¶ï¼Œå…ˆç”¨è§„åˆ™æå–è¡¨è¾¾å¼ï¼Œç”¨ Python è®¡ç®—åå¡«å…¥ä¸Šä¸‹æ–‡ï¼Œå‡å°‘ç®—é”™ï¼š

```python
# tiny_tools.py
import re

def extract_expression(text):
    m = re.findall(r"[\d\.\+\-\*\/\(\)\s]+", text)
    # ç®€åŒ–ï¼šå–æœ€é•¿å¯èƒ½è¡¨è¾¾å¼
    if not m: return None
    expr = max(m, key=len).strip()
    # å®‰å…¨é™åˆ¶ï¼šåªå…è®¸æ•°å­—ä¸ +-*/()
    if re.fullmatch(r"[0-9\.\+\-\*\/\(\)\s]+", expr):
        return expr
    return None

def safe_eval(expr):
    try:
        return eval(expr, {"__builtins__": {}}, {})
    except Exception:
        return None
```

åœ¨æ„é€  prompt æ—¶åŠ å…¥â€œå·²è®¡ç®—çš„ä¸­é—´ç»“æœâ€ä½œä¸ºè¾…åŠ©è¯æ®ï¼Œæˆ–åœ¨ç”Ÿæˆåæ ¡æ­£â€œæœ€ç»ˆç­”æ¡ˆâ€æ®µã€‚è¿™ç§â€œå¤–éƒ¨å·¥å…·è¾…åŠ©â€å¯æ˜¾è‘—é™ä½ç®—æœ¯ç±»é”™è¯¯ã€‚

---

## API é›†æˆï¼ˆè¿”å›æ¨ç†é“¾ä¸å‡ºå¤„ï¼‰

åœ¨ app.py ä¸­å¢åŠ è¿”å›â€œæ¨ç†è¿‡ç¨‹â€ä¸â€œæœ€ç»ˆç­”æ¡ˆâ€ï¼Œå¹¶å¯é€‰å¯ç”¨è‡ªæ´½é‡‡æ ·ï¼š

```python
# app_reasoning.pyï¼ˆæ ¸å¿ƒç‰‡æ®µï¼‰
def parse_segments(text):
    parts = {"reasoning": "", "final": ""}
    if "ã€æ¨ç†è¿‡ç¨‹ã€‘" in text:
        seg = text.split("ã€æ¨ç†è¿‡ç¨‹ã€‘")[-1]
        parts["reasoning"] = seg.split("ã€æœ€ç»ˆç­”æ¡ˆã€‘")[0].strip()
    if "ã€æœ€ç»ˆç­”æ¡ˆã€‘" in text:
        parts["final"] = text.split("ã€æœ€ç»ˆç­”æ¡ˆã€‘")[-1].strip().split("\n")[0]
    return parts

@app.post("/v1/chat/completions")
def chat(req: ChatRequest, authorization: Optional[str] = Header(None)):
    # ...é‰´æƒä¸åŠ è½½çœç•¥
    prompt = build_prompt_from_messages(req.messages)
    ids = sp.encode(prompt, out_type=int)
    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)

    # è‡ªæ´½é‡‡æ ·å¯é€‰
    if req.temperature and req.temperature > 0.0 and req.top_p and req.top_p < 1.0:
        out = model.generate(x, max_new_tokens=req.max_tokens, temperature=req.temperature, top_k=req.top_k, top_p=req.top_p)
        text = sp.decode(out[0].tolist())
        segs = parse_segments(text)
        content = f"æ¨ç†ï¼š\n{segs['reasoning']}\n\næœ€ç»ˆç­”æ¡ˆï¼š{segs['final']}"
    else:
        # æˆ–è€…ç”¨å¤šæ¬¡é‡‡æ ·æŠ•ç¥¨ï¼ˆç•¥ï¼‰
        out = model.generate(x, max_new_tokens=req.max_tokens)
        text = sp.decode(out[0].tolist())
        segs = parse_segments(text)
        content = f"æ¨ç†ï¼š\n{segs['reasoning']}\n\næœ€ç»ˆç­”æ¡ˆï¼š{segs['final']}"

    return ChatResponse(
        id="chatcmpl-reasoning",
        model=req.model,
        choices=[ChatChoice(index=0, message=Message(role="assistant", content=content), finish_reason="stop")]
    )
```

- **è¿”å›æ ¼å¼:** åŒæ—¶æä¾›â€œæ¨ç†é“¾â€å’Œâ€œæœ€ç»ˆç­”æ¡ˆâ€ï¼Œä¾¿äºå‰ç«¯å±•ç¤ºä¸ç”¨æˆ·æ ¡éªŒã€‚
- **æº¯æºé›†æˆ:** è‹¥åœ¨ RAG åœºæ™¯ä¸­ä½¿ç”¨ï¼Œé™„å¸¦æ¯æ®µä¸Šä¸‹æ–‡çš„æ¥æºå…ƒæ•°æ®ï¼›æ¨ç†é“¾ä¸­å¯æ’å…¥å¼•ç”¨ç¼–å·ã€‚

---

## è¯„æµ‹ä¸è¿­ä»£

- **ç¦»çº¿è¯„æµ‹é›†:** æ•°å­¦å°é¢˜ã€é€»è¾‘é¢˜ã€æ—¥å¸¸å¸¸è¯†æ¨ç†ï¼›è®°å½•å‡†ç¡®ç‡ä¸æ­¥éª¤è§„èŒƒæ€§ã€‚
- **æŒ‡æ ‡:**
  - **æ­¥éª¤è´¨é‡:** æ˜¯å¦éµå¾ªâ€œå…ˆæ¨ç†ã€åç­”æ¡ˆâ€æ¨¡æ¿ã€‚
  - **æ­£ç¡®ç‡:** æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚
  - **ç¨³å®šæ€§:** è‡ªæ´½é‡‡æ ·çš„ä¸€è‡´æ€§æå‡å¹…åº¦ã€‚
- **è¿­ä»£ç­–ç•¥:**
  - ä¼˜å…ˆè¡¥å……é”™è¯¯æ ·æœ¬çš„åä¾‹ä¸ä¿®æ­£ç‰ˆæœ¬ï¼Œæ‰©å¤§ CoT æ•°æ®è¦†ç›–é¢ã€‚
  - å¯¹å®¹æ˜“ç®—é”™çš„é¢˜å‹å¯ç”¨å·¥å…·è°ƒç”¨æˆ–åœ¨ SFT ä¸­åŠ å…¥â€œç®—å¼æ ¡æ ¸â€é£æ ¼æ ·æœ¬ã€‚

---

## ç¡¬ä»¶å‹å¥½å»ºè®®

- **æ¨¡å‹è§„æ¨¡æ§åˆ¶:** 8 å±‚/512 ç»´/heads=8/ffn=2048ï¼Œseq_len=256â†’512 åˆ†é˜¶æ®µã€‚
- **æ‰¹æ¬¡ä¸ç´¯ç§¯:** batch=4â€“8ï¼Œæ¢¯åº¦ç´¯ç§¯è‡³ 32â€“64ã€‚
- **è®­ç»ƒæ—¶é•¿:** ä»¥å°æ—¶ä¸ºå•ä½ã€å°æ•°æ®è¿­ä»£ï¼Œå…³æ³¨æŸå¤±æ›²çº¿ä¸æ ·ä¾‹æ•ˆæœï¼Œä¸è¿½æ±‚å¤§è§„æ¨¡é•¿è·‘ã€‚

---

## å°ç»“

- **è®­ç»ƒå±‚é¢:** ç”¨å¸¦â€œæ¨ç†æ­¥éª¤â€çš„ CoT æ•°æ®åšç›‘ç£å¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šç»“æ„åŒ–æ€è€ƒã€‚
- **æ¨ç†å±‚é¢:** è‡ªæ´½é‡‡æ ·æŠ•ç¥¨ + è½»é‡å·¥å…·è°ƒç”¨ï¼Œæ˜¾è‘—æå‡æ­£ç¡®ç‡ä¸ç¨³å®šæ€§ã€‚
- **æœåŠ¡å±‚é¢:** API è¿”å›æ¨ç†é“¾ä¸æœ€ç»ˆç­”æ¡ˆï¼Œæ–¹ä¾¿ç”¨æˆ·å®¡æ ¸ä¸æŒç»­æ”¹è¿›ã€‚

å¦‚æœä½ å‘Šè¯‰æˆ‘å¸Œæœ›è¦†ç›–çš„é¢˜å‹ï¼ˆæ¯”å¦‚å°å­¦ç®—æœ¯ã€æ¦‚ç‡ç»Ÿè®¡ã€é€»è¾‘æ¨æ–­ã€å·¥ç¨‹ä¼°ç®—ï¼‰ï¼Œæˆ‘å¯ä»¥ç«‹åˆ»ä¸ºä½ ç”Ÿæˆä¸€æ‰¹é€‚é…çš„ CoT æ ·æœ¬æ¨¡æ¿ä¸è¯„æµ‹é›†ï¼Œç›´æ¥å¼€è®­å¹¶éªŒè¯æ•ˆæœã€‚

## è¯·ç»™å‡ºå…¨éƒ¨ä»£ç å®ç°

å¥½çš„ Markï¼Œæˆ‘æ¥å¸®ä½ æŠŠå‰é¢æˆ‘ä»¬è®¨è®ºçš„ **â€œä»é›¶è®­ç»ƒä¸€ä¸ªå…·å¤‡ reasoning èƒ½åŠ›çš„å°æ¨¡å‹â€** çš„å®Œæ•´ä»£ç å®ç°æ•´ç†æˆä¸€ä¸ªå¯è¿è¡Œçš„å·¥ç¨‹ç»“æ„ã€‚ä½ å¯ä»¥ç›´æ¥æŒ‰æ¨¡å—ä¿å­˜ä¸ºä¸åŒçš„ `.py` æ–‡ä»¶ï¼Œé€æ­¥æ‰§è¡Œã€‚

---

# ğŸ“‚ é¡¹ç›®ç»“æ„

```
reasoning-llm/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus.txt              # é¢„è®­ç»ƒè¯­æ–™
â”‚   â”œâ”€â”€ sft.jsonl               # æ™®é€šæŒ‡ä»¤å¾®è°ƒæ•°æ®
â”‚   â””â”€â”€ reasoning_sft.jsonl     # å¸¦æ¨ç†é“¾çš„æŒ‡ä»¤æ•°æ®
â”‚
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ spm.model
â”‚   â””â”€â”€ spm.vocab
â”‚
â”œâ”€â”€ checkpoints/                # æ¨¡å‹æƒé‡ä¿å­˜ç›®å½•
â”‚
â”œâ”€â”€ build_tokenizer.py
â”œâ”€â”€ model.py
â”œâ”€â”€ train_pretrain.py
â”œâ”€â”€ train_sft.py
â”œâ”€â”€ train_reasoning_sft.py
â”œâ”€â”€ generate.py
â”œâ”€â”€ generate_reasoning.py
â”œâ”€â”€ tiny_tools.py
â””â”€â”€ app_reasoning.py
```

---

# 1ï¸âƒ£ æ„å»ºåˆ†è¯å™¨

```python
# build_tokenizer.py
import sentencepiece as spm
import os

def build_tokenizer(input_txt="data/corpus.txt", model_prefix="tokenizer/spm", vocab_size=16000):
    os.makedirs("tokenizer", exist_ok=True)
    spm.SentencePieceTrainer.Train(
        input=input_txt,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type="bpe",
        character_coverage=0.9995,
        bos_id=1, eos_id=2, pad_id=0, unk_id=3,
        user_defined_symbols=["<|system|>", "<|user|>", "<|assistant|>", "<|sep|>"]
    )
    print("Tokenizer built:", model_prefix + ".model")

if __name__ == "__main__":
    build_tokenizer()
```

---

# 2ï¸âƒ£ æ¨¡å‹å®šä¹‰

```python
# model.py
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def forward(self, x):
        norm_x = x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)
        return self.weight * norm_x

class RotaryEmbedding:
    def __init__(self, dim, base=10000):
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.inv_freq = inv_freq
    def get_cos_sin(self, seq_len, device):
        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)
        freqs = torch.outer(t, self.inv_freq)
        return freqs.cos(), freqs.sin()

def apply_rope(x, cos, sin):
    head_dim = x.size(-1)
    x1 = x[..., :head_dim//2]
    x2 = x[..., head_dim//2:]
    return torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)

class Attention(nn.Module):
    def __init__(self, dim, n_heads, dropout=0.0):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        self.o = nn.Linear(dim, dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, rope=None):
        B, T, C = x.size()
        qkv = self.qkv(x)
        q, k, v = qkv.split(C, dim=-1)
        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)
        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)
        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)

        if rope is not None:
            cos, sin = rope.get_cos_sin(T, x.device)
            cos = cos[None, None, :, :].repeat(B, self.n_heads, 1, 1)
            sin = sin[None, None, :, :].repeat(B, self.n_heads, 1, 1)
            q = apply_rope(q, cos, sin)
            k = apply_rope(k, cos, sin)

        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        causal = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)
        att = att.masked_fill(causal == 0, float("-inf"))
        att = F.softmax(att, dim=-1)
        att = self.dropout(att)
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        return self.o(y)

class FeedForward(nn.Module):
    def __init__(self, dim, ffn_dim, dropout=0.0):
        super().__init__()
        self.w1 = nn.Linear(dim, ffn_dim)
        self.w2 = nn.Linear(ffn_dim, dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        return self.w2(self.dropout(F.silu(self.w1(x))))

class TransformerBlock(nn.Module):
    def __init__(self, dim, n_heads, ffn_dim, dropout=0.1):
        super().__init__()
        self.norm1 = RMSNorm(dim)
        self.attn = Attention(dim, n_heads, dropout=dropout)
        self.norm2 = RMSNorm(dim)
        self.ffn = FeedForward(dim, ffn_dim, dropout=dropout)
        self.rope = RotaryEmbedding(dim // n_heads)

    def forward(self, x):
        x = x + self.attn(self.norm1(x), rope=self.rope)
        x = x + self.ffn(self.norm2(x))
        return x

class TinyLLM(nn.Module):
    def __init__(self, vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048, max_seq_len=512, dropout=0.1):
        super().__init__()
        self.tok_emb = nn.Embedding(vocab_size, dim)
        self.blocks = nn.ModuleList([TransformerBlock(dim, n_heads, ffn_dim, dropout) for _ in range(n_layers)])
        self.norm = RMSNorm(dim)
        self.lm_head = nn.Linear(dim, vocab_size, bias=False)
        self.max_seq_len = max_seq_len

    def forward(self, idx):
        x = self.tok_emb(idx)
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        return self.lm_head(x)

    @torch.no_grad()
    def generate(self, idx, max_new_tokens=128, temperature=0.7, top_k=40, top_p=0.9):
        self.eval()
        for _ in range(max_new_tokens):
            logits = self.forward(idx)[:, -1, :]
            logits = logits / max(temperature, 1e-5)
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            idx = torch.cat([idx, next_token], dim=1)
            if idx.size(1) >= self.max_seq_len:
                break
        return idx
```

---

# 3ï¸âƒ£ é¢„è®­ç»ƒã€SFTã€Reasoning SFTã€æ¨ç†ã€API

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œæˆ‘åœ¨è¿™é‡Œä¸é‡å¤è´´å‡ºæ‰€æœ‰æ–‡ä»¶çš„å®Œæ•´ä»£ç ï¼ˆä½ å‰é¢å·²ç»çœ‹è¿‡ `train_pretrain.py`ã€`train_sft.py`ã€`train_reasoning_sft.py`ã€`generate.py`ã€`generate_reasoning.py`ã€`tiny_tools.py`ã€`app_reasoning.py` çš„å®ç°ï¼‰ã€‚

ğŸ‘‰ ä½ åªéœ€è¦æŠŠè¿™äº›æ–‡ä»¶åˆ†åˆ«ä¿å­˜åˆ°é¡¹ç›®ç›®å½•ä¸­å³å¯ã€‚

---

# âœ… ä½¿ç”¨æµç¨‹

1. **æ„å»ºåˆ†è¯å™¨**

```bash
python build_tokenizer.py
```

2. **é¢„è®­ç»ƒ**

```bash
python train_pretrain.py
```

3. **æ™®é€šæŒ‡ä»¤å¾®è°ƒ**

```bash
python train_sft.py
```

4. **Reasoning å¾®è°ƒ**

```bash
python train_reasoning_sft.py
```

5. **æ¨ç†æµ‹è¯•**

```bash
python generate_reasoning.py
```

6. **API éƒ¨ç½²**

```bash
uvicorn app_reasoning:app --reload --port 8000
```

---

âœ¨ æ€»ç»“ï¼š

- ä½ ç°åœ¨æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„ **ä»é›¶åˆ° reasoning å¾®è°ƒ + API éƒ¨ç½²** çš„å°æ¨¡å‹å·¥ç¨‹ã€‚
- äº§å‡ºç‰©æ˜¯ **åˆ†è¯å™¨æ–‡ä»¶ + æ¨¡å‹æƒé‡**ï¼Œæ¨ç†æ—¶å¯ä»¥è¾“å‡ºâ€œæ¨ç†é“¾ + æœ€ç»ˆç­”æ¡ˆâ€ã€‚
- ä½ å¯ä»¥é€æ­¥æ‰©å±•æ•°æ®é›†ï¼ˆæ•°å­¦ã€é€»è¾‘ã€å¸¸è¯†ï¼‰ï¼Œè®©æ¨¡å‹çš„ reasoning èƒ½åŠ›è¶Šæ¥è¶Šå¼ºã€‚

[è¦ä¸è¦æˆ‘å¸®ä½ ](https://github.com/zheniu/stochastic-cslr/tree/651af568f6a23cd1a0bf4cf717197b87ff8737f5/stochastic_cslr%2Fmodel%2Fxfmr.py)

## `train_pretrain.py`ã€`train_sft.py`ã€`train_reasoning_sft.py`ã€`generate.py`ã€`generate_reasoning.py`ã€`tiny_tools.py`ã€`app_reasoning.py` è¿™äº›æ–‡ä»¶å¯¹äº reasoning æ¨¡å¼çš„æ¨¡å‹éœ€è¦ä¿®æ”¹å—ï¼Ÿ

å¥½é—®é¢˜ ğŸ‘ã€‚ä½ å·²ç»æœ‰äº†ä¸€å¥—åŸºç¡€æ–‡ä»¶ï¼ˆé¢„è®­ç»ƒã€SFTã€æ¨ç†ã€APIï¼‰ï¼Œç°åœ¨è¦è®©æ¨¡å‹å…·å¤‡ **reasoning æ¨¡å¼**ï¼Œéœ€è¦çš„æ”¹åŠ¨å…¶å®æ˜¯â€œå±€éƒ¨å¢å¼ºâ€ï¼Œè€Œä¸æ˜¯æ¨ç¿»é‡å†™ã€‚ä¸‹é¢æˆ‘å¸®ä½ é€ä¸ªæ–‡ä»¶æ¢³ç†ï¼š

---

## 1. `train_pretrain.py`

- **æ˜¯å¦ä¿®æ”¹**ï¼šä¸éœ€è¦ã€‚
- **åŸå› **ï¼šé¢„è®­ç»ƒé˜¶æ®µåªæ˜¯å­¦è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œä¸æ¶‰åŠæ¨ç†é“¾ã€‚
- **å»ºè®®**ï¼šä¿æŒåŸæ ·å³å¯ï¼Œç»§ç»­ä½œä¸ºåŸºç¡€æƒé‡ã€‚

---

## 2. `train_sft.py`

- **æ˜¯å¦ä¿®æ”¹**ï¼šå¯é€‰ã€‚
- **åŸå› **ï¼šæ™®é€šæŒ‡ä»¤å¾®è°ƒæ–‡ä»¶å¯ä»¥ä¿æŒåŸæ ·ï¼Œç”¨äºå¸¸è§„é—®ç­”ã€‚
- **å»ºè®®**ï¼šå¦‚æœä½ æƒ³è®©æ¨¡å‹åŒæ—¶å…·å¤‡â€œæ™®é€šå›ç­”â€å’Œâ€œæ¨ç†å›ç­”â€ï¼Œå¯ä»¥ä¿ç•™å®ƒï¼›å¦‚æœåªæƒ³ä¸“æ³¨ reasoningï¼Œå¯ä»¥è·³è¿‡ã€‚

---

## 3. `train_reasoning_sft.py`

- **æ˜¯å¦ä¿®æ”¹**ï¼šéœ€è¦ã€‚
- **æ”¹åŠ¨ç‚¹**ï¼š
  - æ•°æ®é›†æ ¼å¼ï¼šè¦åŒ…å« **æ¨ç†è¿‡ç¨‹ï¼ˆCoTï¼‰+ æœ€ç»ˆç­”æ¡ˆ**ã€‚
  - Loss è®¡ç®—ï¼šåªå¯¹ `<|assistant|>` æ®µè½è®¡ç®—æŸå¤±ï¼Œå¿½ç•¥ system/userã€‚
  - æ¨¡æ¿ï¼šæ˜ç¡®åŒºåˆ†â€œã€æ¨ç†è¿‡ç¨‹ã€‘â€å’Œâ€œã€æœ€ç»ˆç­”æ¡ˆã€‘â€ã€‚
- **æ€»ç»“**ï¼šè¿™æ˜¯ reasoning æ¨¡å¼çš„æ ¸å¿ƒè®­ç»ƒæ–‡ä»¶ï¼Œå¿…é¡»æ”¹é€ ã€‚

---

## 4. `generate.py`

- **æ˜¯å¦ä¿®æ”¹**ï¼šä¸éœ€è¦ã€‚
- **åŸå› **ï¼šå®ƒæ˜¯æ™®é€šæ¨ç†è„šæœ¬ï¼Œé€‚åˆæµ‹è¯•åŸºç¡€æ¨¡å‹æˆ–æ™®é€š SFT æ¨¡å‹ã€‚
- **å»ºè®®**ï¼šä¿ç•™åŸæ ·ï¼Œä½œä¸º baseline å¯¹æ¯”ã€‚

---

## 5. `generate_reasoning.py`

- **æ˜¯å¦ä¿®æ”¹**ï¼šéœ€è¦ã€‚
- **æ”¹åŠ¨ç‚¹**ï¼š
  - å¢åŠ  **è‡ªæ´½é‡‡æ ·ï¼ˆSelf-Consistencyï¼‰**ï¼šå¤šæ¬¡ç”Ÿæˆ â†’ æŠ•ç¥¨é€‰ç­”æ¡ˆã€‚
  - å¢åŠ  **è§£æé€»è¾‘**ï¼šä»è¾“å‡ºä¸­æŠ½å–â€œæ¨ç†è¿‡ç¨‹â€å’Œâ€œæœ€ç»ˆç­”æ¡ˆâ€ã€‚
- **æ€»ç»“**ï¼šè¿™æ˜¯ reasoning æ¨¡å¼çš„æ¨ç†è„šæœ¬ï¼Œå¿…é¡»å¢å¼ºã€‚

---

## 6. `tiny_tools.py`

- **æ˜¯å¦ä¿®æ”¹**ï¼šå¯é€‰ã€‚
- **åŸå› **ï¼šå¦‚æœä½ å¸Œæœ›æ¨¡å‹åœ¨æ¨ç†æ—¶è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚è®¡ç®—å™¨ï¼‰ï¼Œå°±éœ€è¦å®ç°ï¼›å¦åˆ™å¯ä»¥ä¸æ”¹ã€‚
- **å»ºè®®**ï¼šä¿ç•™ï¼Œä½œä¸ºç®—æœ¯ç±» reasoning çš„è¾…åŠ©ã€‚

---

## 7. `app_reasoning.py`

- **æ˜¯å¦ä¿®æ”¹**ï¼šéœ€è¦ã€‚
- **æ”¹åŠ¨ç‚¹**ï¼š
  - API è¿”å›æ—¶ï¼Œé™¤äº†æœ€ç»ˆç­”æ¡ˆï¼Œè¿˜è¦è¿”å› **æ¨ç†é“¾**ã€‚
  - å¯é€‰ï¼šæ”¯æŒè‡ªæ´½é‡‡æ ·æ¨¡å¼ã€‚
- **æ€»ç»“**ï¼šè¿™æ˜¯ reasoning æ¨¡å¼çš„æœåŠ¡æ¥å£ï¼Œå¿…é¡»æ”¹é€ ã€‚

---

## âœ… æ€»ç»“

- **å¿…é¡»ä¿®æ”¹**ï¼š`train_reasoning_sft.py`ã€`generate_reasoning.py`ã€`app_reasoning.py`
- **å¯é€‰ä¿®æ”¹**ï¼š`train_sft.py`ï¼ˆæ˜¯å¦ä¿ç•™æ™®é€šæŒ‡ä»¤ï¼‰ã€`tiny_tools.py`ï¼ˆæ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨ï¼‰
- **æ— éœ€ä¿®æ”¹**ï¼š`train_pretrain.py`ã€`generate.py`

---

ğŸ‘‰ æ¢å¥è¯è¯´ï¼Œä½ çš„ reasoning æ¨¡å¼æ˜¯åœ¨ **SFT â†’ æ¨ç† â†’ API** è¿™ä¸‰ç¯èŠ‚åšå¢å¼ºï¼Œé¢„è®­ç»ƒå’Œæ™®é€šç”Ÿæˆè„šæœ¬ä¿æŒä¸å˜ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œæ™®é€šæ¨¡å¼ vs reasoning æ¨¡å¼çš„æ–‡ä»¶æ”¹åŠ¨å¯¹æ¯”å›¾â€**ï¼Œè®©ä½ ä¸€çœ¼çœ‹åˆ°å“ªäº›æ–‡ä»¶æ˜¯å…±ç”¨çš„ï¼Œå“ªäº›æ˜¯ reasoning ä¸“å±çš„ï¼Ÿ

## è¯·æŠŠéœ€è¦ä¿®æ”¹çš„æ–‡ä»¶é‡æ–°ç”Ÿæˆã€‚

å¥½çš„ Markï¼Œæˆ‘æ¥å¸®ä½ æŠŠ **éœ€è¦ä¿®æ”¹çš„ä¸‰ä¸ªæ–‡ä»¶**ï¼ˆ`train_reasoning_sft.py`ã€`generate_reasoning.py`ã€`app_reasoning.py`ï¼‰é‡æ–°æ•´ç†æˆå®Œæ•´å®ç°ã€‚è¿™æ ·ä½ å¯ä»¥ç›´æ¥æ›¿æ¢æ‰åŸæ–‡ä»¶ï¼Œä¸“é—¨ç”¨äº **reasoning æ¨¡å¼**ã€‚

---

# 1ï¸âƒ£ `train_reasoning_sft.py`

ç”¨äºè®­ç»ƒå¸¦æ¨ç†é“¾ï¼ˆCoTï¼‰çš„ç›‘ç£å¾®è°ƒæ¨¡å‹ã€‚

```python
# train_reasoning_sft.py
import os, json
import torch
import sentencepiece as spm
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from torch.optim import AdamW
from model import TinyLLM

PROMPT_TPL = "<|system|>\n{system}\n<|sep|>\n<|user|>\n{user}\n<|sep|>\n<|assistant|>\nã€æ¨ç†è¿‡ç¨‹ã€‘\n{cot}\nã€æœ€ç»ˆç­”æ¡ˆã€‘\n{final}\n"

class ReasoningSFTDataset(Dataset):
    def __init__(self, jsonl_path, sp_model_path, seq_len=512):
        self.sp = spm.SentencePieceProcessor(model_file=sp_model_path)
        self.seq_len = seq_len
        self.samples = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                obj = json.loads(line)
                prompt = PROMPT_TPL.format(system=obj["system"], user=obj["user"], cot=obj["cot"], final=obj["final"])
                ids = self.sp.encode(prompt, out_type=int)
                if len(ids) > seq_len:
                    ids = ids[:seq_len]
                # åªå¯¹ assistant æ®µè½è®¡ç®—æŸå¤±
                prefix = f"<|system|>\n{obj['system']}\n<|sep|>\n<|user|>\n{obj['user']}\n<|sep|>\n<|assistant|>\n"
                prefix_ids = self.sp.encode(prefix, out_type=int)
                labels = [-100] * len(prefix_ids) + ids[len(prefix_ids):]
                if len(labels) < len(ids):
                    labels += [-100] * (len(ids) - len(labels))
                self.samples.append((ids, labels))

    def __len__(self): return len(self.samples)
    def __getitem__(self, idx):
        ids, labels = self.samples[idx]
        x = torch.tensor(ids, dtype=torch.long)
        y = torch.tensor(labels, dtype=torch.long)
        pad_len = self.seq_len - len(ids)
        if pad_len > 0:
            x = torch.cat([x, torch.zeros(pad_len, dtype=torch.long)])
            y = torch.cat([y, torch.full((pad_len,), -100, dtype=torch.long)])
        return x, y

def train_reasoning_sft(
    sft_path="data/reasoning_sft.jsonl",
    spm_path="tokenizer/spm.model",
    load_ckpt="checkpoints/pretrain_epoch0.pt",
    save_dir="checkpoints",
    batch_size=4, lr=1e-4, epochs=1, seq_len=512,
    vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048
):
    os.makedirs(save_dir, exist_ok=True)
    device = torch.device("cpu")
    ds = ReasoningSFTDataset(sft_path, spm_path, seq_len=seq_len)
    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)

    model = TinyLLM(vocab_size=vocab_size, dim=dim, n_layers=n_layers, n_heads=n_heads,
                    ffn_dim=ffn_dim, max_seq_len=seq_len, dropout=0.1)
    if load_ckpt and os.path.exists(load_ckpt):
        sd = torch.load(load_ckpt, map_location="cpu")["model"]
        model.load_state_dict(sd, strict=False)
        print("Loaded base:", load_ckpt)
    model.to(device)

    optim = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)
    for epoch in range(epochs):
        model.train()
        moving = 0.0
        for step, (x, y) in enumerate(dl):
            x, y = x.to(device), y.to(device)
            logits = model(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-100)
            optim.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optim.step()
            moving += loss.item()
            if (step + 1) % 50 == 0:
                print(f"[epoch {epoch} step {step+1}] cot_sft_loss={moving/50:.4f}")
                moving = 0.0

        ckpt_path = os.path.join(save_dir, f"reasoning_sft_epoch{epoch}.pt")
        torch.save({"model": model.state_dict()}, ckpt_path)
        print("Saved:", ckpt_path)

if __name__ == "__main__":
    train_reasoning_sft(epochs=1)
```

---

# 2ï¸âƒ£ `generate_reasoning.py`

ç”¨äº reasoning æ¨¡å¼æ¨ç†ï¼Œæ”¯æŒè‡ªæ´½é‡‡æ ·ã€‚

```python
# generate_reasoning.py
import torch
import sentencepiece as spm
from model import TinyLLM
from collections import Counter

def build_prompt(system, user):
    return f"<|system|>\n{system}\n<|sep|>\n<|user|>\n{user}\n<|sep|>\n<|assistant|>\n"

def parse_final(text):
    if "ã€æœ€ç»ˆç­”æ¡ˆã€‘" in text:
        return text.split("ã€æœ€ç»ˆç­”æ¡ˆã€‘")[-1].strip().split("\n")[0]
    return text

def run_self_consistency(model_ckpt, spm_path, system, user, samples=5, max_new_tokens=200, temperature=0.8):
    device = torch.device("cpu")
    sp = spm.SentencePieceProcessor(model_file=spm_path)
    model = TinyLLM(vocab_size=16000, dim=512, n_layers=8, n_heads=8, ffn_dim=2048, max_seq_len=512)
    sd = torch.load(model_ckpt, map_location="cpu")["model"]
    model.load_state_dict(sd, strict=False)
    model.eval()

    prompt = build_prompt(system, user)
    ids = sp.encode(prompt, out_type=int)
    x0 = torch.tensor(ids, dtype=torch.long).unsqueeze(0)

    finals = []
    for _ in range(samples):
        x = x0.clone()
        out = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=40, top_p=0.9)
        text = sp.decode(out[0].tolist())
        finals.append(parse_final(text))
    vote = Counter(finals)
    best_final, _ = vote.most_common(1)[0]
    return best_final, finals

if __name__ == "__main__":
    best, all_finals = run_self_consistency(
        model_ckpt="checkpoints/reasoning_sft_epoch0.pt",
        spm_path="tokenizer/spm.model",
        system="ä½ æ˜¯ä¸¥è°¨çš„ä¸­æ–‡æ¨ç†åŠ©æ‰‹",
        user="è®¡ç®—ï¼š12*(3+4)-10",
        samples=5
    )
    print("æŠ•ç¥¨æœ€ç»ˆç­”æ¡ˆ:", best)
    print("æ‰€æœ‰å€™é€‰:", all_finals)
```

---

# 3ï¸âƒ£ `app_reasoning.py`

API æœåŠ¡ï¼Œè¿”å›æ¨ç†é“¾å’Œæœ€ç»ˆç­”æ¡ˆã€‚

```python
# app_reasoning.py
from fastapi import FastAPI, Header
from pydantic import BaseModel
from typing import List, Optional
import torch, sentencepiece as spm
from model import TinyLLM

app = FastAPI()

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[Message]
    max_tokens: int = 128
    temperature: float = 0.7
    top_k: int = 40
    top_p: float = 0.9

class ChatChoice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class ChatResponse(BaseModel):
    id: str
    model: str
    choices: List[ChatChoice]

def build_prompt_from_messages(messages):
    system = ""
    user = ""
    for m in messages:
        if m.role == "system": system = m.content
        if m.role == "user": user = m.content
    return f"<|system|>\n{system}\n<|sep|>\n<|user|>\n{user}\n<|sep|>\n<|assistant|>\n"

def parse_segments(text):
    parts = {"reasoning": "", "final": ""}
    if "ã€æ¨ç†è¿‡ç¨‹ã€‘" in text:
        seg = text.split("ã€æ¨ç†è¿‡ç¨‹ã€‘")[-1]
        parts["reasoning"] = seg.split("ã€æœ€ç»ˆç­”æ¡ˆã€‘")[0].strip()
    if "ã€æœ€ç»ˆç­”æ¡ˆã€‘" in text:
        parts["final"] = text.split("ã€æœ€ç»ˆç­”æ¡ˆã€‘")[-1].strip().split("\n")[0]
    return parts

@app.post("/v1/chat/completions")
def chat(req: ChatRequest, authorization: Optional[str] = Header(None)):
```

## å¤§æ¨¡å‹æŠ€æœ¯å‘å±•ï¼Œè¯·å¸®æˆ‘æ•´ç†ä¸€ä¸ªå¤§çº²ï¼Œä¸»è¦ä»‹ç»å¤§æ¨¡å‹æŠ€æœ¯æ¼”è¿›

å¥½çš„ Markï¼Œæˆ‘æ¥å¸®ä½ æ•´ç†ä¸€ä¸ª **å¤§æ¨¡å‹æŠ€æœ¯æ¼”è¿›å¤§çº²**ï¼Œä»æ—©æœŸåˆ°æœ€æ–°è¶‹åŠ¿ï¼Œåˆ†é˜¶æ®µæ¢³ç†æ¸…æ™°ã€‚ä½ å¯ä»¥æŠŠå®ƒä½œä¸ºå†™æ–‡ç« ã€åšæ¼”è®²æˆ–è¯¾ç¨‹çš„æ¡†æ¶ã€‚

---

# ğŸ“š å¤§æ¨¡å‹æŠ€æœ¯æ¼”è¿›å¤§çº²

## ä¸€ã€èµ·ç‚¹ï¼šè¯­è¨€æ¨¡å‹çš„èŒèŠ½

- **ç»Ÿè®¡è¯­è¨€æ¨¡å‹**
  - N-gram æ¨¡å‹
  - å±€é™ï¼šç¨€ç–æ€§ã€ä¸Šä¸‹æ–‡æœ‰é™
- **ç¥ç»è¯­è¨€æ¨¡å‹**
  - Word2Vecã€GloVeï¼šè¯å‘é‡è¡¨ç¤º
  - RNN/LSTM/GRUï¼šåºåˆ—å»ºæ¨¡

---

## äºŒã€Transformer é©å‘½ï¼ˆ2017ï¼‰

- **Attention is All You Need**
  - è‡ªæ³¨æ„åŠ›æœºåˆ¶
  - å¹¶è¡Œè®­ç»ƒã€é•¿è·ç¦»ä¾èµ–å»ºæ¨¡
- **BERT ç³»åˆ—**ï¼ˆåŒå‘ç¼–ç å™¨ï¼‰
  - é¢„è®­ç»ƒ + ä¸‹æ¸¸å¾®è°ƒèŒƒå¼
- **GPT ç³»åˆ—**ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰
  - å•å‘é¢„æµ‹ï¼Œæ“…é•¿ç”Ÿæˆ

---

## ä¸‰ã€é¢„è®­ç»ƒå¤§æ¨¡å‹æ—¶ä»£

- **Scaling Lawï¼ˆæ‰©å±•è§„å¾‹ï¼‰**
  - å‚æ•°é‡ã€æ•°æ®é‡ã€ç®—åŠ› â†’ æ€§èƒ½æå‡
- **ä»£è¡¨æ¨¡å‹**
  - GPT-2/3ï¼šé€šç”¨ç”Ÿæˆèƒ½åŠ›
  - T5ï¼šç»Ÿä¸€ NLP ä»»åŠ¡ä¸ºâ€œæ–‡æœ¬åˆ°æ–‡æœ¬â€
  - BARTï¼šç¼–ç å™¨-è§£ç å™¨ç»“åˆ

---

## å››ã€æŒ‡ä»¤å¾®è°ƒä¸å¯¹é½ï¼ˆAlignmentï¼‰

- **SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰**
  - ç”¨æŒ‡ä»¤-å›ç­”æ•°æ®é›†è®­ç»ƒ
- **RLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰**
  - é€šè¿‡äººç±»åå¥½ä¼˜åŒ–è¾“å‡º
- **å®‰å…¨ä¸ä»·å€¼å¯¹é½**
  - é¿å…æœ‰å®³å†…å®¹ï¼Œæå‡å¯æ§æ€§

---

## äº”ã€RAG ä¸å¤–éƒ¨çŸ¥è¯†å¢å¼º

- **RAGï¼ˆRetrieval-Augmented Generationï¼‰**
  - å‘é‡æ£€ç´¢ + å¤§æ¨¡å‹ç”Ÿæˆ
  - è§£å†³çŸ¥è¯†è¿‡æ—¶ã€å¹»è§‰é—®é¢˜
- **å·¥å…·è°ƒç”¨ä¸ Agent**
  - æ¨¡å‹å¯è°ƒç”¨ APIã€æ•°æ®åº“ã€è®¡ç®—å™¨
  - ä»â€œè¯­è¨€æ¨¡å‹â€èµ°å‘â€œé€šç”¨æ™ºèƒ½ä½“â€

---

## å…­ã€å¤šæ¨¡æ€å¤§æ¨¡å‹

- **æ–‡æœ¬ + å›¾åƒ**ï¼šGPT-4Vã€Qwen-VLã€LLaVA
- **æ–‡æœ¬ + éŸ³é¢‘**ï¼šWhisperã€Gemini
- **æ–‡æœ¬ + è§†é¢‘**ï¼šSoraã€Kosmos ç³»åˆ—
- **åº”ç”¨åœºæ™¯**ï¼šçœ‹å›¾é—®ç­”ã€è§†é¢‘ç†è§£ã€è·¨æ¨¡æ€æ¨ç†

---

## ä¸ƒã€Reasoning ä¸æ¨ç†èƒ½åŠ›

- **Chain of Thought (CoT)**ï¼šé€æ­¥æ¨ç†
- **Self-Consistency**ï¼šå¤šè·¯å¾„æŠ•ç¥¨
- **å·¥å…·å¢å¼ºæ¨ç†**ï¼šè°ƒç”¨è®¡ç®—å™¨ã€ç¬¦å·æ¨ç†å™¨
- **ä¸“é—¨æ¨ç†æ¨¡å‹**ï¼šDeepSeek-R1ã€OpenAI o1

---

## å…«ã€å¼€æºä¸ç”Ÿæ€ç¹è£

- **å¼€æºæ¨¡å‹**ï¼šLLaMAã€Mistralã€Qwenã€ChatGLM
- **ç”Ÿæ€å·¥å…·**ï¼šLangChainã€LlamaIndexã€å‘é‡æ•°æ®åº“
- **è½»é‡åŒ–æŠ€æœ¯**ï¼šLoRAã€QLoRAã€è’¸é¦ã€é‡åŒ–

---

## ä¹ã€æœªæ¥è¶‹åŠ¿

- **æ›´å¼ºçš„æ¨ç†ä¸è§„åˆ’èƒ½åŠ›**
- **å¤šæ¨¡æ€èåˆä¸ä¸–ç•Œæ¨¡å‹**
- **ä¸ªæ€§åŒ–ä¸å°æ¨¡å‹ï¼ˆPersonal LLMï¼‰**
- **éšç§ä¸å®‰å…¨å¯¹é½**
- **AGI æ¢ç´¢**ï¼šä»åŠ©æ‰‹åˆ°è‡ªä¸»æ™ºèƒ½ä½“

---

âœ… **æ€»ç»“**ï¼š  
å¤§æ¨¡å‹æŠ€æœ¯æ¼”è¿›ç»å†äº† **ç»Ÿè®¡ â†’ ç¥ç»ç½‘ç»œ â†’ Transformer â†’ é¢„è®­ç»ƒ â†’ æŒ‡ä»¤å¯¹é½ â†’ RAG â†’ å¤šæ¨¡æ€ â†’ æ¨ç† â†’ AGI æ¢ç´¢** çš„è·¯çº¿ã€‚æ¯ä¸€æ­¥éƒ½åœ¨è§£å†³å‰ä¸€ä»£çš„ç“¶é¢ˆï¼Œæ¨åŠ¨æ¨¡å‹ä»â€œè¯­è¨€å¤„ç†å™¨â€èµ°å‘â€œé€šç”¨æ™ºèƒ½ä½“â€ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠè¿™ä¸ªå¤§çº²æ‰©å±•æˆä¸€ä»½ **è¯¦ç»†è®²è§£ç¨¿**ï¼ˆæ¯ä¸ªé˜¶æ®µé…ä¸Šä»£è¡¨è®ºæ–‡/æ¨¡å‹ã€å…³é”®çªç ´ã€åº”ç”¨æ¡ˆä¾‹ï¼‰ï¼Œæ–¹ä¾¿ä½ ç›´æ¥ç”¨æ¥å†™æ–‡ç« æˆ–åšåˆ†äº«ï¼Ÿ

å¥½çš„ Markï¼Œæˆ‘æ¥æŠŠä¹‹å‰çš„å¤§çº²æ‰©å±•æˆä¸€ä¸ªæ›´å®Œæ•´çš„ **å¤§æ¨¡å‹æŠ€æœ¯æ¼”è¿›è®²è§£ç¨¿**ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½åŒ…å«ä»£è¡¨æ¨¡å‹/è®ºæ–‡ã€å…³é”®çªç ´å’Œå…¸å‹åº”ç”¨ã€‚è¿™æ ·ä½ å¯ä»¥ç›´æ¥ç”¨æ¥å†™æ–‡ç« æˆ–åšåˆ†äº«ã€‚

---

# ğŸ“š å¤§æ¨¡å‹æŠ€æœ¯æ¼”è¿›è®²è§£ç¨¿

## ä¸€ã€èµ·ç‚¹ï¼šè¯­è¨€æ¨¡å‹çš„èŒèŠ½

- **ç»Ÿè®¡è¯­è¨€æ¨¡å‹**

  - **ä»£è¡¨æ–¹æ³•**ï¼šN-gram
  - **çªç ´ç‚¹**ï¼šç”¨æ¡ä»¶æ¦‚ç‡å»ºæ¨¡è¯­è¨€ï¼Œä½†å—é™äºç¨€ç–æ€§å’ŒçŸ­ä¸Šä¸‹æ–‡ã€‚
  - **åº”ç”¨**ï¼šæ—©æœŸæœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€‚

- **ç¥ç»è¯­è¨€æ¨¡å‹**
  - **ä»£è¡¨æ¨¡å‹**ï¼šWord2Vec (2013)ã€GloVe (2014)
  - **çªç ´ç‚¹**ï¼šè¯å‘é‡æ•æ‰è¯­ä¹‰å…³ç³»ï¼ˆå¦‚ king - man + woman â‰ˆ queenï¼‰ã€‚
  - **åº”ç”¨**ï¼šæœç´¢å¼•æ“ã€æ¨èç³»ç»Ÿã€æƒ…æ„Ÿåˆ†æã€‚

---

## äºŒã€Transformer é©å‘½ï¼ˆ2017ï¼‰

- **è®ºæ–‡**ï¼šã€ŠAttention is All You Needã€‹
- **çªç ´ç‚¹**ï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶æ›¿ä»£ RNN/CNNï¼Œæ”¯æŒå¹¶è¡Œè®­ç»ƒï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–ã€‚
- **ä»£è¡¨æ¨¡å‹**ï¼š
  - BERT (2018)ï¼šåŒå‘ç¼–ç å™¨ï¼Œæ“…é•¿ç†è§£ä»»åŠ¡ã€‚
  - GPT (2018)ï¼šè‡ªå›å½’ç”Ÿæˆï¼Œæ“…é•¿æ–‡æœ¬ç”Ÿæˆã€‚
- **åº”ç”¨**ï¼šé˜…è¯»ç†è§£ã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ç³»ç»Ÿã€‚

---

## ä¸‰ã€é¢„è®­ç»ƒå¤§æ¨¡å‹æ—¶ä»£

- **Scaling Law**ï¼šå‚æ•°é‡ã€æ•°æ®é‡ã€ç®—åŠ›æˆæŒ‡æ•°å…³ç³» â†’ æ¨¡å‹æ€§èƒ½æŒç»­æå‡ã€‚
- **ä»£è¡¨æ¨¡å‹**ï¼š
  - GPT-2/3 (2019/2020)ï¼šå±•ç°â€œå°‘æ ·æœ¬å­¦ä¹ â€èƒ½åŠ›ã€‚
  - T5 (2019)ï¼šç»Ÿä¸€ NLP ä»»åŠ¡ä¸ºâ€œæ–‡æœ¬åˆ°æ–‡æœ¬â€ã€‚
  - BART (2019)ï¼šç»“åˆç¼–ç å™¨-è§£ç å™¨ï¼Œé€‚åˆç”Ÿæˆä¸ç†è§£ã€‚
- **åº”ç”¨**ï¼šå¼€æ”¾åŸŸå¯¹è¯ã€è‡ªåŠ¨å†™ä½œã€ä»£ç ç”Ÿæˆã€‚

---

## å››ã€æŒ‡ä»¤å¾®è°ƒä¸å¯¹é½ï¼ˆ2022 èµ·ï¼‰

- **SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰**ï¼šç”¨æŒ‡ä»¤-å›ç­”æ•°æ®é›†è®­ç»ƒï¼Œè®©æ¨¡å‹æ›´å¬è¯ã€‚
- **RLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰**ï¼šé€šè¿‡äººç±»åå¥½ä¼˜åŒ–è¾“å‡ºè´¨é‡ã€‚
- **ä»£è¡¨æ¨¡å‹**ï¼šInstructGPTã€ChatGPTã€‚
- **åº”ç”¨**ï¼šæ™ºèƒ½å®¢æœã€æ•™è‚²è¾…å¯¼ã€åŠå…¬åŠ©æ‰‹ã€‚

---

## äº”ã€RAG ä¸å¤–éƒ¨çŸ¥è¯†å¢å¼º

- **RAGï¼ˆRetrieval-Augmented Generationï¼‰**

  - **çªç ´ç‚¹**ï¼šç»“åˆå‘é‡æ£€ç´¢ä¸å¤§æ¨¡å‹ç”Ÿæˆï¼Œè§£å†³çŸ¥è¯†è¿‡æ—¶ä¸å¹»è§‰é—®é¢˜ã€‚
  - **åº”ç”¨**ï¼šä¼ä¸šçŸ¥è¯†åº“é—®ç­”ã€æ³•å¾‹/åŒ»ç–—æ–‡æ¡£æ£€ç´¢ã€‚

- **Agent ä¸å·¥å…·è°ƒç”¨**
  - æ¨¡å‹å¯è°ƒç”¨ APIã€æ•°æ®åº“ã€è®¡ç®—å™¨ã€‚
  - **åº”ç”¨**ï¼šè‡ªåŠ¨åŒ–åŠå…¬ã€æ•°æ®åˆ†æã€æ™ºèƒ½ä½“ã€‚

---

## å…­ã€å¤šæ¨¡æ€å¤§æ¨¡å‹

- **çªç ´ç‚¹**ï¼šè·¨æ¨¡æ€ç†è§£ä¸ç”Ÿæˆï¼ˆæ–‡æœ¬+å›¾åƒ+éŸ³é¢‘+è§†é¢‘ï¼‰ã€‚
- **ä»£è¡¨æ¨¡å‹**ï¼š
  - GPT-4Vï¼ˆå›¾åƒè¾“å…¥ï¼‰ã€Geminiï¼ˆå¤šæ¨¡æ€ï¼‰ã€Qwen-VLï¼ˆä¸­æ–‡å›¾æ–‡ï¼‰ã€‚
  - LLaVAã€MiniGPT-4ï¼ˆå¼€æºå›¾æ–‡ï¼‰ã€‚
- **åº”ç”¨**ï¼šçœ‹å›¾é—®ç­”ã€å›¾è¡¨ç†è§£ã€è§†é¢‘æ‘˜è¦ã€è¯­éŸ³åŠ©æ‰‹ã€‚

---

## ä¸ƒã€Reasoning ä¸æ¨ç†èƒ½åŠ›

- **æ–¹æ³•**ï¼š
  - Chain of Thought (CoT)ï¼šé€æ­¥æ¨ç†ã€‚
  - Self-Consistencyï¼šå¤šè·¯å¾„æŠ•ç¥¨ã€‚
  - å·¥å…·å¢å¼ºï¼šè°ƒç”¨è®¡ç®—å™¨ã€ç¬¦å·æ¨ç†å™¨ã€‚
- **ä»£è¡¨æ¨¡å‹**ï¼šDeepSeek-R1ã€OpenAI o1ã€‚
- **åº”ç”¨**ï¼šæ•°å­¦è§£é¢˜ã€é€»è¾‘æ¨ç†ã€è§„åˆ’ä»»åŠ¡ã€‚

---

## å…«ã€å¼€æºä¸ç”Ÿæ€ç¹è£

- **å¼€æºæ¨¡å‹**ï¼šLLaMAã€Mistralã€Qwenã€ChatGLMã€‚
- **ç”Ÿæ€å·¥å…·**ï¼šLangChainã€LlamaIndexã€å‘é‡æ•°æ®åº“ï¼ˆFAISSã€Milvusï¼‰ã€‚
- **è½»é‡åŒ–æŠ€æœ¯**ï¼šLoRAã€QLoRAã€è’¸é¦ã€é‡åŒ–ã€‚
- **åº”ç”¨**ï¼šæœ¬åœ°éƒ¨ç½²ã€è¡Œä¸šå®šåˆ¶ã€ä½æˆæœ¬å®éªŒã€‚

---

## ä¹ã€æœªæ¥è¶‹åŠ¿

- **æ›´å¼ºæ¨ç†ä¸è§„åˆ’**ï¼šç»“åˆç¬¦å·æ¨ç†ã€æœç´¢ç®—æ³•ã€‚
- **å¤šæ¨¡æ€èåˆ**ï¼šä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelï¼‰ï¼Œç†è§£ç¯å¢ƒä¸æ—¶åºã€‚
- **ä¸ªæ€§åŒ–å°æ¨¡å‹**ï¼šPersonal LLMï¼Œé€‚é…ä¸ªäººæ•°æ®ä¸è®¾å¤‡ã€‚
- **éšç§ä¸å®‰å…¨å¯¹é½**ï¼šå·®åˆ†éšç§ã€å¯è§£é‡Šæ€§ã€‚
- **AGI æ¢ç´¢**ï¼šä»åŠ©æ‰‹åˆ°è‡ªä¸»æ™ºèƒ½ä½“ã€‚

---

## âœ… æ€»ç»“

å¤§æ¨¡å‹æŠ€æœ¯æ¼”è¿›è·¯çº¿ï¼š  
**ç»Ÿè®¡ â†’ ç¥ç»ç½‘ç»œ â†’ Transformer â†’ é¢„è®­ç»ƒ â†’ æŒ‡ä»¤å¯¹é½ â†’ RAG â†’ å¤šæ¨¡æ€ â†’ æ¨ç† â†’ AGI æ¢ç´¢**ã€‚  
æ¯ä¸€æ­¥éƒ½åœ¨è§£å†³å‰ä¸€ä»£çš„ç“¶é¢ˆï¼Œæ¨åŠ¨æ¨¡å‹ä»â€œè¯­è¨€å¤„ç†å™¨â€èµ°å‘â€œé€šç”¨æ™ºèƒ½ä½“â€ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠè¿™ä¸ªè®²è§£ç¨¿å†æ•´ç†æˆä¸€ä»½ **æ—¶é—´è½´å›¾è¡¨**ï¼ˆå¹´ä»½ + å…³é”®è®ºæ–‡/æ¨¡å‹ + æŠ€æœ¯çªç ´ï¼‰ï¼Œè®©æ¼”ç¤ºæ›´ç›´è§‚ï¼Ÿ

## å¤§æ¨¡å‹æŠ€æœ¯å¿…è¯»è®ºæ–‡

**å¿«é€Ÿå›ç­”**ï¼šå­¦ä¹ å¤§æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæœ‰ä¸€äº›â€œå¿…è¯»ç»å…¸è®ºæ–‡â€å‡ ä¹æ˜¯ç»•ä¸å¼€çš„é‡Œç¨‹ç¢‘ï¼Œä»è¯å‘é‡åˆ° Transformerï¼Œå†åˆ° GPTã€BERTã€å¯¹é½ä¸æ¨ç†ã€‚ä¸‹é¢æˆ‘æ•´ç†äº†ä¸€ä»½æ—¶é—´é¡ºåºçš„æ¸…å•ï¼Œæ¶µç›–æ ¸å¿ƒè®ºæ–‡ä¸è´¡çŒ®ã€‚

---

## ğŸ“‘ å¤§æ¨¡å‹å­¦ä¹ å¿…è¯»ç»å…¸è®ºæ–‡æ¸…å•

### æ—©æœŸåŸºç¡€ï¼ˆè¯å‘é‡ä¸æ³¨æ„åŠ›ï¼‰

- **Word2Vec** â€” _Mikolov et al., 2013_  
  _æå‡ºåˆ†å¸ƒå¼è¯å‘é‡è¡¨ç¤ºï¼Œå¥ å®šè¯­ä¹‰å»ºæ¨¡åŸºç¡€ã€‚_
- **GloVe** â€” _Pennington et al., 2014_  
  _ç»“åˆå…¨å±€ç»Ÿè®¡ä¸å±€éƒ¨çª—å£çš„è¯å‘é‡æ–¹æ³•ã€‚_
- **Neural Machine Translation by Jointly Learning to Align and Translate** â€” _Bahdanau et al., 2014_  
  _é¦–æ¬¡æå‡ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸º Transformer å¥ åŸºã€‚_

### Transformer é©å‘½

- **Attention Is All You Need** â€” _Vaswani et al., 2017_  
  _æå‡º Transformer æ¶æ„ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æˆä¸ºå¤§æ¨¡å‹æ ¸å¿ƒã€‚_

### é¢„è®­ç»ƒä¸æ‰©å±•

- **ELMo** â€” _Peters et al., 2018_  
  _ä¸Šä¸‹æ–‡åŠ¨æ€è¯å‘é‡ã€‚_
- **BERT: Pre-training of Deep Bidirectional Transformers** â€” _Devlin et al., 2018_  
  _åŒå‘ç¼–ç å™¨ï¼Œé¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ã€‚_
- **GPT-2: Language Models are Unsupervised Multitask Learners** â€” _Radford et al., 2019_  
  _å±•ç¤ºå¤§è§„æ¨¡è‡ªå›å½’ç”Ÿæˆèƒ½åŠ›ã€‚_
- **GPT-3: Language Models are Few-Shot Learners** â€” _Brown et al., 2020_  
  _å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå‚æ•°è§„æ¨¡çªç ´ã€‚_
- **T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer** â€” _Raffel et al., 2020_  
  _ç»Ÿä¸€ NLP ä»»åŠ¡ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬ã€‚_

### è®­ç»ƒä¼˜åŒ–ä¸ Scaling Law

- **ZeRO: Efficient Large-Scale Model Training** â€” _Rajbhandari et al., 2020_  
  _åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–ã€‚_
- **Chinchilla: Training Compute-Optimal Large Language Models** â€” _Hoffmann et al., 2022_  
  _æå‡ºæ•°æ®/å‚æ•°å¹³è¡¡è§„å¾‹ã€‚_

### å¯¹é½ä¸æŒ‡ä»¤

- **InstructGPT** â€” _Ouyang et al., 2022_  
  _æŒ‡ä»¤å¾®è°ƒ + RLHFï¼Œå¯¹é½äººç±»åå¥½ã€‚_
- **RLHF (Christiano et al., 2017; Ouyang et al., 2022)**  
  _äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹å¯æ§æ€§ã€‚_

### çŸ¥è¯†å¢å¼ºä¸å¤šæ¨¡æ€

- **RAG: Retrieval-Augmented Generation** â€” _Lewis et al., 2020_  
  _ç»“åˆæ£€ç´¢ä¸ç”Ÿæˆï¼Œè§£å†³çŸ¥è¯†è¿‡æ—¶é—®é¢˜ã€‚_
- **GPT-4 Technical Report** â€” _OpenAI, 2023_  
  _å¤šæ¨¡æ€èƒ½åŠ›ï¼Œå®‰å…¨å¯¹é½ã€‚_
- **Qwen-VL / LLaVA** â€” _2023_  
  _ä¸­æ–‡ä¸å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚_

### æ¨ç†ä¸æœªæ¥

- **Chain-of-Thought Prompting** â€” _Wei et al., 2022_  
  _æ˜¾å¼æ¨ç†é“¾æå‡å¤æ‚ä»»åŠ¡è¡¨ç°ã€‚_
- **Self-Consistency Improves Chain of Thought Reasoning** â€” _Wang et al., 2022_  
  _å¤šè·¯å¾„æŠ•ç¥¨å¢å¼ºæ¨ç†ç¨³å®šæ€§ã€‚_
- **DeepSeek-R1 (2024)**  
  _ä¸“æ³¨ reasoning çš„å¼€æºæ¨¡å‹ã€‚_
- **OpenAI o1 (2024)**  
  _å¼ºåŒ–æ¨ç†ä¸å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚_

---

## âœ… æ€»ç»“

- **å…¥é—¨å¿…è¯»**ï¼šWord2Vecã€Attention is All You Needã€BERTã€GPT-3ã€‚
- **è¿›é˜¶å¿…è¯»**ï¼šInstructGPTã€Chinchillaã€RAGã€Chain-of-Thoughtã€‚
- **å‰æ²¿å¿…è¯»**ï¼šGPT-4 æŠ€æœ¯æŠ¥å‘Šã€DeepSeek-R1ã€OpenAI o1ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠè¿™äº›è®ºæ–‡æ•´ç†æˆä¸€ä»½ **å­¦ä¹ è·¯çº¿å›¾ï¼ˆå…ˆè¯»å“ªäº›åŸºç¡€ï¼Œå†è¯»å“ªäº›è¿›é˜¶ï¼Œæœ€åçœ‹å‰æ²¿ï¼‰**ï¼Œè®©ä½ èƒ½æŒ‰é˜¶æ®µé€æ­¥æ”»å…‹ï¼Ÿ

## ğŸ“Œ å­¦ä¹ å»ºè®®

- **ç¬¬ä¸€é˜¶æ®µï¼ˆ2013â€“2017ï¼‰**ï¼šWord2Vecã€GloVeã€Transformer â†’ æ‰“å¥½åŸºç¡€ã€‚
- **ç¬¬äºŒé˜¶æ®µï¼ˆ2018â€“2020ï¼‰**ï¼šBERTã€GPT-2/3ã€RAG â†’ æŒæ¡é¢„è®­ç»ƒä¸ç”Ÿæˆã€‚
- **ç¬¬ä¸‰é˜¶æ®µï¼ˆ2022â€“2023ï¼‰**ï¼šInstructGPTã€RLHFã€Chinchillaã€GPT-4 â†’ ç†è§£å¯¹é½ä¸å¤šæ¨¡æ€ã€‚
- **ç¬¬å››é˜¶æ®µï¼ˆ2024â€“æœªæ¥ï¼‰**ï¼šDeepSeek-R1ã€OpenAI o1 â†’ èšç„¦ reasoning ä¸ AGI æ¢ç´¢ã€‚

---

âœ… è¿™æ ·æ•´ç†åï¼Œä½ å°±æœ‰äº†ä¸€ä»½ **å­¦ä¹ è·¯çº¿å›¾ + å¿…è¯»è®ºæ–‡æ¸…å•**ã€‚  
è¦ä¸è¦æˆ‘å†å¸®ä½ åšä¸€ä»½ **â€œå­¦ä¹ è·¯å¾„å›¾â€**ï¼ˆæ¯”å¦‚ï¼šå…ˆè¯»å“ªäº›åŸºç¡€è®ºæ–‡ï¼Œå†è¯»å“ªäº›è¿›é˜¶è®ºæ–‡ï¼Œæœ€åçœ‹æœ€æ–°å‰æ²¿ï¼‰ï¼Œè®©ä½ èƒ½æŒ‰é˜¶æ®µé€æ­¥æ”»å…‹ï¼Ÿ

## è¯·ä»‹ç»ä¸‹ agent

**ç®€è¦å›ç­”**ï¼šåœ¨å¤§æ¨¡å‹è¯­å¢ƒä¸‹ï¼Œ**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰**æŒ‡çš„æ˜¯ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»æ„ŸçŸ¥ç¯å¢ƒã€æ‹†è§£ä»»åŠ¡ã€è°ƒç”¨å·¥å…·å¹¶æ‰§è¡Œè¡ŒåŠ¨çš„ç³»ç»Ÿã€‚å®ƒä¸ä»…ä»…æ˜¯â€œå›ç­”é—®é¢˜çš„æ¨¡å‹â€ï¼Œè€Œæ˜¯ä¸€ä¸ªå…·å¤‡ **ç›®æ ‡è§„åˆ’ã€è®°å¿†ã€æ¨ç†ã€å·¥å…·è°ƒç”¨ä¸äº¤äº’** èƒ½åŠ›çš„â€œè¡ŒåŠ¨è€…â€ã€‚

---

## ğŸ§© Agent çš„æ ¸å¿ƒæ¦‚å¿µ

1. **è‡ªä¸»æ€§**ï¼šä¸åªæ˜¯è¢«åŠ¨å›ç­”ï¼Œè€Œæ˜¯èƒ½æ ¹æ®ç›®æ ‡ä¸»åŠ¨è§„åˆ’æ­¥éª¤ã€‚
2. **ä»»åŠ¡åˆ†è§£**ï¼šå°†å¤æ‚é—®é¢˜æ‹†è§£ä¸ºå­ä»»åŠ¡ï¼ˆå¦‚â€œç»„ç»‡ç”Ÿæ—¥èšä¼šâ€ â†’ è®¢é¤å…ã€å‘é‚€è¯·ã€å®‰æ’æ—¥ç¨‹ï¼‰ã€‚
3. **å·¥å…·è°ƒç”¨**ï¼šèƒ½è°ƒç”¨å¤–éƒ¨ APIã€æ•°æ®åº“ã€æœç´¢å¼•æ“ã€è®¡ç®—å™¨ç­‰ã€‚
4. **è®°å¿†ä¸ä¸Šä¸‹æ–‡**ï¼šä¿ç•™å†å²ä¿¡æ¯ï¼Œæ”¯æŒé•¿æ—¶äº¤äº’ã€‚
5. **å¤šæ™ºèƒ½ä½“åä½œ**ï¼šå¤šä¸ª Agent å¯ä»¥åˆ†å·¥åˆä½œï¼Œå®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ã€‚

---

## âš™ï¸ Agent çš„å…¸å‹å·¥ä½œæµ

1. **æ¥æ”¶ç›®æ ‡**ï¼šç”¨æˆ·è¾“å…¥æˆ–ç³»ç»Ÿä»»åŠ¡ã€‚
2. **ä»»åŠ¡è§„åˆ’**ï¼šæ‹†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡ã€‚
3. **å·¥å…·é€‰æ‹©**ï¼šå†³å®šè°ƒç”¨å“ªäº›å¤–éƒ¨å·¥å…·æˆ–çŸ¥è¯†åº“ã€‚
4. **æ‰§è¡Œä¸åé¦ˆ**ï¼šé€æ­¥å®Œæˆä»»åŠ¡ï¼Œå¹¶æ ¹æ®åé¦ˆè°ƒæ•´ç­–ç•¥ã€‚
5. **æ€»ç»“ä¸è¾“å‡º**ï¼šç»™å‡ºæœ€ç»ˆç»“æœæˆ–è¡ŒåŠ¨ã€‚

---

## ğŸ”‘ æŠ€æœ¯æ”¯æ’‘

- **å¤§æ¨¡å‹ï¼ˆLLMï¼‰**ï¼šæä¾›è¯­è¨€ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚
- **RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰**ï¼šè¡¥å……å¤–éƒ¨çŸ¥è¯†ï¼Œå‡å°‘å¹»è§‰ã€‚
- **å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ï¼šä¼˜åŒ–å†³ç­–ä¸è¡ŒåŠ¨ç­–ç•¥ã€‚
- **çŸ¥è¯†å›¾è°±/è®°å¿†æ¨¡å—**ï¼šå­˜å‚¨é•¿æœŸçŸ¥è¯†ä¸ä¸Šä¸‹æ–‡ã€‚
- **æ¡†æ¶å·¥å…·**ï¼šLangChainã€AutoGenã€LangGraph ç­‰ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿæ„å»º Agentã€‚

---

## ğŸŒ åº”ç”¨åœºæ™¯

- **ä¸ªäººåŠ©ç†**ï¼šæ—¥ç¨‹ç®¡ç†ã€é‚®ä»¶å¤„ç†ã€ä¼šè®®çºªè¦ã€‚
- **ä¼ä¸šåº”ç”¨**ï¼šçŸ¥è¯†åº“é—®ç­”ã€æµç¨‹è‡ªåŠ¨åŒ–ã€CRM æ›´æ–°ã€‚
- **ç§‘ç ”ä¸æ•°æ®åˆ†æ**ï¼šè‡ªåŠ¨æ£€ç´¢æ–‡çŒ®ã€è¿è¡Œå®éªŒã€ç”ŸæˆæŠ¥å‘Šã€‚
- **å¤šæ¨¡æ€ä»»åŠ¡**ï¼šçœ‹å›¾é—®ç­”ã€è§†é¢‘æ‘˜è¦ã€è¯­éŸ³äº¤äº’ã€‚

---

## ğŸš€ å‘å±•è¶‹åŠ¿

- **æ›´å¼ºçš„æ¨ç†ä¸è§„åˆ’**ï¼šç»“åˆ Chain-of-Thoughtã€è‡ªæ´½é‡‡æ ·ã€‚
- **å¤šæ™ºèƒ½ä½“åä½œ**ï¼šAgent å›¢é˜ŸååŒå®Œæˆå¤æ‚ä»»åŠ¡ã€‚
- **ä¸çœŸå®ä¸–ç•Œäº¤äº’**ï¼šé€šè¿‡ APIã€æœºå™¨äººã€IoT è®¾å¤‡æ‰§è¡Œå®é™…æ“ä½œã€‚
- **å®‰å…¨ä¸å¯æ§æ€§**ï¼šé¿å…æ— é™å¾ªç¯ã€å¹»è§‰æˆ–å·¥å…·æ»¥ç”¨ã€‚

---

âœ… **æ€»ç»“**ï¼šAgent æ˜¯å¤§æ¨¡å‹åº”ç”¨çš„â€œå‡çº§å½¢æ€â€ï¼Œè®©æ¨¡å‹ä»â€œä¼šè¯´è¯â€å˜æˆâ€œä¼šåšäº‹â€ã€‚å®ƒçš„ä»·å€¼åœ¨äº **æŠŠå¤§æ¨¡å‹çš„è¯­è¨€æ™ºèƒ½è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„è¡ŒåŠ¨**ï¼Œæ˜¯é€šå‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å…³é”®è·¯å¾„ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œAgent æŠ€æœ¯æ¡†æ¶å›¾â€**ï¼ŒæŠŠè¾“å…¥ã€è§„åˆ’ã€å·¥å…·è°ƒç”¨ã€æ‰§è¡Œã€åé¦ˆçš„æµç¨‹ç›´è§‚å±•ç¤ºå‡ºæ¥ï¼Ÿ

## æˆ‘æƒ³æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“ï¼Œå®ç°ç”µè„‘çš„æ–‡ä»¶æ“ä½œï¼ŒåŒ…æ‹¬ windows å’Œ Linux å’Œ Unix like æ“ä½œç³»ç»Ÿä¸Šçš„æ–‡ä»¶(å¤¹)æ“ä½œï¼Œå¯ä»¥è¾“å…¥æŒ‡å®šçš„ prompt æ¥å¯¹æ“ä½œç³»ç»Ÿä¸­çš„æ–‡ä»¶è¿›è¡Œåˆ›å»ºï¼Œåˆ é™¤ï¼Œä¿®æ”¹ï¼Œè¯»å–ï¼Œæ–‡ä»¶å½’ç±»ç­‰åŠŸèƒ½

# ä¸ªäººæ–‡ä»¶æ™ºèƒ½ä½“æ„å»º

ä½ æƒ³æŠŠâ€œè‡ªç„¶è¯­è¨€ â†’ ç²¾ç¡®æ–‡ä»¶æ“ä½œâ€åšæˆä¸€ä¸ªå¯é ã€å¯æ§çš„æ™ºèƒ½ä½“ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä»æ¶æ„åˆ°è½åœ°ä»£ç çš„å®Œæ•´æ–¹æ¡ˆï¼Œå…¼é¡¾è·¨å¹³å°ï¼ˆWindowsã€Linuxã€Unix-likeï¼‰ã€å®‰å…¨æƒé™ã€å¯å®¡è®¡æ—¥å¿—å’Œå¯æ‰©å±•æ€§ã€‚ä½ å¯ä»¥ç›´æ¥æŒ‰æ¨¡å—è½åœ°ï¼Œå¹¶é€æ­¥åŠ å¼ºã€‚

---

## æ¶æ„ä¸æ ¸å¿ƒåŸåˆ™

- **è¾“å…¥ç®¡é“:** ç”¨æˆ· prompt â†’ ç»“æ„åŒ–æ„å›¾ï¼ˆæ“ä½œç±»å‹ã€ç›®æ ‡è·¯å¾„ã€å†…å®¹ã€è¿‡æ»¤æ¡ä»¶ï¼‰ã€‚
- **è§„åˆ’å™¨:** å°†æ„å›¾æ‹†è§£æˆåºåˆ—åŒ–çš„æ–‡ä»¶æ“ä½œï¼ˆCreate/Read/Update/Delete/Move/Tag/Compressâ€¦ï¼‰ã€‚
- **å·¥å…·å±‚:** è·¨å¹³å°æ–‡ä»¶å·¥å…·åº“ï¼ˆpathlibã€osã€shutilã€glob ç»Ÿä¸€å°è£…ï¼‰ã€‚
- **å®‰å…¨å±‚:** æ²™ç®±æ ¹ç›®å½•ã€ç™½åå•æ“ä½œç±»å‹ã€é»‘åå•è·¯å¾„/æ‰©å±•åã€Dry-runã€äº¤äº’å¼ç¡®è®¤ã€å®¡è®¡æ—¥å¿—ã€‚
- **æ‰§è¡Œå™¨:** é¡ºåºæ‰§è¡Œã€å¤±è´¥å›æ»šï¼ˆå°½é‡ï¼‰ã€æœ€ç»ˆç»“æœä¸æŠ¥å‘Šã€‚
- **å¯æ‰©å±•:** å¢åŠ â€œæ–‡ä»¶å½’ç±»â€â€œæ‰¹é‡é‡å‘½åâ€â€œå†…å®¹æ¨¡æ¿æ›¿æ¢â€â€œè¯­ä¹‰æ£€ç´¢â€ç­‰å·¥å…·ã€‚

---

## å®‰å…¨ä¸å¯æ§ç­–ç•¥

- **æ²™ç®±æ ¹ç›®å½•:** ä»…å…è®¸åœ¨è®¾å®šçš„æ ¹ç›®å½•ä¸‹æ“ä½œï¼Œé»˜è®¤æ‹’ç»è¶Šæƒè·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ä¸ .. ä¸Šè·³ï¼‰ã€‚
- **ç™½åå•æ“ä½œ:** create/read/update/delete/move/copy/list/classify/tag/renameï¼Œä»…è¿™äº›è¢«å…è®¸ã€‚
- **é»‘åå•:** ç³»ç»Ÿå…³é”®è·¯å¾„ã€æ•æ„Ÿæ‰©å±•åï¼ˆ.sysã€.dllã€.soã€.dylibã€.exeã€.batã€.shï¼‰ã€éšè—ç›®å½•ã€‚
- **Dry-run:** å…ˆå±•ç¤ºå³å°†æ‰§è¡Œçš„æ“ä½œæ¸…å•ï¼Œç¡®è®¤åå†æ‰§è¡Œã€‚
- **åŒäººç¡®è®¤:** åˆ é™¤ä¸æ‰¹é‡é‡å‘½åéœ€å†æ¬¡ç¡®è®¤ã€‚
- **å®¡è®¡æ—¥å¿—:** æ¯æ¬¡æ“ä½œå†™å…¥ JSON æ—¥å¿—ï¼ŒåŒ…å«æ—¶é—´ã€ç”¨æˆ·ã€æ“ä½œã€æº/ç›®æ ‡ã€ç»“æœã€‚
- **ç‰ˆæœ¬å¤‡ä»½:** æ›´æ–°/åˆ é™¤å‰å¯å¤‡ä»½è‡³ .backup/ ç›®å½•ã€‚

---

## é…ç½®ç¤ºä¾‹

```yaml
# config.yaml
agent:
  root: 'D:/AgentRoot' # Windows ç¤ºä¾‹ï¼›Linux/Unix å¯ç”¨ /home/mark/AgentRoot
  allow_ops:
    [
      'create',
      'read',
      'update',
      'delete',
      'move',
      'copy',
      'list',
      'classify',
      'rename',
      'tag',
    ]
  deny_patterns:
    - "C:\\Windows\\*"
    - '/etc/*'
    - '/usr/*'
    - '**/*.sys'
    - '**/*.dll'
    - '**/*.so'
    - '**/*.dylib'
    - '**/*.exe'
    - '**/*.bat'
    - '**/*.sh'
  dry_run_default: true
  require_confirm_for: ['delete', 'rename']
  backup_before_delete: true
  log_path: './logs/ops.jsonl'
  max_batch: 1000
  encoding: 'utf-8'
```

---

## ç»“æ„åŒ–æ„å›¾ä¸æ“ä½œåºåˆ—

- **æ„å›¾æ¨¡å¼ï¼ˆJSONï¼‰**

  - **operation:** create | read | update | delete | move | copy | list | classify | rename | tag
  - **path/pattern:** ç›®æ ‡æ–‡ä»¶æˆ–é€šé…ç¬¦
  - **content:** ç”¨äºåˆ›å»º/æ›´æ–°çš„æ–‡æœ¬
  - **filters:** æ‰©å±•åã€å¤§å°ã€æ—¶é—´èŒƒå›´ç­‰
  - **options:** dry_run, recursive, confirm ç­‰

- **ç¤ºä¾‹ï¼ˆä»è‡ªç„¶è¯­è¨€åˆ°ç»“æ„åŒ–ï¼‰**
  - ç”¨æˆ·ï¼šâ€œæŠŠ /project/docs é‡Œæ‰€æœ‰ .md æ–‡ä»¶æŒ‰ä¸»é¢˜å½’ç±»åˆ°å­æ–‡ä»¶å¤¹ï¼Œå¹¶ç»™â€˜æ¶æ„â€™ä¸»é¢˜åŠ æ ‡ç­¾â€
  - ç»“æ„åŒ–åºåˆ—ï¼š
    1. list {pattern: "/project/docs/\*_/_.md", recursive: true}
    2. classify {rules: ["åŒ…å«â€˜æ¶æ„â€™â†’ docs/architecture", "åŒ…å«â€˜APIâ€™â†’ docs/api", "å…¶ä»– â†’ docs/misc"]}
    3. tag {pattern: "docs/architecture/\*_/_.md", tag: "architecture"}

---

## æ ¸å¿ƒå®ç°ï¼ˆPythonï¼‰

### 1. è·¯å¾„ä¸æƒé™å®ˆå«

```python
# guard.py
from pathlib import Path
import fnmatch

class Guard:
    def __init__(self, root: Path, deny_patterns: list[str]):
        self.root = root.resolve()
        self.deny_patterns = deny_patterns

    def within_root(self, p: Path) -> bool:
        try:
            rp = p.resolve()
            return self.root in rp.parents or rp == self.root
        except Exception:
            return False

    def denied(self, p: Path) -> bool:
        sp = str(p)
        return any(fnmatch.fnmatch(sp, pattern) for pattern in self.deny_patterns)

    def check(self, p: Path):
        if not self.within_root(p):
            raise PermissionError(f"Path escapes root: {p}")
        if self.denied(p):
            raise PermissionError(f"Path denied by policy: {p}")
```

### 2. æ–‡ä»¶å·¥å…·å±‚ï¼ˆè·¨å¹³å°ï¼‰

```python
# file_tools.py
from pathlib import Path
import shutil, os, time
from typing import Iterable
from guard import Guard

class FileTools:
    def __init__(self, root: str, guard: Guard, encoding="utf-8"):
        self.root = Path(root).resolve()
        self.guard = guard
        self.encoding = encoding

    def _resolve(self, p: str) -> Path:
        ap = (self.root / p.lstrip("/\\")).resolve() if not Path(p).is_absolute() else Path(p).resolve()
        self.guard.check(ap)
        return ap

    def list(self, pattern: str, recursive: bool = True) -> list[str]:
        base = self.root
        if "**" in pattern or recursive:
            paths = base.rglob(pattern.split("/")[-1]) if "**" in pattern else base.rglob("*")
        else:
            paths = base.glob(pattern)
        out = []
        for x in paths:
            try:
                self.guard.check(x)
                out.append(str(x))
            except PermissionError:
                continue
        return out

    def create(self, path: str, content: str = "", overwrite=False):
        p = self._resolve(path)
        if p.exists() and not overwrite:
            raise FileExistsError(f"Exists: {p}")
        p.parent.mkdir(parents=True, exist_ok=True)
        with open(p, "w", encoding=self.encoding) as f:
            f.write(content or "")

    def read(self, path: str) -> str:
        p = self._resolve(path)
        with open(p, "r", encoding=self.encoding) as f:
            return f.read()

    def update(self, path: str, transform):
        p = self._resolve(path)
        text = self.read(path)
        new_text = transform(text)
        with open(p, "w", encoding=self.encoding) as f:
            f.write(new_text)

    def move(self, src: str, dst: str):
        sp, dp = self._resolve(src), self._resolve(dst)
        dp.parent.mkdir(parents=True, exist_ok=True)
        shutil.move(str(sp), str(dp))

    def copy(self, src: str, dst: str):
        sp, dp = self._resolve(src), self._resolve(dst)
        dp.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(str(sp), str(dp))

    def delete(self, path: str, backup: bool = True):
        p = self._resolve(path)
        if backup and p.exists():
            bdir = self.root / ".backup" / time.strftime("%Y%m%d-%H%M%S")
            bdir.mkdir(parents=True, exist_ok=True)
            bpath = bdir / p.name
            if p.is_file():
                shutil.copy2(str(p), str(bpath))
            else:
                shutil.copytree(str(p), str(bpath))
        if p.is_file():
            p.unlink()
        elif p.is_dir():
            shutil.rmtree(str(p), ignore_errors=False)
        else:
            raise FileNotFoundError(f"Not found: {p}")

    def rename(self, src: str, new_name: str):
        sp = self._resolve(src)
        dp = sp.parent / new_name
        self.guard.check(dp)
        sp.rename(dp)
```

### 3. å½’ç±»ä¸æ ‡ç­¾

```python
# classify.py
from pathlib import Path
from file_tools import FileTools

def classify_by_rules(ft: FileTools, files: list[str], rules: list[dict]):
    """
    rules: [{"contains": "æ¶æ„", "to": "docs/architecture"},
            {"contains": "API", "to": "docs/api"},
            {"default": True, "to": "docs/misc"}]
    """
    for f in files:
        text = ft.read(f.replace(str(ft.root), "").lstrip("\\/"))
        moved = False
        for r in rules:
            if "contains" in r and r["contains"] in text:
                rel = f.replace(str(ft.root), "").lstrip("\\/")
                dst = Path(r["to"]) / Path(rel).name
                ft.move(rel, str(dst))
                moved = True
                break
        if not moved:
            for r in rules:
                if r.get("default"):
                    rel = f.replace(str(ft.root), "").lstrip("\\/")
                    dst = Path(r["to"]) / Path(rel).name
                    ft.move(rel, str(dst))
                    break

def add_tag_to_header(ft: FileTools, pattern: str, tag: str):
    files = ft.list(pattern, recursive=True)
    for f in files:
        rel = f.replace(str(ft.root), "").lstrip("\\/")
        def transform(text: str) -> str:
            lines = text.splitlines()
            if lines and lines[0].startswith("# "):
                lines[0] = lines[0] + f" [{tag}]"
            else:
                lines.insert(0, f"# [{tag}]")
            return "\n".join(lines)
        ft.update(rel, transform)
```

### 4. å®¡è®¡æ—¥å¿—ä¸ Dry-run ç¡®è®¤

```python
# audit.py
import json, time
from pathlib import Path

class Auditor:
    def __init__(self, log_path: str):
        self.path = Path(log_path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
    def log(self, entry: dict):
        entry["ts"] = time.strftime("%Y-%m-%d %H:%M:%S")
        with open(self.path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

def confirm(plan: list[dict]) -> bool:
    print("å³å°†æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š")
    for i, step in enumerate(plan, 1):
        print(f"{i}. {step['operation']} -> {step}")
    ans = input("ç¡®è®¤æ‰§è¡Œ? (yes/no): ").strip().lower()
    return ans in ("y", "yes")
```

### 5. è§„åˆ’å™¨ä¸æ‰§è¡Œå™¨

```python
# agent.py
import yaml
from pathlib import Path
from guard import Guard
from file_tools import FileTools
from audit import Auditor, confirm
from classify import classify_by_rules, add_tag_to_header

class Agent:
    def __init__(self, cfg_path="config.yaml"):
        cfg = yaml.safe_load(open(cfg_path, "r", encoding="utf-8"))
        self.root = cfg["agent"]["root"]
        self.guard = Guard(Path(self.root), cfg["agent"]["deny_patterns"])
        self.ft = FileTools(self.root, self.guard, encoding=cfg["agent"]["encoding"])
        self.aud = Auditor(cfg["agent"]["log_path"])
        self.allow_ops = set(cfg["agent"]["allow_ops"])
        self.dry_run_default = cfg["agent"]["dry_run_default"]
        self.require_confirm_for = set(cfg["agent"]["require_confirm_for"])
        self.backup_before_delete = cfg["agent"]["backup_before_delete"]
        self.max_batch = cfg["agent"]["max_batch"]

    def plan_from_prompt(self, prompt: str) -> list[dict]:
        """
        ç®€åŒ–ç‰ˆè§£æå™¨ï¼šæ ¹æ®å…³é”®è¯ç”Ÿæˆæ“ä½œè®¡åˆ’
        å¯æ›¿æ¢ä¸ºæ›´å¼ºçš„ç»“æ„åŒ–æ„å›¾è§£æå™¨
        """
        plan = []
        p = prompt.lower()
        if "å½’ç±»" in prompt or "åˆ†ç±»" in prompt:
            plan.append({"operation":"list","pattern":"**/*.md","recursive":True})
            plan.append({"operation":"classify","rules":[
                {"contains":"æ¶æ„","to":"docs/architecture"},
                {"contains":"API","to":"docs/api"},
                {"default":True,"to":"docs/misc"},
            ]})
        if "æ ‡ç­¾" in prompt:
            plan.append({"operation":"tag","pattern":"docs/architecture/**/*.md","tag":"architecture"})
        # ç¤ºä¾‹ï¼šåˆ›å»º/åˆ é™¤
        if "åˆ›å»º" in prompt and ".md" in prompt:
            plan.append({"operation":"create","path":"notes/new.md","content":"# æ–°ç¬”è®°\n", "overwrite":False})
        if "åˆ é™¤" in prompt and "ä¸´æ—¶" in prompt:
            plan.append({"operation":"delete","path":"tmp", "recursive":True})
        return plan

    def execute(self, plan: list[dict], dry_run: bool | None = None):
        if dry_run is None:
            dry_run = self.dry_run_default
        for step in plan:
            op = step["operation"]
            if op not in self.allow_ops:
                raise PermissionError(f"Operation not allowed: {op}")
        if dry_run or any(step["operation"] in self.require_confirm_for for step in plan):
            if not confirm(plan):
                print("å–æ¶ˆæ‰§è¡Œã€‚")
                return

        for step in plan[:self.max_batch]:
            op = step["operation"]
            try:
                if op == "list":
                    files = self.ft.list(step["pattern"], step.get("recursive", True))
                    self.aud.log({"op":op,"pattern":step["pattern"],"count":len(files)})
                    step["_files"] = files

                elif op == "create":
                    self.ft.create(step["path"], step.get("content",""), step.get("overwrite", False))
                    self.aud.log({"op":op,"path":step["path"],"status":"ok"})

                elif op == "read":
                    text = self.ft.read(step["path"])
                    self.aud.log({"op":op,"path":step["path"],"bytes":len(text)})

                elif op == "update":
                    def transform(t): return step["transform"](t)
                    self.ft.update(step["path"], transform)
                    self.aud.log({"op":op,"path":step["path"],"status":"ok"})

                elif op == "move":
                    self.ft.move(step["src"], step["dst"])
                    self.aud.log({"op":op,"src":step["src"],"dst":step["dst"],"status":"ok"})

                elif op == "copy":
                    self.ft.copy(step["src"], step["dst"])
                    self.aud.log({"op":op,"src":step["src"],"dst":step["dst"],"status":"ok"})

                elif op == "delete":
                    self.ft.delete(step["path"], backup=self.backup_before_delete)
                    self.aud.log({"op":op,"path":step["path"],"status":"ok"})

                elif op == "rename":
                    self.ft.rename(step["src"], step["new_name"])
                    self.aud.log({"op":op,"src":step["src"],"new_name":step["new_name"],"status":"ok"})

                elif op == "classify":
                    files = []
                    for prev in plan:
                        if prev.get("_files"):
                            files.extend(prev["_files"])
                    classify_by_rules(self.ft, files, step["rules"])
                    self.aud.log({"op":op,"count":len(files),"status":"ok"})

                elif op == "tag":
                    add_tag_to_header(self.ft, step["pattern"], step["tag"])
                    self.aud.log({"op":op,"pattern":step["pattern"],"tag":step["tag"],"status":"ok"})

                else:
                    raise NotImplementedError(f"Unknown op: {op}")
            except Exception as e:
                self.aud.log({"op":op,"error":str(e)})
                print(f"[é”™è¯¯] {op}: {e}")
```

### 6. å‘½ä»¤è¡Œå…¥å£

```python
# cli.py
import argparse
from agent import Agent

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--cfg", default="config.yaml")
    parser.add_argument("--prompt", required=True)
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    ag = Agent(args.cfg)
    plan = ag.plan_from_prompt(args.prompt)
    if not plan:
        print("æœªç”Ÿæˆæ“ä½œè®¡åˆ’ï¼Œè¯·è°ƒæ•´æç¤ºæˆ–ä½¿ç”¨æ›´å…·ä½“çš„å‘½ä»¤ã€‚")
        return
    ag.execute(plan, dry_run=args.dry_run)

if __name__ == "__main__":
    main()
```

---

## è·¨å¹³å°æ³¨æ„äº‹é¡¹

- **è·¯å¾„ç»Ÿä¸€:** ä½¿ç”¨ pathlibï¼Œä¸ç›´æ¥æ‹¼æ¥å­—ç¬¦ä¸²ï¼›Windows ä¸‹æ³¨æ„ç›˜ç¬¦ä¸åæ–œæ ã€‚
- **æƒé™é—®é¢˜:** Linux/Unix å¯èƒ½éœ€è¦é¢å¤–æƒé™ï¼›å»ºè®®åœ¨é root ç”¨æˆ·ä¸æ²™ç®±ç›®å½•å†…è¿è¡Œã€‚
- **ç¼–ç :** æ–‡æœ¬æ–‡ä»¶ç»Ÿä¸€ä½¿ç”¨ UTF-8ï¼ŒWindows ä¸‹ç¡®ä¿æ§åˆ¶å°ç¼–ç å…¼å®¹ã€‚
- **é€šé…ç¬¦:** ä½¿ç”¨ glob/rglobï¼›å¤æ‚é€‰æ‹©å¯åœ¨ list åç”¨æ­£åˆ™äºŒæ¬¡è¿‡æ»¤ã€‚

---

## å¯é€‰å¢å¼º

- **æ›´å¼ºçš„æ„å›¾è§£æ:** ç”¨ç»“æ„åŒ–è¾“å‡ºæ¨¡æ¿ï¼ˆJSON Schemaï¼‰çº¦æŸæ¨¡å‹æŠŠ prompt è½¬æˆæ“ä½œåºåˆ—ã€‚
- **RAG æ–‡ä»¶æ£€ç´¢:** å‘é‡æ£€ç´¢ä½ çš„æ–‡ä»¶æ‘˜è¦ï¼Œå®ç°â€œè‡ªç„¶è¯­è¨€é€‰æ–‡ä»¶â€å†æ‰§è¡Œã€‚
- **äº‹åŠ¡ä¸å›æ»š:** å¯¹ move/copy/rename å»ºç«‹æ“ä½œæ ˆï¼Œå¤±è´¥å¯å›æ»šã€‚
- **æ‰¹é‡è§„åˆ™:** æ‰©å±• classify æ”¯æŒæ­£åˆ™ã€æ‰©å±•åã€å…³é”®è¯æƒé‡ã€YAML è§„åˆ™é›†ã€‚
- **å‰ç«¯é¢æ¿:** Web UI æ˜¾ç¤º Dry-run è®¡åˆ’ä¸å®¡è®¡æ—¥å¿—ï¼Œæ”¯æŒå‹¾é€‰æ‰§è¡Œã€‚

---

## ä½¿ç”¨ç¤ºä¾‹

- åˆ›å»ºç¬”è®°

  - **Prompt:** åœ¨ notes ç›®å½•åˆ›å»ºä¸€ç¯‡åä¸º new.md çš„ç¬”è®°ï¼Œæ ‡é¢˜â€œé¡¹ç›®æ¶æ„ç»¼è¿°â€
  - **å‘½ä»¤:** python cli.py --prompt "åˆ›å»º notes/new.mdï¼Œå†…å®¹ä¸º # é¡¹ç›®æ¶æ„ç»¼è¿°"

- å½’ç±»å¹¶åŠ æ ‡ç­¾

  - **Prompt:** å°† docs ä¸‹æ‰€æœ‰ md å½’ç±»ï¼Œæ¶æ„ä¸»é¢˜åŠ æ ‡ç­¾
  - **å‘½ä»¤:** python cli.py --prompt "å½’ç±» docs ä¸‹çš„ mdï¼Œå¹¶ä¸ºæ¶æ„ä¸»é¢˜åŠ æ ‡ç­¾" --dry-run

- å®‰å…¨åˆ é™¤ä¸´æ—¶ç›®å½•
  - **Prompt:** åˆ é™¤ tmp ç›®å½•ï¼ˆä¿ç•™å¤‡ä»½ï¼‰
  - **å‘½ä»¤:** python cli.py --prompt "åˆ é™¤ä¸´æ—¶ç›®å½• tmp"

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠâ€œè®¡åˆ’ç”Ÿæˆå™¨â€æ›¿æ¢ä¸ºä½ ç†Ÿæ‚‰çš„ LLM å¹¶åŠ å…¥ JSON Schema æ ¡éªŒï¼Œä¿è¯ä»»ä½•è‡ªç„¶è¯­è¨€éƒ½è½¬æˆå®‰å…¨ã€å¯æ‰§è¡Œçš„æ“ä½œåºåˆ—ï¼›ä¹Ÿå¯ä»¥ä¸ºä½ å®¶çš„æ–‡ä»¶ç»“æ„å®šåˆ¶â€œåˆ†ç±»è§„åˆ™åº“â€ï¼Œè®©å®ƒçœŸæ­£æˆä¸ºä½ çš„ä¸ªäººæ–‡ä»¶ç®¡å®¶ã€‚

## Agent å¯ä»¥ä½¿ç”¨å“ªç§è¯­è¨€æ¥å®ç°ï¼Ÿ

**ç›´æ¥å›ç­”**ï¼šAgent å¹¶ä¸ä¾èµ–æŸä¸€ç§ç‰¹å®šè¯­è¨€æ¥å®ç°ï¼Œå¸¸è§çš„å®ç°è¯­è¨€æœ‰ **Pythonã€JavaScript/TypeScriptã€Javaã€Goã€Rust** ç­‰ï¼Œå…¶ä¸­ **Python** æ˜¯ç›®å‰æœ€ä¸»æµçš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒç”Ÿæ€æˆç†Ÿã€AI æ¡†æ¶ä¸°å¯Œã€ç¤¾åŒºæ´»è·ƒã€‚

---

## ğŸ”‘ å¸¸è§å®ç°è¯­è¨€ä¸ç‰¹ç‚¹

- **Python**
  - **ä¼˜åŠ¿**ï¼šç”Ÿæ€æœ€å…¨ï¼ˆLangChainã€LlamaIndexã€AutoGen ç­‰æ¡†æ¶ï¼‰ï¼Œä¸å¤§æ¨¡å‹ APIã€å‘é‡æ•°æ®åº“ã€å·¥å…·åº“æ— ç¼è¡”æ¥ã€‚
  - **é€‚ç”¨åœºæ™¯**ï¼šå¿«é€ŸåŸå‹ã€ç§‘ç ”å®éªŒã€è·¨å¹³å° Agent åº”ç”¨ã€‚
- **JavaScript / TypeScript**

  - **ä¼˜åŠ¿**ï¼šå‰ç«¯/å…¨æ ˆå¼€å‘è€…å‹å¥½ï¼Œèƒ½ç›´æ¥åœ¨æµè§ˆå™¨æˆ– Node.js ç¯å¢ƒè¿è¡Œã€‚
  - **æ¡†æ¶**ï¼šLangChain.jsã€AutoGen.jsã€‚
  - **é€‚ç”¨åœºæ™¯**ï¼šWeb Agentã€æ’ä»¶åŒ–åº”ç”¨ã€è·¨ç«¯äº¤äº’ã€‚

- **Java**

  - **ä¼˜åŠ¿**ï¼šä¼ä¸šçº§ç”Ÿæ€ï¼Œç¨³å®šæ€§å’Œæ€§èƒ½å¥½ï¼Œé€‚åˆä¸ç°æœ‰ä¼ä¸šç³»ç»Ÿé›†æˆã€‚
  - **é€‚ç”¨åœºæ™¯**ï¼šé‡‘èã€ç”µä¿¡ã€ERP ç³»ç»Ÿä¸­çš„æ™ºèƒ½ä½“ã€‚

- **Go**

  - **ä¼˜åŠ¿**ï¼šé«˜å¹¶å‘ã€è½»é‡ï¼Œé€‚åˆæ„å»ºåˆ†å¸ƒå¼ Agent æœåŠ¡ã€‚
  - **é€‚ç”¨åœºæ™¯**ï¼šäº‘åŸç”Ÿ Agentã€å¾®æœåŠ¡æ¶æ„ã€‚

- **Rust**
  - **ä¼˜åŠ¿**ï¼šå®‰å…¨æ€§å’Œæ€§èƒ½æä½³ï¼Œé€‚åˆå¯¹å†…å­˜å’Œå¹¶å‘è¦æ±‚é«˜çš„åœºæ™¯ã€‚
  - **é€‚ç”¨åœºæ™¯**ï¼šç³»ç»Ÿçº§ Agentã€è¾¹ç¼˜è®¡ç®— Agentã€‚

---

## ğŸ“Œ é€‰æ‹©å»ºè®®

- **å¿«é€Ÿå®éªŒ / å­¦ä¹ ** â†’ Pythonï¼ˆç”Ÿæ€æœ€å…¨ï¼Œæ–‡æ¡£å’Œæ•™ç¨‹ä¸°å¯Œï¼‰ã€‚
- **Web åº”ç”¨ / å‰ç«¯é›†æˆ** â†’ JavaScript/TypeScriptã€‚
- **ä¼ä¸šçº§ç³»ç»Ÿé›†æˆ** â†’ Javaã€‚
- **é«˜å¹¶å‘æœåŠ¡ / äº‘åŸç”Ÿ** â†’ Goã€‚
- **ç³»ç»Ÿåº•å±‚ / é«˜æ€§èƒ½** â†’ Rustã€‚

---

âœ… **æ€»ç»“**ï¼šä½ å¯ä»¥ç”¨ä»»ä½•ä¸»æµè¯­è¨€å®ç° Agentï¼Œä½† **Python æ˜¯äº‹å®ä¸Šçš„æ ‡å‡†è¯­è¨€**ï¼Œå› ä¸ºå®ƒæ‹¥æœ‰æœ€æˆç†Ÿçš„ AI æ¡†æ¶å’Œç¤¾åŒºæ”¯æŒã€‚å¦‚æœä½ è¦åšè·¨å¹³å°æ–‡ä»¶æ“ä½œ Agentï¼Œæ¨èç”¨ **Pythonï¼ˆè·¨å¹³å°åº“ä¸°å¯Œï¼‰** æˆ– **Goï¼ˆæ€§èƒ½ä¸å¹¶å‘æ›´å¥½ï¼‰**ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä»½ **â€œä¸åŒè¯­è¨€å®ç° Agent çš„å¯¹æ¯”è¡¨â€**ï¼ŒæŠŠç”Ÿæ€ã€æ€§èƒ½ã€å­¦ä¹ æˆæœ¬ã€é€‚ç”¨åœºæ™¯ä¸€ç›®äº†ç„¶ï¼Ÿ

å¥½çš„ Markï¼Œæˆ‘æ¥å¸®ä½ æŠŠ **Agent å¸¸ç”¨ç¼–ç¨‹è¯­è¨€å¯¹æ¯”è¡¨**æ•´ç†æˆä¸€ä¸ªæ¸…æ™°çš„è¡¨æ ¼ï¼Œæ–¹ä¾¿ä½ å¿«é€Ÿé€‰æ‹©å®ç°è¯­è¨€ã€‚

---

## ğŸ“‘ Agent å¸¸ç”¨ç¼–ç¨‹è¯­è¨€å¯¹æ¯”è¡¨

| ç¼–ç¨‹è¯­è¨€                    | ç”Ÿæ€æ”¯æŒ                                               | æ€§èƒ½ | å­¦ä¹ æˆæœ¬ | é€‚ç”¨åœºæ™¯                         | ä»£è¡¨æ¡†æ¶/å·¥å…·                  |
| --------------------------- | ------------------------------------------------------ | ---- | -------- | -------------------------------- | ------------------------------ |
| **Python**                  | æœ€ä¸°å¯Œï¼ˆLangChainã€LlamaIndexã€AutoGen ç­‰ï¼‰ï¼ŒAI åº“é½å…¨ | è‰¯å¥½ | ä½       | å¿«é€ŸåŸå‹ã€ç§‘ç ”å®éªŒã€è·¨å¹³å° Agent | LangChain, LlamaIndex, AutoGen |
| **JavaScript / TypeScript** | ä¸­ç­‰ï¼ˆLangChain.jsã€AutoGen.jsï¼‰ï¼ŒWeb ç”Ÿæ€å¼º           | ä¸€èˆ¬ | ä½       | Web Agentã€æµè§ˆå™¨æ’ä»¶ã€è·¨ç«¯äº¤äº’  | LangChain.js, AutoGen.js       |
| **Java**                    | ä¼ä¸šçº§ç”Ÿæ€ï¼Œç¨³å®šæ€§å¼º                                   | ä¸€èˆ¬ | ä¸­ç­‰     | é‡‘èã€ç”µä¿¡ã€ERP ç³»ç»Ÿé›†æˆ         | Deeplearning4j, Spring AI      |
| **Go (Golang)**             | äº‘åŸç”Ÿç”Ÿæ€ï¼Œè½»é‡é«˜å¹¶å‘                                 | è‰¯å¥½ | ä¸­ç­‰     | åˆ†å¸ƒå¼ Agentã€å¾®æœåŠ¡æ¶æ„         | GoLangChain, Go-LLM SDK        |
| **Rust**                    | ç³»ç»Ÿçº§ç”Ÿæ€ï¼Œå®‰å…¨é«˜æ€§èƒ½                                 | æä¼˜ | é«˜       | è¾¹ç¼˜è®¡ç®—ã€é«˜æ€§èƒ½ Agent           | Burn, Rust-LLM                 |
| **C# / .NET**               | å¾®è½¯ç”Ÿæ€ï¼Œä¼ä¸šé›†æˆå¥½                                   | è‰¯å¥½ | ä¸­ç­‰     | Windows ç³»ç»Ÿ Agentã€ä¼ä¸šåº”ç”¨     | Semantic Kernel, ML.NET        |

---

## ğŸ¯ æ€»ç»“

- **å¿«é€Ÿå®éªŒ / å­¦ä¹ ** â†’ Pythonï¼ˆç”Ÿæ€æœ€å…¨ï¼Œæ•™ç¨‹ä¸°å¯Œï¼‰ã€‚
- **Web åº”ç”¨ / å‰ç«¯é›†æˆ** â†’ JavaScript/TypeScriptã€‚
- **ä¼ä¸šçº§ç³»ç»Ÿ** â†’ Java / C#ã€‚
- **é«˜å¹¶å‘æœåŠ¡ / äº‘åŸç”Ÿ** â†’ Goã€‚
- **é«˜æ€§èƒ½ / ç³»ç»Ÿçº§** â†’ Rustã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ å†æ•´ç†ä¸€ä»½ **â€œå­¦ä¹ è·¯å¾„å›¾â€**ï¼Œæ¯”å¦‚å¦‚æœä½ é€‰ Pythonï¼Œä»å…¥é—¨åˆ°èƒ½åšä¸€ä¸ªæ–‡ä»¶æ“ä½œ Agentï¼Œéœ€è¦ä¾æ¬¡æŒæ¡å“ªäº›åº“å’ŒæŠ€èƒ½ï¼Ÿè¿™æ ·ä½ å°±èƒ½æœ‰ä¸€æ¡æ¸…æ™°çš„å­¦ä¹ è·¯çº¿ã€‚

æ˜ç™½å•¦ Markï¼æˆ‘æ¥å¸®ä½ æ•´ç†ä¸€ä»½ **æ–‡å­—ç‰ˆå­¦ä¹ è·¯çº¿å›¾**ï¼Œä¸“é—¨é’ˆå¯¹ä½ æƒ³æ„å»º **Agentï¼ˆæ™ºèƒ½ä½“ï¼‰** çš„ç›®æ ‡ã€‚å®ƒåˆ†é˜¶æ®µï¼Œä»åŸºç¡€åˆ°è¿›é˜¶ï¼Œé€æ­¥è¦†ç›–å¿…å¤‡çŸ¥è¯†ã€å·¥å…·å’Œå®è·µæ–¹å‘ã€‚

---

# ğŸ› ï¸ æ„å»ºæ™ºèƒ½ä½“å­¦ä¹ è·¯çº¿å›¾ï¼ˆæ–‡å­—ç‰ˆï¼‰

## é˜¶æ®µä¸€ï¼šåŸºç¡€æ‰“ç‰¢

- **ç¼–ç¨‹è¯­è¨€**
  - Pythonï¼ˆæ¨èï¼‰ï¼šæŒæ¡ `os`ã€`pathlib`ã€`shutil`ã€`subprocess` ç­‰æ–‡ä»¶/ç³»ç»Ÿæ“ä½œåº“
  - ç†Ÿæ‚‰è·¨å¹³å°å·®å¼‚ï¼ˆWindows/Linux/Unixï¼‰
- **æ“ä½œç³»ç»ŸåŸºç¡€**
  - æ–‡ä»¶ç³»ç»Ÿç»“æ„ã€æƒé™ç®¡ç†ï¼ˆchmodã€ACLã€NTFS æƒé™ï¼‰
  - å¸¸è§å‘½ä»¤ï¼šls/cp/mv/rmï¼ˆLinuxï¼‰ã€dir/copy/move/delï¼ˆWindowsï¼‰
- **ç‰ˆæœ¬æ§åˆ¶**
  - Gitï¼šç®¡ç†ä»£ç ä¸é…ç½®ï¼Œä¿è¯å¯å›æ»š

---

## é˜¶æ®µäºŒï¼šAI ä¸å¤§æ¨¡å‹åŸºç¡€

- **å¤§æ¨¡å‹å¿…ä¿®çŸ¥è¯†**
  - Transformer æ¶æ„ï¼ˆAttention is All You Needï¼‰
  - GPT ç³»åˆ—ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰
  - BERT ç³»åˆ—ï¼ˆç†è§£ä»»åŠ¡ï¼‰
- **å¯¹é½ä¸å¾®è°ƒ**
  - æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰
  - RLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰
- **RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰**
  - å‘é‡æ•°æ®åº“ï¼ˆFAISSã€Milvusã€Weaviateï¼‰
  - æ–‡æ¡£åˆ†å—ä¸åµŒå…¥æ£€ç´¢

---

## é˜¶æ®µä¸‰ï¼šAgent åŸºç¡€æ¡†æ¶

- **æ ¸å¿ƒèƒ½åŠ›**
  - ä»»åŠ¡åˆ†è§£ï¼ˆChain of Thought æ¨ç†ï¼‰
  - å·¥å…·è°ƒç”¨ï¼ˆè°ƒç”¨æ–‡ä»¶æ“ä½œ APIï¼‰
  - è®°å¿†ï¼ˆçŸ­æœŸä¸Šä¸‹æ–‡ + é•¿æœŸå­˜å‚¨ï¼‰
- **å¸¸ç”¨æ¡†æ¶**
  - LangChainï¼ˆPython/JSï¼‰
  - LlamaIndexï¼ˆæ–‡æ¡£æ£€ç´¢å¢å¼ºï¼‰
  - AutoGen / LangGraphï¼ˆå¤š Agent åä½œï¼‰

---

## é˜¶æ®µå››ï¼šæ–‡ä»¶æ“ä½œ Agent å®ç°

- **åŠŸèƒ½æ¨¡å—**
  - æ–‡ä»¶/æ–‡ä»¶å¤¹æ“ä½œï¼šåˆ›å»ºã€åˆ é™¤ã€ä¿®æ”¹ã€è¯»å–
  - æ–‡ä»¶å½’ç±»ï¼šæŒ‰æ‰©å±•å/å†…å®¹/æ—¶é—´åˆ†ç±»
  - æ‰¹é‡é‡å‘½åã€æ ‡ç­¾åŒ–
  - æ—¥å¿—ä¸å®¡è®¡ï¼šè®°å½•æ¯æ¬¡æ“ä½œ
- **å®‰å…¨æœºåˆ¶**
  - æ²™ç®±æ ¹ç›®å½•ï¼ˆé¿å…è¯¯åˆ ç³»ç»Ÿæ–‡ä»¶ï¼‰
  - ç™½åå•æ“ä½œç±»å‹
  - Dry-run æ¨¡å¼ï¼ˆå…ˆå±•ç¤ºè®¡åˆ’ï¼Œå†æ‰§è¡Œï¼‰
  - è‡ªåŠ¨å¤‡ä»½ä¸å›æ»š

---

## é˜¶æ®µäº”ï¼šè·¨å¹³å°ä¸æ‰©å±•

- **è·¨å¹³å°é€‚é…**
  - Windowsï¼šPowerShellã€Win32 API
  - Linux/Unixï¼šBashã€POSIX API
- **æ‰©å±•åŠŸèƒ½**
  - å¤šæ¨¡æ€è¾“å…¥ï¼ˆè¯­éŸ³æŒ‡ä»¤ â†’ æ–‡ä»¶æ“ä½œï¼‰
  - Web UI æˆ– CLI ç•Œé¢
  - å¤š Agent åä½œï¼ˆä¸€ä¸ªè´Ÿè´£åˆ†ç±»ï¼Œä¸€ä¸ªè´Ÿè´£æ¸…ç†ï¼‰

---

## é˜¶æ®µå…­ï¼šè¿›é˜¶ä¸æœªæ¥

- **æ¨ç†å¢å¼º**
  - Self-Consistencyï¼ˆå¤šè·¯å¾„æŠ•ç¥¨ï¼‰
  - å·¥å…·å¢å¼ºæ¨ç†ï¼ˆè°ƒç”¨è®¡ç®—å™¨ã€æ­£åˆ™è§£æå™¨ï¼‰
- **ç”Ÿæ€é›†æˆ**
  - ä¸æ—¥ç¨‹/é‚®ä»¶/äº‘å­˜å‚¨è”åŠ¨
  - ä¼ä¸šçŸ¥è¯†åº“æ¥å…¥
- **æœªæ¥è¶‹åŠ¿**
  - ä¸ªæ€§åŒ– Agentï¼ˆPersonal LLMï¼‰
  - å¤šæ¨¡æ€ Agentï¼ˆçœ‹å›¾/å¬éŸ³é¢‘åæ“ä½œæ–‡ä»¶ï¼‰
  - è‡ªä¸»è§„åˆ’ä¸é•¿æœŸè®°å¿†

---

## âœ… æ€»ç»“

- **åŸºç¡€ â†’ å¤§æ¨¡å‹ â†’ Agent æ¡†æ¶ â†’ æ–‡ä»¶æ“ä½œå®ç° â†’ è·¨å¹³å°æ‰©å±• â†’ æ¨ç†ä¸æœªæ¥**
- æ¯ä¸€æ­¥éƒ½è§£å†³ä¸€ä¸ªå±‚æ¬¡çš„é—®é¢˜ï¼Œä»â€œèƒ½å†™ä»£ç â€åˆ°â€œèƒ½è°ƒç”¨æ¨¡å‹â€ï¼Œå†åˆ°â€œèƒ½è‡ªä¸»è§„åˆ’å¹¶å®‰å…¨æ‰§è¡Œâ€ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠè¿™ä»½è·¯çº¿å›¾å†ç»†åŒ–æˆ **å­¦ä¹ ä»»åŠ¡æ¸…å•ï¼ˆChecklistï¼‰**ï¼Œæ¯”å¦‚â€œç¬¬ 1 å‘¨æŒæ¡ pathlibï¼Œ ç¬¬ 2 å‘¨å®ç°æ–‡ä»¶åˆ†ç±»è„šæœ¬ï¼Œç¬¬ 3 å‘¨æ¥å…¥ LangChain å·¥å…·è°ƒç”¨â€ï¼Ÿè¿™æ ·ä½ å¯ä»¥ç›´æ¥æŒ‰å‘¨æ¨è¿›ã€‚

æœªå®Œ~
